<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Strategic Research Office of Individual Eleven</title>
  
  <subtitle>Frontier Explorer</subtitle>
  <link href="http://inlmouse.github.io/atom.xml" rel="self"/>
  
  <link href="http://inlmouse.github.io/"/>
  <updated>2024-08-14T02:56:55.234Z</updated>
  <id>http://inlmouse.github.io/</id>
  
  <author>
    <name>Patrick Sylvestre</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PerseidsMeteorShower24</title>
    <link href="http://inlmouse.github.io/PerseidsMeteorShower24/"/>
    <id>http://inlmouse.github.io/PerseidsMeteorShower24/</id>
    <published>2024-08-14T02:50:30.000Z</published>
    <updated>2024-08-14T02:56:55.234Z</updated>
    
    <content type="html"><![CDATA[<img src="/PerseidsMeteorShower24/composed-postpro.jpg" class width="6720" height="4480"> <center> 动图4：一个三层KAN的训练过程。</center>]]></content>
    
    
    <summary type="html">2024年英仙座流星雨摄影</summary>
    
    
    
    <category term="Misc" scheme="http://inlmouse.github.io/categories/Misc/"/>
    
    
    <category term="photography" scheme="http://inlmouse.github.io/tags/photography/"/>
    
  </entry>
  
  <entry>
    <title>2024阿里巴巴全球数学竞赛决赛(应用与计算数学)神经网络部分赏析</title>
    <link href="http://inlmouse.github.io/Ali2024GMC/"/>
    <id>http://inlmouse.github.io/Ali2024GMC/</id>
    <published>2024-07-09T09:52:48.000Z</published>
    <updated>2024-07-09T13:37:18.665Z</updated>
    
    <content type="html"><![CDATA[<div style="display:none">\(  \newcommand{\trsp}{\mathrm{T}}  \newcommand{\spr}{\mathbb{R}}  \newcommand{\Tr}{\mathrm{Tr}}  \newcommand{\expe}[2]{\mathbb{E}_{ #1}\begin{bmatrix}#2\end{bmatrix}}  \newcommand{\Var}{\mathrm{Var}}  \newcommand\defeq{\equiv}  \newcommand{\prob}[1]{\mathbb{P}\begin{bmatrix}#1\end{bmatrix}}  \newcommand\rvw{\boldsymbol{w}}  \newcommand\rvx{\boldsymbol{x}}\)</div><h2 id="题目2"><a href="#题目2" class="headerlink" title="题目2"></a>题目2</h2><p>假设$F(x;w)$是一个输出标量的深度神经网络，其中$x$是输入，$w$表示权重。假设$F$关于$w$连续可微，并且对于调练数据$\{x_j,y_j\}_{j=1}^m$过参数化：即存在$w^*$使得对所有$j$满足$F(x_j;w^*)=y_j$。为了研究训练神经网络在$w^*$的局部优化动力学，我们考虑线性化神经网络$\widetilde{F}(x;w)=F(x;w^*)+(w-w^*)^\trsp\nabla F(x;w^*)$，其损失函数为：<br>\begin{equation}<br>    \text{Loss}(w) = \frac{1}{2m}\sum_{j=1}^m (y_j-\widetilde{F}(x_j;w))^2.<br>\end{equation}<br>令$\eta$表示学习率，梯度下降法为$w_{i+1} = w_i - \eta\nabla\text{Loss}(w_i)$，而随机梯度下降法为$w_{i+1} = w_i - \eta(\nabla\text{Loss}(w_i) + \epsilon_i)$，其中噪声项$\epsilon_i$满足$\expe{}{\epsilon_i}=0$和$\expe{}{\epsilon_i\epsilon_i^\trsp}=M(w_i)/b$，$b$是mini-batch的大小。假设协方差矩阵$M$与<br>\begin{equation}<br>    \Sigma = \frac1m \sum_{j=1}^m\nabla F(x;w^*)\nabla F(x;w^*)^\trsp,<br>\end{equation}</p><p>在以下意义上对齐：<br>\begin{equation}<br>    \frac{\Tr(M(w)\Sigma)}{2\text{Loss}(w)|\Sigma|_F^2}\geq \delta,<br>\end{equation}</p><p>对于$\delta &gt;0$和所有$w$成立。这里$|\cdot|_F$表示Frobenius范数。</p><ol><li>对于梯度下降，证明如果$\Sigma$的谱范数满足：<br>\begin{equation}<br> |\Sigma|_2 \leq \frac{2}{\eta},<br>\end{equation}<br>则梯度下降是局部稳定的（即对所有$i$，$\text{Loss}(w_i)$有界）。（注意，这里蕴含了一个依赖维度的界：$|\Sigma|_F \leq \frac{2\sqrt{d}}{\eta}$，其中$d$是$w$的维度。）</li><li>对于随机梯度下降，如果$\expe{}{\text{Loss}(w_i)}$对所有$i$都有界,则以下独立于维度的不等式必须成立：<br>\begin{equation}<br> |\Sigma|_F \leq \frac{\sqrt{b/\delta}}{\eta}.<br>\end{equation}</li></ol><h2 id="题目2的证明"><a href="#题目2的证明" class="headerlink" title="题目2的证明"></a>题目2的证明</h2><ol><li>注意到损失函数可以改写为：<br>\begin{aligned}<br> \text{Loss}(w) &amp;= \frac{1}{2m}\sum_{j=1}^m (y_j-\widetilde{F}(x_j;w))^2\\<br> &amp; = \frac{1}{2m}\sum_{j=1}^m (y_j-F(x_j;w^*)-(w-w^*)^\trsp\nabla F(x_j;w^*))^2\\<br> &amp; = \frac{1}{2m}\sum_{j=1}^m ((w-w^*)^\trsp\nabla F(x_j;w^*))^2.<br>\end{aligned}</li></ol><p>显然：<br>\begin{equation}<br>    \nabla\text{Loss}(w) = \frac{1}{2m}\sum_{j=1}^m 2\nabla F(x_j;w^*)\nabla F(x_j;w^*)^\trsp(w-w^*) = \Sigma(w-w^*).<br>\end{equation}<br>考虑梯度下降法：<br>\begin{aligned}<br>    w_{i+1} - w^* &amp;= w_i - \eta\nabla\text{Loss}(w_i) -w^* =  w_i - \eta\Sigma(w_i-w^*) -w^*\\<br>    &amp; = (I-\eta\Sigma)(w_i-w^*),<br>\end{aligned}<br>其中$I$为单位阵。注意到$\Sigma$为Fisher信息矩阵，半正定，且由条件$|\Sigma|_2 \leq \frac{2}{\eta}$，得到：<br>\begin{equation}<br>    |I-\eta\Sigma|_2 \leq 1.<br>\end{equation}<br>于是递推得到：<br>\begin{equation}<br>    |w_{i+1} - w^*|_2 \leq |w_{i} - w^*|_2 \leq \cdots \leq |w_0 - w^*|_2.<br>\end{equation}<br>自然，$\forall i \in\mathbb{N}$，有：<br>\begin{aligned}<br>    \text{Loss}(w_i) &amp;= \frac{1}{2m}\sum_{j=1}^m ((w_i-w^*)^\trsp\nabla F(x_j;w^*))^2\\<br>    &amp; \leq \frac12 (w_i-w^*)^\trsp\Sigma(w_i-w^*) \leq \frac12 |(w_i-w^*)|_2^2|\Sigma|_F\\<br>    &amp; \leq \frac12 \frac{2\sqrt{d}}{\eta}|(w_0-w^*)|_2^2 = \frac{\sqrt{d}}{\eta}|(w_0-w^*)|_2^2.<br>\end{aligned}<br>其中$d$是$w$的维度，从而梯度下降法是局部稳定的。</p><ol><li>记$z_i = w_i - w^*$，考察随机梯度下降法的损失期望：<br>\begin{aligned}<br> \expe{}{\text{Loss}(w_{i+1})}  &amp;= \expe{}{\frac12 z_{i+1}^\trsp\Sigma z_{i+1}} = \expe{}{\frac12 (z_i -\eta\nabla\text{Loss}(w_i)-\eta \varepsilon_i )^\trsp\Sigma (z_i -\eta\nabla\text{Loss}(w_i)-\eta \varepsilon_i )}\\<br> &amp; = \expe{}{\frac12 z_i^\trsp\Sigma z_i - \eta z_i^\trsp\Sigma(\text{Loss}(w_i) +\varepsilon_i) + \frac12\eta^2(\text{Loss}(w_i) +\varepsilon_i)^\trsp\Sigma(\text{Loss}(w_i) +\varepsilon_i)}\\<br> &amp; = \expe{}{\frac12 z_i^\trsp\Sigma z_i - \eta z_i^\trsp\Sigma^2z_i - \underbrace{\eta z_i^\trsp\Sigma\varepsilon_i}_{\expe{}{\varepsilon_i}=0} + \frac12\eta^2(\Sigma z_i+\varepsilon_i)^\trsp\Sigma(\Sigma z_i+\varepsilon_i)}\\<br> &amp; = \expe{}{\frac12 z_i^\trsp\Sigma z_i - \eta z_i^\trsp\Sigma^2z_i} + \frac12\eta^2\expe{}{z_i^\trsp\Sigma^3z_i + \varepsilon_i^\trsp\Sigma\varepsilon_i + \underbrace{\varepsilon_i^\trsp\Sigma^2z_i}_{\expe{}{\varepsilon_i}=0}}\\<br> &amp;\overset{*}{=} \expe{}{\frac12 z_i^\trsp\Sigma z_i - \eta z_i^\trsp\Sigma^2z_i + \frac12\eta^2z_i^\trsp\Sigma^3z_i} + \frac12\eta^2\expe{}{\Tr(\Var[\varepsilon_i]\Sigma)}\\<br> &amp; \overset{\Var[\varepsilon_i] = M(w_i)/b}{=} \expe{}{\frac12 z_i^\trsp\Sigma z_i - \eta z_i^\trsp\Sigma^2z_i + \frac12\eta^2z_i^\trsp\Sigma^3z_i} + \frac{\eta^2}{2b}\expe{}{\Tr(M(w_i)\Sigma)}\\<br> &amp; \defeq \expe{}{s(z_i)\text{Loss}(w_{i})} + \frac{\eta^2}{2b}\expe{}{\Tr(M(w_i)\Sigma)}<br>\end{aligned}</li></ol><p>其中*式使用了恒等式$\expe{}{x^\trsp\Sigma x}=\Tr(\Sigma\Var[x])+\expe{}{x}^\trsp\Sigma\expe{}{x}$，$s(z) \defeq 1 -2\eta\frac{z^\trsp\Sigma^2z}{z^\trsp\Sigma z} + \eta^2\frac{z^\trsp\Sigma^3z}{z^\trsp\Sigma z}$。</p><p>这里使用Rayleigh Quotient归一化技巧，令：<br>\begin{equation}<br>    x = \frac{\Sigma^{\frac12}z}{|\Sigma^{\frac12}z|_2},<br>\end{equation}<br>记$\Sigma$的特征值为$\lambda_j \geq 0$，显然：<br>\begin{aligned}<br>    s(z) &amp; = 1 -2\eta x^\trsp\Sigma x+\eta^2x^\trsp\Sigma^2 x \\<br>    &amp; = 1 + \sum_j(-2\eta\lambda_j + \eta^2\lambda_j^2) \\<br>    &amp; \geq \inf_{\lambda = \min_j\{\lambda_j\}\geq 0}(1-2\eta\lambda +\eta^2\lambda^2) \geq 0.<br>\end{aligned}<br>那么：<br>\begin{aligned}<br>    \expe{}{\text{Loss}(w_{i+1})} &amp; = \expe{}{s(z)\text{Loss}(w_{i})} +  \frac{\eta^2}{2b}\expe{}{\Tr(M(w_i)\Sigma)}\\<br>    &amp;\geq  \frac{\eta^2}{2b}\expe{}{\Tr(M(w_i)\Sigma)}\\<br>    &amp;\overset{\text{alignment property}}{\geq } \frac{\eta^2}{2b} 2\delta|\Sigma|_F^2\text{Loss}(w_{i}).<br>\end{aligned}<br>由于$\text{Loss}(w_{i})$有界，因此$\expe{}{\text{Loss}(w_{i+1})}/\expe{}{\text{Loss}(w_{i})} \leq 1$，所以自然：<br>\begin{aligned}<br>     \frac{\eta^2\delta}{b} |\Sigma|_F^2 &amp;\leq 1\\<br>     |\Sigma|_F &amp;\leq \frac{\sqrt{b/\delta}}{\eta}.<br>\end{aligned}</p><h2 id="题目6"><a href="#题目6" class="headerlink" title="题目6"></a>题目6</h2><p>研究大模型的缩放定律对减少其训练开销至关重要。即，最终的测试损失是如何随着训练步数和模型大小的变化而变化的？本题中，我们研究训练线性模型时的缩放定律。</p><ol><li>在本小问中，我们考虑使用梯度下降学习一个一维线性模型的情况。  </li></ol><ul><li>定义数据分布$\mathcal{D}$为一个$\spr^2$上的分布，每个数据是一个数对$(x,y)$，分别代表输入和输出，并服从分布$x\sim\mathcal{N}(0,1),y\sim\mathcal{N}(3x,1)$.</li><li>用梯度下降算法学习线性模型$f_{w}(x)=w\cdot x$，其中$w,x\in\spr$。初始化$w_0=0$并进行多步迭代。每次选代时，从$\mathcal{D}$中采样$(x_t.y_t)$，然后更新$w_t$为$w_{t+1}\gets w_t - \eta\nabla\ell_t(w_t)$，其中$\ell_t = \frac12(f_w(x_t)-y_t)^2$是平方损失函数，$\eta&gt;0$是学习率。</li></ul><p>设学习率$\eta \in (0,\frac13]$，那么$T\geq 0$步迭代之后的测试损失期望<br>\begin{equation}<br>    \bar{\mathcal{L}}_{\eta,T} = \expe{w_T}{\expe{(x,y)\sim\mathcal{D}}{\frac12(f_{w_T}(x)-y)^2}}<br>\end{equation}<br>是多少？</p><ol><li>现在我们在第一小问的设定下，考虑学习率$\eta$被调到最优的情况。求函数$g(T)$，使得当$T\to +\infty$时，以下条件成立：<br>\begin{equation}<br> \left|\inf_{\eta\in (0,\frac13]} \bar{\mathcal{L}}_{\eta,T} -g(T) \right|=O\left(\frac{(\log T)^2}{T^2}\right).<br>\end{equation}</li><li>一个常常被观测到的实验现象是大语言模型的预训练过程大致遵循Chinchilla缩放定律：<br>\begin{equation}<br> \bar{\mathcal{L}}_{N,T} \approx \frac{A}{N^\alpha}+\frac{B}{T^\beta} +C,<br>\end{equation}<br>其中$\bar{\mathcal{L}}_{N,T}$是在经过$T$步训练后具有$N$个参数的模型的测试损失的期望，$A,B,\alpha,\beta,C$是常数。现在，我们来举一个训练多维线性模型的例子，使其也遵循类似的缩放定律。</li></ol><ul><li>固定$a&gt;0,b\geq 1$。每个数据$(x_{\bullet},y)$由一个输入和输出组成，其中输入$x_{\bullet}$是一个无限维向量(可看作一个序列)，输出$y$满足$y\in\spr$。定义数据分布$\mathcal{D}$如下，首先，从Zipf分布中采样$k$，$\prob{k=i}\propto i^{-(a+1)}(i\geq 1)$。令$j=\lceil k^b \rceil$. 然后，从$\mathcal{N}(0,1)$中采样得到$x_{\bullet}$的第$j$个坐标$x_j$，并令其余坐标为$0$。最后，$y\sim\mathcal{N}(3x_j,1)$。这样得到的$(x_{\bullet},y)$的分布即数据分布$\mathcal{D}$。</li><li>我们研究一个仅关注前$N$个输入坐标的线性模型。定义函数$\phi_N(x_{\bullet})=(x_1,\cdots,x_N)$。我们研究的线性模型具有参数$\rvw\in\spr^N$，输出为$f_{\rvw}(x_{\bullet})=\langle \rvw,\phi_N(x_{\bullet}) \rangle$。</li><li>我们使用梯度下降算法来学习该线性模型，初始化$\rvw_0 = \boldsymbol{0}$并进行多步达代每次迭代时。从$\mathcal{D}$中采样$(x_{t,\bullet},y_t)$，然后更新$\rvw_t$为$\rvw_{t+1}\gets \rvw_t - \eta\nabla\ell_t(\rvw_t)$，其中$\ell_t = \frac12(f_\rvw(x_{t,\bullet})-y_t)^2$。</li></ul><p>令$\bar{\mathcal{L}}_{\eta,N,T} = \expe{\rvw_T}{\expe{(\rvx,y)\sim\mathcal{D}}{\frac12(f_{\rvw_T}(\rvx)-y)^2}}$为以学习率$\eta \in (0,\frac13]$对具有$N$个参数的线性模型进行$T\geq 0$步训练后的测试损失的期望。</p><p>请求出$\alpha,\beta,C$，使得$\exists \gamma &gt;0, \forall c&gt;0$，当$T = N^{c+o(1)}$且$N$足够大时，以下条件成立：<br>\begin{equation}<br>    \epsilon(N,T)=\frac{\inf_{\eta \in (0,\frac13]}\bar{\mathcal{L}}_{\eta,N,T} -C}{\frac{1}{N^\alpha}+\frac{1}{T^\beta}}, (\log N + \log T)^{-\gamma} \leq \epsilon(N,T) \leq (\log N + \log T)^{\gamma}.<br>\end{equation}<br>即$\inf_{\eta \in (0,\frac13]}\bar{\mathcal{L}}_{\eta,N,T} = \widetilde{\Theta}(N^{-\alpha}+T^{-\beta})+C$，$\widetilde{\Theta}$表示忽略任何关于$\log N$和$\log T$的多项式。</p><h2 id="题目6的证明"><a href="#题目6的证明" class="headerlink" title="题目6的证明"></a>题目6的证明</h2><p>注意到，$y=3x+\varepsilon, \varepsilon\sim\mathcal{N}(0,1)$，那么显然：<br>\begin{aligned}<br>\bar{\mathcal{L}}_{\eta,T} &amp;= \expe{w_T}{\expe{(x,y)\sim\mathcal{D}}{\frac12(f_{w_T}(x)-y)^2}}\\<br>&amp; = \expe{w_T}{\expe{x\sim\mathcal{N}(0,1),\varepsilon\sim\mathcal{N}(0,1)}{\frac12(w_Tx-(3x+\varepsilon)))^2}}\\<br>&amp; = \expe{w_T}{\frac12(w_T-3)^2\expe{x\sim\mathcal{N}(0,1)}{x^2}+\frac12\expe{\varepsilon\sim\mathcal{N}(0,1)}{\varepsilon^2}}\\<br>&amp; = \frac12 \expe{w_T}{(w_T-3)^2}+\frac12.<br>\end{aligned}<br>问题划归为求$w_T$的一阶矩和二阶矩。考察梯度下降公式：<br>\begin{equation}<br>    w_{t+1} = w_t - \eta\nabla\ell_t(w_t) = w_t - \eta(w_tx_t^2-x_ty_t).<br>\end{equation}<br>考察一阶矩有：<br>\begin{aligned}<br>    \expe{}{w_{t+1}} &amp;= \expe{}{w_t} - \eta (\expe{}{w_t}\expe{}{x_t^2}-\expe{}{x_ty_t})\\<br>    &amp; = \expe{}{w_t} - \eta \expe{}{w_t} + 3\eta \\<br>    &amp; = (1-\eta)\expe{}{w_t} + 3\eta.<br>\end{aligned}<br>也即是$\expe{}{w_{t+1}} - 3  = (1-\eta)\expe{}{w_t} + 3\eta - 3 = (1-\eta)(\expe{}{w_t} - 3)$，递推得到$\expe{}{w_T} = (1-\eta)^T(\expe{}{w_0}-3)+3 = 3\left(1-(1-\eta)^T\right)$.</p><p>同理考察二阶矩有：<br>\begin{aligned}<br>    \expe{}{w_{t+1}^2} &amp;= \expe{}{w_t^2} - 2\eta (\expe{}{w_t^2}\expe{}{x_t^2}-\expe{}{w_t}\expe{}{x_ty_t}) \\<br>        \,\,\, &amp;+ \eta^2\left(\expe{}{w_t^2}\expe{}{x_t^4} -2 \expe{}{w_t}\expe{}{x_t^3y_t} + \expe{}{x_t^2y_t^2}\right)\\<br>    &amp; = \expe{}{w_t^2} - 2\eta(\expe{}{w_t^2}-3\expe{}{w_t}) + \eta^2(3\expe{}{w_t^2} -18 \expe{}{w_t} + 28)\\<br>    &amp; = (1-2\eta+3\eta^2)\expe{}{w_t^2} - (18\eta^2-6\eta)\expe{}{w_t} + 28\eta^2<br>\end{aligned}<br>带入一阶矩递推公式，凑配得：<br>\begin{aligned}<br>    \expe{}{w_{t+1}^2} - 6\expe{}{w_{t+1}} &amp; = (1-2\eta+3\eta^2)\expe{}{w_t^2} - (18\eta^2-6\eta)\expe{}{w_t} + 28\eta^2 - 6(1-\eta)\expe{}{w_t} - 18\eta\\<br>    &amp; = (1-2\eta+3\eta^2)(\expe{}{w_t^2}-6\expe{}{w_t}) + 28\eta^2 - 18\eta.<br>\end{aligned}<br>记$s = \frac{28\eta -18}{2-3\eta}$，也即是：<br>\begin{aligned}<br>    \expe{w_t}{(w_t-3)^2} &amp;= \expe{}{w_{t}^2} - 6\expe{}{w_{t}} + 9 \\<br>    &amp; = (1-2\eta+3\eta^2)^{t}(\expe{}{w_0^2} - 6\expe{}{w_0} + s) - s(1-2\eta+3\eta^2) + 9\\<br>    &amp; = s\left((1-2\eta+3\eta^2)^{t} -1 + 2\eta -3\eta^2 \right) + 9.<br>\end{aligned}<br>带回原式得到：<br>\begin{aligned}<br>    \bar{\mathcal{L}}_{\eta,T} &amp;= \frac12 \expe{w_T}{(w_T-3)^2}+\frac12\\<br>    &amp; = \frac{s}{2}\left((1-2\eta+3\eta^2)^{T} -1+ 2\eta -3\eta^2\right) + 5\\<br>    &amp; = \frac{(14\eta -9)\left((1-2\eta+3\eta^2)^{T} -1+ 2\eta -3\eta^2\right)}{2-3\eta}+5.<br>\end{aligned}</p><p>…第三问待续…</p>]]></content>
    
    
    <summary type="html">The Solution of 2024 Alibaba Global Mathematics Competition Finals(Applied and Computational Mathematics) Q2&amp; Q6.(in Chinese)</summary>
    
    
    
    <category term="Mathematical Competition" scheme="http://inlmouse.github.io/categories/Mathematical-Competition/"/>
    
    
    <category term="Math" scheme="http://inlmouse.github.io/tags/Math/"/>
    
    <category term="Statistic" scheme="http://inlmouse.github.io/tags/Statistic/"/>
    
    <category term="Neural Scaling Law" scheme="http://inlmouse.github.io/tags/Neural-Scaling-Law/"/>
    
  </entry>
  
  <entry>
    <title>Search Engines, Information Cocoons, and Corpus Database Pollution</title>
    <link href="http://inlmouse.github.io/CNSearchEngine/"/>
    <id>http://inlmouse.github.io/CNSearchEngine/</id>
    <published>2024-06-03T06:16:38.000Z</published>
    <updated>2024-06-03T07:47:44.987Z</updated>
    
    <content type="html"><![CDATA[<p>Common Crawl, a nonprofit 501 organization that crawls the web and freely provides its archives and datasets to the public. Every June, the organization releases the metadata it crawled that year. What’s interesting is that after post-processing such as removing the NSFW content, it was found that the compressed data of simplified Chinese was only 6TB, and after decompression it was about 30TB, and the compressed data of traditional Chinese was 6TB. Compared with the proportion of Internet users in the world, this data volume is quite low. The Chinese data reached a maximum in 2019, and then fell back. The reason is that the “QingLang Action 2020” organized by the Cyberspace Administration of China closed a large number of forums and actually stopped the establishment of new forums. Such as Baidu Tieba, a forum that once produced half of the Chinese internet’s content, deleted all data before 2015.</p><p>Last month, OpenAI released GPT-4o, a decidedly flirty new large language model (LLM) equipped with new and advanced capabilities. However, some Chinese reseachers started to notice that something seemed off about this newest version of the chatbot: the tokens it uses to parse text were full of spam and porn phrases. It is not rare for a language model to crawl spam when collecting training data, but usually there will be significant effort taken to clean or wash the data before it’s used. What’s even more interesting is that in fact, it is not a difficult task to clean such data, at least most of them. This technology has been applied in spam filtering more than 20 years ago. The content of these Chinese tokens could suggest that they have been polluted by a specific phenomenon: websites hijacking unrelated content in Chinese or other languages to boost spam messages. Although OpenAI has not responded to this yet, if it was done intentionally for some purpose, it would be extremely terrifying. From a pessimistic perspective, this means that the Chinese corpus data has withered or be polluted to the point where it is difficult to clean. After all, the possibility that OpenAI made such a low-level mistake as “forgetting to clean the data” is very small.</p><p>Not only websites hijacking will destroy search results, AI driven SEO(search engine optimization) techniques are more destructive. Doubao, a AI chatbot of ByteDance, used SEO technology in order to gain a higher weight in search engines. It directly allows AI to produce content(conversation data), fix it into static web pages, and then it is captured by search engines to drive traffic to itself. A “content farm” or “content mill” is a company that employs large numbers of freelance writers or uses automated tools to generate a large amount of textual web content which is specifically designed to satisfy algorithms for maximal retrieval by search engines, i.e. SEO. Now, in the AI ​​era, the speed of producing low-quality content has increased thousands of times compared to before. I really didn’t expect that ByteDance officially would blatantly operate a content farm. Furthermore, this is no longer an SEO issue, but a privacy and security issue. We all know that the LLMs will use our conversation data for training, and this is already a default consensus. However, it is outrageous that the chat records are actually made public and can be searched by search engines. </p><p>I can’t imagine that if this phenomenon is allowed to continue, the Chinese corpus will be filled with low-quality data generated by AI. Training a good Chinese LLM is even more difficult, and ordinary users will be buried in a dump of information garbage. <em>When a language can no longer generate new information, that is when it dies.</em></p><p>Recently some people have been discussing whether ChatGPT will replace search engines. I must say: it is dangerous to let AI chatbots such like ChatGPT replace search engines.</p><p>Not only should ChatGPT not replace search engines, but also short video platforms, social media platforms and shopping platforms that you have often used in recent years should not replace search engines.</p><p><strong>The supply of knowledge must not be monopolized.</strong></p><p>Search engines can certainly control the ranking of search results, but this is a weak intervention after all. At the same time, there are many different search engines in the world. Once we get used to something feeding the only answer of a question to our mouths, we are inviting a cyber BigBrother back.</p>]]></content>
    
    
    <summary type="html">The supply of knowledge must not be monopolized.(in English)</summary>
    
    
    
    <category term="Political Commentary" scheme="http://inlmouse.github.io/categories/Political-Commentary/"/>
    
    
    <category term="politic" scheme="http://inlmouse.github.io/tags/politic/"/>
    
  </entry>
  
  <entry>
    <title>Kolmogorov-Arnold Networks</title>
    <link href="http://inlmouse.github.io/KAN/"/>
    <id>http://inlmouse.github.io/KAN/</id>
    <published>2024-05-23T05:25:59.000Z</published>
    <updated>2024-05-23T06:01:10.633Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数学机理"><a href="#数学机理" class="headerlink" title="数学机理"></a>数学机理</h2><p>基于Universal Approximation Theorem(UAT)的Multilayer Perceptron(MLP)作为构建神经网络的基本组件，由于其可以拟合任意函数的能力其本身及其变体在过去几十年来被广泛使用。然而由于其不可解释性，长期以来饱受诟病。近日，一个基于Kolmogorov-Arnold Representation Theorem(KART)的新神经网络结构Kolmogorov-Arnold Network(KAN)[5]被提出，在各项指标中都有替代MLP之势。本文对这个新结构KAN进行简单解读。</p><p>首先重述一下MLP的原理，ML本质上是寻找/拟合一个尽可能逼近一个需要但不一定完全正确的函数。那么为什么MLP如此naïve的结构可以拟合我们可能想寻找的万千函数？UAT告诉我们，在包含足够多隐层神经元的FNN可以以任意精度近似任何连续函数。也即是，无论我们的目标函数有多复杂，一定存在一个NN可以以任意高的精度去近似，进一步地，也即是NN理论上可以解决任何近似问题，这也是近年PINN在求解ODE/PDE上大放异彩的原因。</p><p><strong>Theorem 1</strong>(Universal Approximation Theorem[1]). $\forall n,m \in \mathbb{N}, \varepsilon &gt; 0$，闭区间$K \subseteq \mathbb{R}^n$及其上的连续函数$f: K \mapsto \mathbb{R}^m$，存在$k\in \mathbb{N}, W\in \mathbb{R}^{k\times n}, b\in \mathbb{R}^{k}, C\in \mathbb{R}^{m\times k}$，使得：<br>\begin{equation}<br>    \sup_{x\in K}|f(x)-C\cdot(\sigma\circ(W\cdot x+b))| &lt; \varepsilon.<br>\end{equation}<br>其中$\sigma:\mathbb{R}\mapsto\mathbb{R}$为激活函数。</p><p>但是遗憾的是，这个定理是一个存在性定理，定理本身没有给出如何构造的指导，关于如何更好逼近的大量工作都是经验性的。同时在很多结构中，例如Transformer，MLP的参数量异常巨大，这给NN的可解释性问题更加蒙上了一层阴影。为此，业界一直在寻找MLP的一种替代性方案，目标是使用更少的参数获得同等或者更好的效果，并且具有较强的可解释性。</p><p>早在1993年，就有研究发现KART在多层感知器的研究中扮演着类似于通用逼近定理的角色[2]，本文的工作也启发自KART。Theorem 2即是KART的表述。</p><p><strong>Theorem 2</strong>(Kolmogorov-Arnold Representation Theorem[3]). 如果$f$是多元连续函数，则$f$可以写成单变量连续函数与二元加法运算的有限复合。也即是：<br>\begin{equation}<br>    f(\mathbf {x} )=f(x_{1},\ldots ,x_{n})=\sum _{q=0}^{2n+1}\Phi _{q}\left(\sum _{p=1}^{n}\phi _{q,p}(x_{p})\right).<br>\end{equation}<br>其中：$\phi_{q,p}: [0,1]\to \mathbb{R}, \Phi_{q}: \mathbb{R} \to \mathbb{R}$。</p><p>这个定理还给出了一个重要的陈述：多元函数的本质就是加法，因为均可以由一元函数和加法构造出来[4]。这里再给出一个$n=2$的示例，如图1所示。</p><img src="/KAN/KART.png" class width="540" height="270"> <center> 图1：$n=2$的KART示例图。</center><p>注意到无论是外层函数$\Phi$还是内层函数$\phi$这里都是一元函数，一个直观的理解就是可以直接用图像表示出来，如图2所示。</p><img src="/KAN/KARTplot.png" class width="535" height="275"> <center> 图2：$n=2$的KART示例图，将一元函数用图像表示。</center><h2 id="KAN的结构"><a href="#KAN的结构" class="headerlink" title="KAN的结构"></a>KAN的结构</h2><p>注意到KART的表示可以立马转换为一个网络，这早在上世纪九十年代就有人意识到了，当时叫Kolmogorov Network特指这个两层的单输出结构，如图3所示。那么自然的，这结构可以类似于MLP的抽象，并且定义为KAN Layer。并且可以把网络做得更深。<br><img src="/KAN/KAN.png" class width="570" height="345"> </p><center> 图3：KAN结构的抽象。</center><p>KART明确两层结构就可以完成拟合任务，那么为什么我们需要研究更深的KAN结构呢？实际上KART没有约束函数ϕ的数学性质，有可能是病态甚至是分形的。我们更希望使用光滑可导的函数来表达目标，以期能使用更方便的优化方式。</p><p>举个例子，$e^{\sin(x_1^2+x_2^2)+\sin(x_3^2+x_4^2)}$这个结构确实需要三层的结构去拟合。如动图4所示就是其训练的过程，我们发现如果是一层3层的KAN，那么在训练结束后确实可以在每一层得到和理论结果对应的激活函数。</p><img src="/KAN/KAN.gif" class width="300" height="400"> <center> 动图4：一个三层KAN的训练过程。</center><p>如图5所示，展示了MLP和KAN的一些对比。值得一提的是，MLP和KAN背后的表示论基础，也即是UAT和KART，这二者之间的联系还相当不清楚，这二者孰优孰劣也没有定论。就模型结构而言，二者一个主要差别是MLP的激活函数在模型节点上，而KAN是在边上。</p><img src="/KAN/KANvsMLP.png" class width="560" height="350"> <center> 图5：KAN vs MLP。</center><h2 id="算法细节"><a href="#算法细节" class="headerlink" title="算法细节"></a>算法细节</h2><p>本节将简单描述一下KAN在实现过程中的一些算法细节，虽然原论文中的理论做了一些理论上的证明以期保证了其收敛，但其实算法本身非常简单几句话就能说明白，这也是这个工作较为难能可贵的地方。</p><p>第一个问题是，如何把上面所说的一元函数转化为或者参数化为一个可学习的激活函数。那么本质上就是构造一种一元函数，样条函数应该是一个不错的选择。简单来说，样条函数就是一种分段函数，在每一段都具有连续的导数，最后。在论文中使用了B样条，如图6所示，B样条可以设置一些样条基函数，最后这个样条函数就是这些基函数的线性组合。所以真正学习的参数就是这个线性组合的系数$c_i$。样条函数一般在低维空间的函数逼近任务上效果很好，因此这个选择可以说是很自然的，同时注意到基函数具有连续导数，那么整个网络就是可导的，也即可以使用BP来训练。而B样条的基函数可以使用Cox-de Boor递归公式来定义，这个公式很优雅，但是涉及到递归求解，所以这一步是比较耗时的。样条函数还有个好处就是，我们可以在不同的分段细粒度上面切换，如图6所示，$G_1=5$和$G_2=10$分别就是一个比较粗和比较细的网格划分，这个好处就是在网络结构正确的情况下，如果我们有更多的数据，我们不需要重新训练网络，只需要把网格划分变细，就能简单地得到一个更好的网络。</p><img src="/KAN/Bsplines.png" class width="985" height="455"> <center> 图6：左:流经网络的激活符号。右:激活函数是参数化为B样条，它允许在粗粒度和细粒度网格之间切换。</center><p>在原始的实现中，作者使用了一些技巧，与其是技巧，不如说是实际上是作者尝试的很多方法中，他试出来第一个能work的结果。作者承认在这一部分还有更多的优化空间。第一个trick是Residual activation functions，如果我们一开始就直接把激活函数设置为样条，那大概率这是一些局部光滑但全局不那么光滑的函数，它对于loss landscape是不好的。因此作者在样条函数之上添加了一个光滑的基座作为初始化，这里选用了SiLU函数。样条的系数初始化几乎为0，这个和残差链接有一点相似。第二个是激活范围的问题，这个在MLP当中也存在，一般希望节点激活输出后，激活值尽可能保持在区间$[0,1]$之间，这里选用的方案是把基函数系数$c_i\sim\mathcal{N}(0,\sigma^2)$，方差$\sigma$是一个较小的数。在整体外设置权重$w$来控制整体激活函数的幅值，并且和MLP同样使用Xavier初始化。最后一点是，由于样条函数的定义域是有界的，而每一层的输出并不确定，所以自然需要动态地更新这个网格区间，作者经过了相当多的尝试后，采用了每训练迭代20次，对节点输出的范围做一次估计，根据其分布再来重新划定区间。</p><p>把细节讲明白之后最后我们完整地讲讲其训练过程。首先从一个全连接的KAN开始，第一步是通过一个修改过的L1正则把网络变得更稀疏，把较小权重的链接剪枝；有意思的地方来了，由于一元函数的图像人类是比较熟悉的，这时候根据观察一元函数的图像以及其具体值，人类是可以猜测这个一元函数的的符号形式，如图7所示，猜测这个结果是正弦、平方和指数函数，全部设置完毕后，求解出来的结果是可以达到机器精度的。这样是可以求解出正切的符号公式的，这个决策过程相当透明化，也具有很强的可信度和可解释性。虽然这个学习也可以全自动化不需要人类干预，但是实际上在很多任务下，就是需要人类先验知识才能获得更好的效果。</p><img src="/KAN/Training.png" class width="835" height="425"> <center> 图7：一个如何用KAN进行符号回归的例子。</center><h2 id="性质和效果"><a href="#性质和效果" class="headerlink" title="性质和效果"></a>性质和效果</h2><p>上面我们讲了KAN是什么，接下来我们来介绍KAN有上面好处，更中性的说法是首先尽量理解其性质再说它的好处。</p><p>首先我们注意到的是这个Scaling性质，这个在摩尔定律和LLM中有很多体现。简而言之就是模型的参数量越多其表现越好。不严谨地讲，具体到公式(Neural scaling laws)上就是$\ell\propto N^{-\alpha}$，其中$\ell$是近似误差(Loss/Error)，$N$是参数量，$\alpha$是缩放比例指数(scaling exponent)[6]。就$d$元的样条函数而言，$\alpha=(k+1)/d$，其中$k$是基函数的阶。而根据KART，这里的$d=1$。很显然，这是一个比较理想的界，比MLP的理论上界要好很多。</p><p>正是因为 KAN 的收敛速度比 MLP 更快，因此具有较少参数的KAN比MLP 可以获得更加的准确性的准确性，如图8所示。即便是特殊函数，如图9所示，我们用模型参数的数量和RMSE损失在平面上展示了KAN和MLP的Pareto Frontiers 。在所有特殊函数中，KAN都比MLP具有更好的Pareto Frontiers(关于Pareto Frontiers的概念请参考[7])。</p><img src="/KAN/FitRes.png" class width="980" height="210"> <center> 图8：用5个玩具模型来比较KAN和MLP。KANs几乎可以饱和我们的理论预测的最快缩放定律($\alpha = 4$)，而MLP收敛相对缓慢。</center><img src="/KAN/SFRes.png" class width="575" height="280"> <center> 图9：KAN对特殊函数的拟合。</center><p>灾难性遗忘几乎是困扰所有机器任务的问题，而KAN由于其结构具有局部可塑性，可以通过利用样条的局域性来避免灾难性遗忘。这是因为样条基函数是局部的，一个样本只会影响附近的几个样条系数，而留下较远的系数不变(这是可取的，因为较远的区域可能已经存储了我们想要保存的信息)。相比之下，由于MLP通常使用全局激活，例如，ReLU/Tanh/SiLU等，任何局部变化都可能不受控制地传播到遥远的区域，破坏存储在那里的信息。如图10所示，展示了一个简单的持续学习问题。数据集是一个具有5个高斯峰的1D回归任务(最上面一行)。每个峰值周围的数据按顺序呈现(而不是一次全部呈现)给KAN和MLP。KAN(中排)可以完全避免灾难性遗忘，而MLP(下排)则表现出严重的灾难性遗忘。</p><img src="/KAN/Forgetting.png" class width="915" height="400"> <center> 图10：KAN对抗灾难性遗忘。</center><h2 id="应用与批评"><a href="#应用与批评" class="headerlink" title="应用与批评"></a>应用与批评</h2><p>由于KAN的开创性和对MLP的直接可替代性，在预印本论文发布以来，极短时间内有大量的同行对其进行了大量的改进和实验，在业内有较为重大的影响。截至目前已经有一些工作得到了同行的认可，这里我们做一个简单的介绍。</p><p>针对于KAN的原本实现计算缓慢的问题， EfficientKAN将计算重新表述为用不同的基函数激活输入，然后将它们线性组合。这种重新表述可以显著降低内存成本，并使计算成为简单的矩阵乘法，并且可以自然地处理向前传递和向后传递。显著提升了其计算效率。同时，不同结构的神经网络也尝试将原来的连接方式变为了KAN链接，例如 GraphKAN和 ConvolutionKAN，也获得了一些效果。同时在大语言模型上，KAN-GPT和 Kansformer相继进行了实验，基本可以做到快速精度提升的效果。KAN作为一个即插即用的模块，基本可以无差别的对现有MLP结构做直接替换。</p><p>虽然KAN在的讨论程度极高，但是其效果还没有经过大规模验证和同行评议。而美国科学院院士， PINN(Physics-Informed Neural Network)发明者，布朗大学George Karniadakis教授在社交网络上公开质疑KAN的效果，原论文声称KAN在求解ODE/PDE有很好的效果，但是George院士表示使用了KAN作者在GitHub上的代码给对求解NS方程出了完全错误的结果，甚至对最简单的泊松方程的求解都得到了24%的误差。此外，论文对于光滑样条后的存在性证明也没有良好的叙述，什么样的目标函数能够使用KAN进行表述这在数学上没有保证或者限制，边界条件没有做任何讨论。目前做PDE/ODE的科学家对该方法的有效性大规模存疑。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]: Funahashi K I. On the approximate realization of continuous mappings by neural networks[J]. Neural networks, 1989, 2(3): 183-192.<br>[2]: Lin J N, Unbehauen R. On the realization of a Kolmogorov network[J]. Neural Computation, 1993, 5(1): 18-20.<br>[3]: Arnold H K. Dessert: Hilbert’s 13th Problem, in Full Colour[J].<br>[4]: Diaconis P, Shahshahani M. On nonlinear functions of linear combinations[J]. SIAM Journal on Scientific and Statistical Computing, 1984, 5(1): 175-191.<br>[5]: Liu Z, Wang Y, Vaidya S, et al. Kan: Kolmogorov-arnold networks[J]. arXiv preprint arXiv:2404.19756, 2024.<br>[6]: Hestness J, Narang S, Ardalani N, et al. Deep learning scaling is predictable, empirically[J]. arXiv preprint arXiv:1712.00409, 2017.<br>[7]: Goodarzi E, Ziaei M, Hosseinipour E Z. Introduction to optimization analysis in hydrosystem engineering[M]. New York, NY, USA: Springer International Publishing, 2014.</p>]]></content>
    
    
    <summary type="html">An Introduction to Kolmogorov-Arnold Networks.(in Chinese)</summary>
    
    
    
    <category term="Machine Learning" scheme="http://inlmouse.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Machine Learning" scheme="http://inlmouse.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>SK Model in Statistical Physics with Replica Method</title>
    <link href="http://inlmouse.github.io/ReplicaSKModel/"/>
    <id>http://inlmouse.github.io/ReplicaSKModel/</id>
    <published>2023-08-02T03:36:14.000Z</published>
    <updated>2023-08-02T06:27:46.625Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Sherrington–Kirkpatrick-Model"><a href="#Sherrington–Kirkpatrick-Model" class="headerlink" title="Sherrington–Kirkpatrick Model"></a>Sherrington–Kirkpatrick Model</h2><p>Sherrington–Kirkpatrick模型(以下简称SK模型)是相互作用范围为无限的Edwards-Anderson模型，是描述自旋玻璃(Spin-Glass)的平均场模型。该模型在形式上基本等价于玻尔兹曼机(Boltzmann Machine)。SK模型源于Sherrington和Kirkpatrick为了使平均场理论有效而对哈密顿量的修改。</p><p>这里讨论SK模型是为了阐述使用副本方法(Replica Method)的基本计算方法。因为尽管SK模型很简单，但它的理论分析非常丰富，涉及连续的现象、序参量、状态的超度量空间和随机分支过程等。</p><h2 id="理论模型"><a href="#理论模型" class="headerlink" title="理论模型"></a>理论模型</h2><p>我们先从哈密顿量开始，SK 模型的哈密顿量为：<br>\begin{equation}<br>    H = - \sum_{i&lt;j}J_{ij}S_{i}S_{j} - h \sum_{i}S_{i}. \label{equ:skhamilton}<br>\end{equation}<br>其中，$S_{i} \in \{-1,+1\}$，在 Ising 模型中表示微观磁矩的指向是上方还是下方；第一项求和遍及$N$格点格子中的所有自旋对，但并不双重计数；$h$ 是均匀磁场；耦合常数$J_{ij}\sim\mathcal{N}(\frac{J_{0}}{N}, \frac{J^{2}}{N})$为高斯随机变量：</p><p>\begin{equation}<br>    P(J_{ij}) = \frac{1}{J} \sqrt{\frac{N}{2\pi}} e^{-\frac{N}{2J^{2}} (J_{ij}-\frac{J_{0}}{N})^{2}}.<br>\end{equation}<br>宽度$J$与位置$i,j$的距离无关。注意到加入非零的相互作用均值$J_0$不会带来其他问题，只会让公式更复杂。这里对上式进行简化：</p><p>\begin{equation}\label{equ:JProb}<br>    P(J_{ij}) = \frac{1}{J} \sqrt{\frac{N}{2\pi}} e^{-\frac{NJ_{ij}^2}{2J^{2}}}. \tag{1}<br>\end{equation}</p><p>对于无限范围内平移不变的相互作用，平均场理论是严格的，因此我们期望有一种平均场类型的理论能够对这个模型进行有效求解。</p><p>由于这里的无序是淬火<sup><a href="#fn_1" id="reffn_1">1</a></sup>的，因此我们必须计算自由能并按照概率分布函数式\eqref{equ:JProb}对其求期望，即：<br>\begin{equation}<br>    f=-\frac{k_BT}{N}\prod_{i&lt;j}^N\mathbb{E}_{J_{ij}}\left[\ln Z(\{J_{ij}\},h,T)\right]=-\frac{k_BT}{N}\prod_{i&lt;j}^N\int {\mathrm d} J_{ij}P(J_{ij})\ln Z(\{J_{ij}\},h,T).<br>\end{equation}<br>其中$k_B$为玻尔兹曼常数，且有：<br>\begin{equation}\label{equ:skpartial}<br>    \begin{aligned}<br>        Z &amp; = \underbrace{\sum_{S_1=\pm 1}\cdots\sum_{S_N=\pm 1}}_\text{N重求和} \exp\left\{ \beta \left[ \sum_{i&lt;j}^N J_{ij}S_{i}S_{j} + h \sum_{i=1}^N S_{i}\right] \right\}\\<br>        &amp; = \sum_{S_1=\pm 1}\cdots\sum_{S_N=\pm 1}\exp\left\{ \beta \left[ \sum_{i&lt;j}^N J_{ij}S_{i}S_{j} + \frac{h}{2(N-1)} \sum_{i&lt;j}^N(S_i+S_j)\right] \right\}\\<br>        &amp; = \sum_{S_1=\pm 1}\cdots\sum_{S_N=\pm 1}\prod_{i&lt;j}^N \exp\left\{ \beta \left[ J_{ij}S_{i}S_{j} + \frac{h}{2(N-1)}(S_i+S_j)\right] \right\}.<br>    \end{aligned}\tag{2}<br>\end{equation}</p><blockquote id="fn_1"><sup>1</sup>. 如果我们从冷却出发，系统如果冷却得很快称为<strong>淬火</strong>，如果冷却得慢称为<strong>退火</strong>。如果我们从无序出发，如果一个系统依赖于不随时间演化的随机变量，那么它称为<strong>淬火无序</strong>；如果它依赖于随时间演化的随机变量，那么它称为<strong>退火无序</strong>。淬火无序的一个具体例子即是当温度变化时，磁性原子分布不会变化。<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><h2 id="基于Replica-Method的求解框架"><a href="#基于Replica-Method的求解框架" class="headerlink" title="基于Replica Method的求解框架"></a>基于Replica Method的求解框架</h2><p>这里我们可以使用狄拉克符号，引入矩阵$\boldsymbol{P}$，其矩阵元:<br>\begin{equation}<br>\langle S_i|\boldsymbol{P}|S_j\rangle = \exp\left\{ \beta \left[ J_{ij}S_{i}S_{j} + \frac{h}{2(N-1)}(S_i+S_j)\right] \right\}.<br>\end{equation}</p><p>注意到$S_i,S_j$只能取值$\pm 1$，因此矩阵$\boldsymbol{P}$为如下的$2\times 2$的矩阵：<br>\begin{equation}<br>    \begin{aligned}<br>        \boldsymbol{P} &amp;=<br>        \begin{pmatrix}\langle 1|\boldsymbol{P}|1\rangle &amp; \langle 1|\boldsymbol{P}|-1\rangle\\<br>        \langle -1|\boldsymbol{P}|1\rangle &amp; \langle -1|\boldsymbol{P}|-1\rangle<br>        \end{pmatrix}\\<br>        &amp;= \begin{pmatrix} e^{\beta J_{ij}+h\beta/(N-1)} &amp; e^{-\beta J_{ij}}\\<br>        e^{-\beta J_{ij}} &amp; e^{\beta J_{ij}-h\beta/(N-1)}<br>        \end{pmatrix}.<br>    \end{aligned}<br>\end{equation}</p><p>那么配分函数\eqref{equ:skpartial}式，可以改写为<sup><a href="#fn_2" id="reffn_2">2</a></sup>：<br>\begin{equation}<br>    \begin{aligned}<br>        Z &amp; = \sum_{S_1=\pm 1}\cdots\sum_{S_N=\pm 1}\prod_{i&lt;j}^N \langle S_i|\boldsymbol{P}|S_j\rangle\\<br>        &amp; =\text{Tr}_{\{S_i\}}\exp[-\beta H(\{J_{ij}\},\{S_{i}\})].<br>    \end{aligned}<br>\end{equation}<br>事实上，计算至此，由于随机变量$J_{ij}$的存在，已经无法继续计算。<strong>为了交换对于耦合常数$J_{ij}$求期望和对所有自旋位形$\{S_{i}\}$求迹的顺序，我们才使用replica方法。</strong><sup><a href="#fn_3" id="reffn_3">3</a></sup><br>因此，配分函数的$n$次幂为：<br>\begin{equation}<br>Z^n = \prod_{\alpha=1}^n\text{Tr}_{\{S_i^\alpha\}}\exp[-\beta H(\{J_{ij}\},\{S_i^\alpha\})] = \text{Tr}_{\{S_i^\alpha\}}\exp\left\{ \beta\sum_{\alpha=1}^n\left[ \sum_{i&lt;j}J_{ij}S_i^\alpha S_j^\alpha + h \sum_{i}S_i^\alpha\right] \right\}.<br>\end{equation}</p><p>注意到<sup><a href="#fn_4" id="reffn_4">4</a></sup>：<br>\begin{equation}<br>\mathbb{E}_{J_{ij}}\left[\exp(\beta J_{ij}\sum_{\alpha=1}^nS_i^\alpha S_j^\alpha)\right]=\exp\left[ \beta^2J^2/2N\sum_{\alpha,\gamma=1}^n S_i^\alpha S_j^\alpha S_i^\gamma S_j^\gamma\right].<br>\end{equation}</p><p>那么显然：<br>\begin{equation}<br>\mathbb{E}_{}\left[Z^n\right] = \text{Tr}_{\{S_i^\alpha\}}\exp\left[ \frac{\beta^2J^2}{2N}\sum_{i&lt;j}^N \sum_{\alpha,\gamma=1}^n S_i^\alpha S_j^\alpha S_i^\gamma S_j^\gamma + \beta h \sum_{i=1}^N\sum_{\alpha=1}^n S_i^\alpha\right].<br>\end{equation}</p><p>我们得到了不同副本之间的自旋的有效相互作用为：<br>\begin{equation}\label{equ:freeengrep}<br>    \begin{aligned}<br>        -\beta f &amp;= \lim_{n\to 0}\frac1n[\mathbb{E}_{}[Z^n]-1]\\<br>        &amp;=\lim_{n\to 0, N\to\infty}\frac{1}{nN}\left\{\text{Tr}_{\{S_i^\alpha\}}\exp\left[ \frac{\beta^2J^2}{2N}\sum_{i&lt;j}^N \sum_{\alpha,\gamma=1}^n S_i^\alpha S_j^\alpha S_i^\gamma S_j^\gamma + \beta h \sum_{i=1}^N\sum_{\alpha=1}^n S_i^\alpha\right]-1\right\}\\<br>        &amp;\approx \lim_{n\to 0, N\to\infty}\frac{1}{nN}\left[\text{Tr}_{\{S_i^\alpha\}}\exp(\mathcal{H})-1\right].<br>    \end{aligned}<br>    \tag{3}<br>\end{equation}<br><strong>事实上，至此，通用的计算就已经结束。下面至文末的计算是基于某一个具体模型的各种分析技巧，除了正交解耦技巧外，其他不具备普适性。</strong></p><blockquote id="fn_2"><sup>2</sup>. 如果$J_{ij}$回退Ising模型，视为常数而非随机变量，那么：<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><p>\begin{equation}<br>    \begin{aligned}<br>        Z &amp; = \sum_{S_1=\pm 1}\cdots\sum_{S_N=\pm 1}\prod_{i&lt;j}^N \langle S_i|\boldsymbol{P}|S_j\rangle\\<br>        &amp; =\sum_{S_1}\langle S_1|\boldsymbol{P}^{N(N-1)/2}|S_1\rangle\\<br>        &amp; = \text{Tr}\boldsymbol{P}^{N(N-1)/2}.<br>    \end{aligned}<br>\end{equation}<br>这里求矩阵的迹Tr是严格的，下文中不严格的复用了这个表示，指代对所有可能求和。</p><blockquote id="fn_3"><sup>3</sup>. 这便是自平均性的体现，当$N\gg \beta J\gg 1$时，以$1/N$为基展开，则有：<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><p>\begin{equation}<br>\mathbb{E}_{}[Z^n] = \mathbb{E}_{}[Z]^n.<br>\end{equation}</p><blockquote id="fn_4"><sup>4</sup>. 这里使用恒等式：<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><p>\begin{equation}\label{equ:guassianintreq}<br>    \sqrt{\frac{a}{2\pi}}\int_{\mathbb{R}}e^{-\frac12(ax^2-\lambda\sqrt{2}x)}{\mathrm d} x = e^{\lambda^2/4a}. \tag{4}<br>\end{equation}</p><h2 id="继续求解"><a href="#继续求解" class="headerlink" title="继续求解"></a>继续求解</h2><p>再次注意到：<br>\begin{equation}<br>    \begin{aligned}<br>        \sum_{i&lt;j}^N \sum_{\alpha,\gamma=1}^n S_i^\alpha S_j^\alpha S_i^\gamma S_j^\gamma  &amp;= \frac12\sum_{i&lt;j}^N \sum_{\alpha\neq\gamma}^n S_i^\alpha S_j^\alpha S_i^\gamma S_j^\gamma + \frac{nN(N-1)}{2} \\<br>        &amp; = \frac12\sum_{\alpha\neq\gamma}^n\left(\sum_{i=1}^N S_i^\alpha S_i^\gamma\right)^2+\frac{nN(N-1)}{2}.<br>    \end{aligned}<br>\end{equation}</p><p>因此有：<br>\begin{equation}<br>\mathcal{H} = \frac{\beta^2J^2nN}{4} + \frac{\beta^2J^2}{2N}\sum_{\alpha,\gamma=1}^n\left(\sum_{i=1}^N S_i^\alpha S_i^\gamma\right)^2 + \beta h\sum_{i=1}^N\sum_{\alpha=1}^n S_i^\alpha.<br>\end{equation}<br>其中，副本的相互总用仅在第二项中出现。由于$\mathcal{H}$中不同位置的自旋总是耦合在一起的，因此，式\eqref{equ:freeengrep}中的求迹运算极端复杂(矩阵不是对角阵)。我们可以使用对角化技术，也即引入一组虚拟变量来消除这些耦合。对式\eqref{equ:guassianintreq}中的$a=N,\lambda = \sqrt{2}\beta J\sum_{i=1}^n S_i^\alpha S_i^\gamma$，有恒等式：<br>\begin{equation}<br>    \sqrt{\frac{N}{2\pi}}\int_{\mathbb{R}}e^{-\frac12Nx_{\alpha\gamma}^2-\beta J\left(\sum_{i=1}^N S_i^\alpha S_i^\gamma\right) x_{\alpha\gamma} }{\mathrm d} x_{\alpha\gamma} = e^{\beta^2J^2\left(\sum_{i=1}^N S_i^\alpha S_i^\gamma\right)^2/2N}.<br>\end{equation}</p><p>这直接导致：<br>\begin{equation}<br>    \begin{aligned}<br>        e^{\mathcal{H}} &amp; = \exp\left[\frac{\beta^2J^2nN}{4} + \beta h\sum_{i=1}^N\sum_{\alpha=1}^n S_i^\alpha\right]\prod_{\alpha,\gamma=1}^n \sqrt{\frac{N}{2\pi}}\int_{\mathbb{R}}e^{-\frac12Nx_{\alpha\gamma}^2-\beta J\left(\sum_{i=1}^N S_i^\alpha S_i^\gamma\right) x_{\alpha\gamma} }{\mathrm d} x_{\alpha\gamma}<br>    \end{aligned}<br>\end{equation}<br>中不同位置的自旋不再耦合在一起，并且上式中求迹就化为对一个位置上(例如$i$)的自旋求迹，并有$N$项乘积。因此，忽略位置标记$i$，研究不同副本之间的自旋耦合上。</p><p>这里使用最速下降法(Method of Steepest Descent，细节可以参考<a href="https://inlmouse.github.io/SteepestDescent/">上一篇博文</a>)，处理上述积分，可以得到：<br>\begin{equation}<br>    \text{Tr}e^{\mathcal{H}}=\exp\left[\frac{\beta^2J^2N}{4}[n-n(n-1)q^2]\right] \left[\text{Tr}\exp\left\{ \beta h\sum_{\alpha=1}^nS^\alpha + \beta^2J^2q\sum_{\alpha,\gamma=1}^nS^\alpha S^\gamma\right\}\right]^N<br>\end{equation}<br>其中$q$为该模型的序参量<sup><a href="#fn_5" id="reffn_5">5</a></sup>。</p><p>再次使用式\eqref{equ:guassianintreq}技巧解耦自旋：<br>\begin{equation}<br>\exp\left[ \beta^2J^2q\sum_{\alpha,\gamma=1}^nS^\alpha S^\gamma \right] = \exp\left[-\frac{n\beta^2J^2q}{2}\right]\int_{\mathbb{R}}e^{-z^2/2}\exp\left[z\beta \sqrt{q}\sum_{\alpha=1}^nS^\alpha\right]\frac{\operatorname{d} z}{\sqrt{2\pi}},<br>\end{equation}<br>得到：<br>\begin{equation}<br>    \begin{aligned}<br>        \text{Tr}e^{\mathcal{H}}&amp;=e^P\left[\text{Tr}\int_{\mathbb{R}}e^{-z^2/2}\exp\left[(z\beta J\sqrt{q} + \beta h)\sum_{\alpha=1}^nS^\alpha\right]\frac{\operatorname{d} z}{\sqrt{2\pi}}\right]^N\\<br>        &amp;=e^P\left[\int_{\mathbb{R}}e^{-z^2/2}(2\cosh\mathcal{Z})^n\frac{\operatorname{d} z}{\sqrt{2\pi}}\right]^N\\<br>        &amp; = e^P\exp\left[N\ln\int_{\mathbb{R}}e^{-z^2/2}(2\cosh\mathcal{Z})^n\frac{\operatorname{d} z}{\sqrt{2\pi}}\right]\\<br>        &amp;\approx  e^P\exp\left[N\ln\int_{\mathbb{R}}e^{-z^2/2}(1+n\ln (2\cosh\mathcal{Z}))\frac{\operatorname{d} z}{\sqrt{2\pi}}\right]\\<br>        &amp;\approx e^P\exp\left[nN\ln\int_{\mathbb{R}}e^{-z^2/2}\ln(2\cosh\mathcal{Z})\frac{\operatorname{d} z}{\sqrt{2\pi}}\right].<br>    \end{aligned}<br>\end{equation}<br>其中$P=N\beta^2J^2(n-n(n-1)q^2)/4-nN\beta^2J^2q/2$，$\mathcal{Z} = z\beta J\sqrt{q}+\beta h$。将上述结果带入\eqref{equ:freeengrep}式，并取极限：<br>\begin{equation}<br>    \beta f = -\frac{\beta^2J^2}{4}(1-q)^2-\int_{\mathbb{R}}e^{-z^2/2}\ln(2\cosh\mathcal{Z})\frac{\operatorname{d} z}{\sqrt{2\pi}}.<br>\end{equation}<br>至此，理论计算求解结束。</p><blockquote id="fn_5"><sup>5</sup>. 该序参量由下式给出：<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><p>\begin{equation}<br>    \begin{aligned}<br>        q = \langle\langle S^\alpha S^\gamma\rangle\rangle &amp;= \left. \frac{\text{Tr}S^\alpha S^\gamma e^{\mathcal{H}}}{\text{Tr}e^{\mathcal{H}}}\right|_{n\to 0}\\<br>        &amp; = \left.\frac{\int_{\mathbb{R}}e^{-z^2/2}(2\sinh\mathcal{Z})^2(2\cosh\mathcal{Z})^{n-2}\frac{\operatorname{d} z}{\sqrt{2\pi}}}{\int_{\mathbb{R}}e^{-z^2/2}(2\cosh\mathcal{Z})^n\frac{\operatorname{d} z}{\sqrt{2\pi}}}\right|_{n\to 0}\\<br>        &amp; = \int_{\mathbb{R}}e^{-z^2/2}\tanh^2\mathcal{Z}\frac{\operatorname{d} z}{\sqrt{2\pi}}.<br>    \end{aligned}<br>\end{equation}</p>]]></content>
    
    
    <summary type="html">Solving the analytical expression of the SK Model using the replica method.(in Chinese)</summary>
    
    
    
    <category term="Statistical Physics" scheme="http://inlmouse.github.io/categories/Statistical-Physics/"/>
    
    
    <category term="Replica Method" scheme="http://inlmouse.github.io/tags/Replica-Method/"/>
    
    <category term="SK Model" scheme="http://inlmouse.github.io/tags/SK-Model/"/>
    
    <category term="Boltzmann Machine" scheme="http://inlmouse.github.io/tags/Boltzmann-Machine/"/>
    
  </entry>
  
  <entry>
    <title>Method of Steepest Descent in Numerical Analysis</title>
    <link href="http://inlmouse.github.io/SteepestDescent/"/>
    <id>http://inlmouse.github.io/SteepestDescent/</id>
    <published>2023-07-02T03:43:17.000Z</published>
    <updated>2023-08-02T06:20:15.417Z</updated>
    
    <content type="html"><![CDATA[<h2 id="最速下降法"><a href="#最速下降法" class="headerlink" title="最速下降法"></a>最速下降法</h2><p>最速下降法<sup><a href="#fn_1" id="reffn_1">1</a></sup>，也称为鞍点法(saddle-point method)，作为Laplace方法的在复分析中的应用[1,2]，辅之以留数定理，以找出一个过最速下降点的contour<sup><a href="#fn_2" id="reffn_2">2</a></sup>的曲线积分，用来取代原有的复数空间的contour积分。该方法是估计型如下的积分：<br>\begin{equation}<br>    I(\lambda) = \int_C f(z)e^{\lambda g(z)}{\mathrm d} z.<br>\end{equation}<br>其中$C$为contour，对于$\lambda \rightarrow \infty$, $f(z),g(z)$为$z$的解析函数。由于被积函数解析，contour $C$可以被另一条性质更好的contour $C’$而不改变积分。特别的，我们选取contour $C’$使得$\Im(g(z))$为常数，剩余部分的估计可以退化为Laplace方法进行估计。</p><p>也即是：<br>\begin{equation}<br>    I(\lambda) = \int_C f(z)e^{\lambda g(z)}{\mathrm d} z = e^{i\lambda \Im(g(z))}\int_{C’}f(z)e^{\lambda \Re(g(z))} {\mathrm d} z.<br>\end{equation}</p><p>由于该方法中，已经利用另一条通过最速下降的鞍点来取代原有的contour积分，经过变数变换后就会变得有如拉普拉斯方法。因此，我们可以透过这新的contour，找到原本的积分的渐进近似解，而这将大大的简化整个计算，得名最速下降法。就好像原本的路径像是在蜿蜒的山路开车，而新的路径就像干脆绕过这座山开，反正目的只是到达目的地而已，留数定理已经帮我们把中间的差都算好了。</p><blockquote id="fn_1"><sup>1</sup>. 尤其注意该方法不同于优化算法中的梯度下降法(gradient descent)。<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. 翻译为”路径”的话，会与path integral相冲，因此这里还是以英文原字称呼。<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>考察积分：<br>\begin{equation}<br>    I(\lambda) = \int_0^1 \cos(\lambda x)\ln x{\mathrm d} x.<br>\end{equation}<br>在$\lambda \to \infty$的渐进数值特性。</p><p>考虑：<br>\begin{equation}<br>    J(\lambda) = \int_0^1 e^{i\lambda z}\ln z{\mathrm d} z.<br>\end{equation}<br>显然$I(\lambda) = \Re(J(\lambda))$。考虑复平面上三点：$P(1,0), Q(0, R),S(1,R)$，那么由柯西积分定理，原积分路径$OP$可以更变为$OQSP$。</p><p>接下来我们令$R\to\infty$，那么积分中$QS$段中由于被积函数$e^{-\lambda R}$项的存在，其贡献趋近于0。那么在$OQ$段记$z=is$，$SP$段记$z=1+is$：<br>\begin{equation}<br>    I(\lambda) = \Re \left[\int_0^\infty e^{-\lambda s}\ln(is){\mathrm d} is\right] -  \Re \left[\int_0^\infty e^{-\lambda s + i\lambda }\ln(1+is){\mathrm d} is\right].<br>\end{equation}<br>其中右边第二个积分的符号是负的因为$SP$向下遍历。</p><p>注意到有恒等式：<br>\begin{equation}<br>    \ln(is) = \ln(se^{i\frac{\pi}{2}}) = \ln(s) + i\frac{\pi}{2}.<br>\end{equation}</p><p>那么：<br>\begin{equation}<br>    \Re \left[\int_0^\infty e^{-\lambda s}\ln(is){\mathrm d} is\right] = \int_0^\infty e^{-\lambda s}\Re\left( i \ln(s) - \frac{\pi}{2} \right){\mathrm d} s = -\frac{\pi}{2\lambda}.<br>\end{equation}</p><p>同理：<br>\begin{equation}<br>    \begin{aligned}<br>        \Re \left[\int_0^\infty e^{-\lambda s + i\lambda }\ln(1+is){\mathrm d} is\right] &amp; = \Re \left[e^{i\lambda} \int_0^\infty e^{-\lambda s}i\ln(1+is){\mathrm d} s\right]\\<br>        &amp;\overset{\ln(1+is)\approx is}{\approx} \Re (e^{i\lambda})\int_0^\infty e^{-\lambda s}s{\mathrm d} s\\<br>        &amp;=-\frac{\cos\lambda}{\lambda^2}.<br>    \end{aligned}<br>\end{equation}</p><p>综上二式，得到：<br>\begin{equation}\label{equ:approxmsd}<br>     I(\lambda) = \int_0^1 \cos(\lambda x)\ln x{\mathrm d} x \approx -\frac{\pi}{2\lambda}+\frac{\cos\lambda}{\lambda^2}.<br>\end{equation}</p><img src="/SteepestDescent/appA.png" class width="400" height="250"> <center> 在$2\leq \lambda \leq 20$的逼近效果。实线为估计式的数值结果，虚线为原积分的数值结果。</center><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]: Bender C M, Orszag S A. Advanced mathematical methods for scientists and engineers I: Asymptotic methods and perturbation theory[M]. Springer Science &amp; Business Media, 1999.<br>[2]: Jones L M. Introduction to mathematical methods of physics[J]. 1979.</p>]]></content>
    
    
    <summary type="html">An introduction of an special extension of Laplace&#39;s method for approximating an integral.(in Chinese)</summary>
    
    
    
    <category term="Numerical Analysis" scheme="http://inlmouse.github.io/categories/Numerical-Analysis/"/>
    
    
    <category term="Math" scheme="http://inlmouse.github.io/tags/Math/"/>
    
    <category term="Complex Analysis" scheme="http://inlmouse.github.io/tags/Complex-Analysis/"/>
    
  </entry>
  
  <entry>
    <title>Stable Diffusion LoRA Model, A First Try</title>
    <link href="http://inlmouse.github.io/sdchunmomo/"/>
    <id>http://inlmouse.github.io/sdchunmomo/</id>
    <published>2023-04-16T02:00:00.000Z</published>
    <updated>2023-04-16T04:37:48.776Z</updated>
    
    <content type="html"><![CDATA[<p>After the craziest March in the recent history of AI development, everyone has witnessed the powerful capabilities of generative AI. In the field of computer vision, there have been countless AIGC (AI-generated content) methods and tools, such as GigaGAN, Stable Diffusion, DALL·E2, MidJourney, LoRA, and many more, whose results are stunning and awe-inspiring.</p><p>I had planned to write a technical report, sort-out and studying <strong>generative models from the perspective of probability theory</strong>. With a rigorous attitude, I intended to first experiment with the capabilities of these new technologies before writing this note. However, things spiraled out of control, and even though I had already played around with things like StyleTransfer 8 years ago, I still found them extremely fascinating.</p><p>I spent some time training a Stable Diffusion RoLA model(<a href="https://civitai.com/models/41309/chunmomo">civitai link</a>), this post will demonstrate some technical details.</p><img src="/sdchunmomo/00074-383632874.png" class width="512" height="768"> <center> The generated image sample, which have an extremely high level of completion to the human eyes.</center><h2 id="LoRA-Low-rank-Adaptation-for-Fast-Text-to-Image-Diffusion-Fine-tuning"><a href="#LoRA-Low-rank-Adaptation-for-Fast-Text-to-Image-Diffusion-Fine-tuning" class="headerlink" title="LoRA: Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning"></a>LoRA: Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning</h2><p>Many people like to fine-tune stable diffusion models to fit their needs and generate higher fidelity images. However, the fine-tuning process is very slow, and finding a good balance between the number of steps and the quality of the results is not easy. Furthermore, the final output of a fully fine-tuned model is very large. Some people use textual inversion as an alternative method, but clearly this is not optimal: textual inversion only creates a small word embedding, resulting in a final image that is not as good as that produced by a fully fine-tuned model.</p><p>As the results, LoRA came out. <a href="https://arxiv.org/abs/2106.09685">LoRA</a> stands for Low-Rank Adaptation, a mathematical technique to deal with the problem of fine-tuning LLMs and reduce the number of parameters that are trained. The idea is very simple. Just like ResNet, training residuals instead of the model itself, which is indeed the work of Microsoft Research. By applying LoRA to cross-attention layers, it links image representations with prompts describing them. </p><img src="/sdchunmomo/latent-diffusion.png" class width="800" height="397"> <center> Latent Diffusion Models(https://arxiv.org/abs/2112.10752), which is used to fine-turn high resolution images in stable diffusion. The yellow blocks are cross-attention layers.</center><h2 id="The-Self-trained-Model-Some-Details"><a href="#The-Self-trained-Model-Some-Details" class="headerlink" title="The Self-trained Model: Some Details"></a>The Self-trained Model: Some Details</h2><p>This LoRA model is trained by a non-NSFW subset of the photobook of the famous cosplayer Chunmomo(蠢沫沫). <strong>So it won’t work well with NSFW prompts</strong>.</p><p>I manually selected 96 face images and 259 bust/full body images, as shown in the figure below. The selection of face concept set is to use images with no occlusion and a single background as much as possible.<br><img src="/sdchunmomo/face.png" class width="880" height="408"> </p><center> Face concept set.</center><img src="/sdchunmomo/body.png" class width="880" height="408"> <center> Body concept set.</center><p>The training process has gone through 50 epochs, and each concept have trained 10 times. Image resolution 512x768, batch size 1(RTX 2080 8GB). </p><img src="/sdchunmomo/loss.png" class width="1038" height="336"> <p>After 19.5 hours of waiting, I finally got the LoRA model. </p><h2 id="Some-Generated-Image-Results"><a href="#Some-Generated-Image-Results" class="headerlink" title="Some Generated Image Results"></a>Some Generated Image Results</h2><img src="/sdchunmomo/00071-3836818767.png" class width="512" height="768"> <center> Key Prompts: white_dress, sailor_collar, sailor_dress, polka_dot, straw_hat, v_arm</center><img src="/sdchunmomo/00062-6173687.png" class width="512" height="768"> <center> Key Prompts: goggles, goggles_on_head, shorts, socks, long_sleeves, street, outdoor</center><img src="/sdchunmomo/00067-1673675340.png" class width="512" height="768"> <center> Key Prompts: goggles, goggles_on_head, shorts, socks, short_sleeves, ocean, water, twin_braids</center><img src="/sdchunmomo/00043-281637387.png" class width="512" height="768"> <center> Key Prompts: sailor_collar, sailor_dress, pleated_skirt, ponytail, cross_arm, sitting</center><h2 id="Important-Statement"><a href="#Important-Statement" class="headerlink" title="Important Statement"></a>Important Statement</h2><p><strong>This model is for academic research use only, commercial use is strictly prohibited. Use of this model to generate NSFW content, as well as any use that compromises individual rights and privacy is prohibited.</strong></p>]]></content>
    
    
    <summary type="html">Technical details of a self-trained LoRA model. 一个自己训练的LoRA模型的技术细节。(in English)</summary>
    
    
    
    <category term="AIGC" scheme="http://inlmouse.github.io/categories/AIGC/"/>
    
    
    <category term="Stable Diffusion" scheme="http://inlmouse.github.io/tags/Stable-Diffusion/"/>
    
    <category term="LoRA" scheme="http://inlmouse.github.io/tags/LoRA/"/>
    
  </entry>
  
  <entry>
    <title>第十三届全国大学生数学竞赛决赛(数学类高年级组)第九题赏析</title>
    <link href="http://inlmouse.github.io/13thNUMCFQ9/"/>
    <id>http://inlmouse.github.io/13thNUMCFQ9/</id>
    <published>2023-03-30T00:38:09.000Z</published>
    <updated>2023-03-30T09:03:57.283Z</updated>
    
    <content type="html"><![CDATA[<p>本题目是于2023年3月在中国上海华东师范大学举办的第十三届全国大学生数学竞赛决赛(数学类高年级组)中的第九题，题目考察范围广泛，需要应用灵活运用矩阵求导、概率论、高维高斯随机向量的性质、条件期望与重期望公式以及Fubini定理等少量积分技巧来解决该问题。问题本身实际为机器学习的背景，稍作修改可以变形为更为真实的应用场景，是难得值得该领域研究者品鉴的好题目。</p><p>官方给出的答案太过于简略，这里给出一个详细解答。</p><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>记$p\geq 2$是整数，$\beta$是$p$维末知参数，$\boldsymbol{X}$是$p$维随机向量服从正态分布$\mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma})$。其中$\boldsymbol{\Sigma}$是$p$阶对称正定矩阵，随机误差$\varepsilon$是1维随机变量,服从正态分布$\mathcal{N}(0, \sigma^2)$，且与$\boldsymbol{X}$独立，其中$\sigma^2&gt;0$。记$\text{sgn}(\cdot)$为符号函数，$Y = \text{sgn}(\boldsymbol{X}^{T}\beta_0+\varepsilon)$是1维随机变量。定义：<br>\begin{equation}<br>    \beta^* = \text{argmin}_{\beta\in\mathbb{R}^p}\mathbb{E}\left[(Y-\boldsymbol{X}^T\beta)^2\right].<br>\end{equation}</p><ol><li>请给出$\beta^*$的显式表达式。</li><li>若$F(\cdot)$为$\varepsilon$的分布函数，证明：<br>\begin{equation}<br> \text{Cov}(\boldsymbol{X}, Y) = -2\text{Cov}(\boldsymbol{X}, F(-\boldsymbol{X}^T\beta_0)).<br>\end{equation}</li><li>$\forall \alpha,\beta\in \mathbb{R}^p$，证明：<br>\begin{equation}<br> \mathbb{E}\left[\alpha^T\boldsymbol{X}|\beta^T\boldsymbol{X}\right] = (\alpha^T\boldsymbol{\Sigma}\beta)\left((\beta^T\boldsymbol{\Sigma}\beta)^{-1}\beta^T\boldsymbol{X}\right).<br>\end{equation}</li><li>证明$\beta^*$与$\beta_0$成比例，且要求找到比例因子的显式表达式。</li></ol><h2 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h2><ul><li>首先计算：</li></ul><p>\begin{aligned}<br>            \mathbb{E}\left[(Y-\boldsymbol{X}^T\beta)^2\right] &amp; = \mathbb{E}\left[1 - 2Y\boldsymbol{X}^T\beta + \beta^T\boldsymbol{X}\boldsymbol{X}^T\beta\right]\\<br>            &amp; = 1- 2\mathbb{E}\left[Y\boldsymbol{X}^T\right]\beta + \beta^T\mathbb{E}\left[\boldsymbol{X}\boldsymbol{X}^T\right]\beta\\<br>            &amp; \overset{\mathbb{E}\left[\boldsymbol{X}\right]=\boldsymbol{0}}{=} 1 - 2\text{Cov}(\boldsymbol{X},Y)\beta + \beta^T\boldsymbol{\Sigma}\beta.<br>\end{aligned}</p><p>再对$\beta$求导，直接得到：</p><p>\begin{aligned}<br>            \boldsymbol{0}  = \frac{\partial \left[1 - 2\text{Cov}(\boldsymbol{X},Y)\beta + \beta^T\boldsymbol{\Sigma}\beta\right]}{\partial \beta} &amp; = -2\text{Cov}(\boldsymbol{X},Y) + (\boldsymbol{\Sigma}+\boldsymbol{\Sigma}^T)\beta\\<br>            \beta^* &amp;= \boldsymbol{\Sigma}^{-1}\text{Cov}(\boldsymbol{X},Y).<br>\end{aligned}</p><ul><li>其次注意到：</li></ul><p>\begin{aligned}<br>            \text{Cov}(\boldsymbol{X},Y) &amp; \overset{\text{重期望公式}}{=} \mathbb{E}_{\varepsilon}\left[\mathbb{E}_{\boldsymbol{X}}\left[\boldsymbol{X}\text{sgn}(\boldsymbol{X}^{T}\beta_0+\varepsilon)|\varepsilon\right]\right]\\<br>            &amp; \overset{\text{Fubini定理}}{=} \mathbb{E}_{\boldsymbol{X}}\left[\mathbb{E}_{\varepsilon}\left[\boldsymbol{X}\text{sgn}(\boldsymbol{X}^{T}\beta_0+\varepsilon)|\varepsilon\right]\right]\\<br>            &amp; = \mathbb{E}_{\boldsymbol{X}}\left[F(-\boldsymbol{X}^T\beta_0)\cdot -\boldsymbol{X} + (1-F(-\boldsymbol{X}^T\beta_0))\boldsymbol{X}\right]\\<br>            &amp; = \mathbb{E}_{\boldsymbol{X}}\left[-2F(-\boldsymbol{X}^T\beta_0)\boldsymbol{X} +\boldsymbol{X}\right]\\<br>            &amp; = -2\text{Cov}(F(-\boldsymbol{X}^T\beta_0), \boldsymbol{X}).<br>\end{aligned}</p><ul><li>注意到两个高斯随机变量组成的二维随机向量，依旧服从高斯分布：</li></ul><p>\begin{equation}<br>    \begin{pmatrix}<br>        \alpha^T\boldsymbol{X} \\<br>        \beta^T\boldsymbol{X}<br>    \end{pmatrix}<br>    \sim\mathcal{N}<br>    \left(<br>    \begin{pmatrix}<br>        0 \\<br>        0<br>    \end{pmatrix},<br>    \begin{pmatrix}<br>        \alpha^T\boldsymbol{\Sigma}\alpha &amp;  \alpha^T\boldsymbol{\Sigma}\beta\\<br>        \beta^T\boldsymbol{\Sigma}\alpha &amp; \beta^T\boldsymbol{\Sigma}\beta<br>    \end{pmatrix}<br>    \right).<br>\end{equation}<br> 那么其条件期望可以直接计算<sup><a href="#fn_1" id="reffn_1">1</a></sup>得到：<br>\begin{equation}\label{equ:condexpexe9}<br>        \mathbb{E}\left[\alpha^T\boldsymbol{X}|\beta^T\boldsymbol{X}\right] = (\alpha^T\boldsymbol{\Sigma}\beta)\left((\beta^T\boldsymbol{\Sigma}\beta)^{-1}\beta^T\boldsymbol{X}\right). \tag{1}<br>\end{equation}</p><ul><li>根据\ref{equ:condexpexe9}式，令$\alpha=\text{diag}(1,\cdots,1) = \boldsymbol{E}$为单位阵有：</li></ul><p>\begin{aligned}<br>        \mathbb{E}\left[\boldsymbol{X} F(-\boldsymbol{X}^T\beta_0)\right] &amp;\overset{\text{重期望公式}}{=} \mathbb{E}\left[\mathbb{E}\left[\boldsymbol{X} F(-\boldsymbol{X}^T\beta_0)|\boldsymbol{X}^T\beta_0\right]\right]\\<br>            &amp; = \boldsymbol{\Sigma}\beta_0(\beta_0^T\boldsymbol{\Sigma}\beta_0)^{-1}\text{Cov}(\boldsymbol{X}^T \beta_0, F(-\boldsymbol{X}^T\beta_0)).<br>\end{aligned}</p><p>综上几问的结果我们有：</p><p>\begin{aligned}<br>        \beta^* &amp; = \boldsymbol{\Sigma}^{-1}\text{Cov}(\boldsymbol{X},Y) \\<br>        &amp; = -2\boldsymbol{\Sigma}^{-1}\text{Cov}(F(-\boldsymbol{X}^T\beta_0), \boldsymbol{X})\\<br>        &amp; = -2\boldsymbol{\Sigma}^{-1}\boldsymbol{\Sigma}\beta_0(\beta_0^T\boldsymbol{\Sigma}\beta_0)^{-1}\text{Cov}(\boldsymbol{X}^T \beta_0, F(-\boldsymbol{X}^T\beta_0))\\<br>        &amp; = -2\left[ (\beta_0^T\boldsymbol{\Sigma}\beta_0)^{-1}\text{Cov}(\boldsymbol{X}^T \beta_0, F(-\boldsymbol{X}^T\beta_0)) \right]\beta_0\\<br>        &amp; = c\beta_0.<br>\end{aligned}</p><p>其中，记$f$为$\varepsilon$的密度函数，令$\boldsymbol{X}^T\beta_0 = Z\sim\mathcal{N}(0,\sigma’^2)$，$\sigma’^2 = \beta_0^T\boldsymbol{\Sigma}\beta_0$有密度函数$\rho(x)$。注意到$x\rho(x) = -\sigma’^2\rho’(x)$：</p><p>\begin{aligned}<br>        \mathbb{E}\left[ZF(-Z)\right] &amp; = \int_{\mathbb{R}}xF(-x)\rho(x)\text{d} x = -\sigma’^2\int_{\mathbb{R}}F(-x)\text{d} \rho(x)\\<br>        &amp; \overset{\text{分部积分}}{=} -\sigma’^2\underbrace{\left.F(-x)\rho(x)\right|_{-\infty}^{+\infty}}_{=0} - \sigma’^2\int_{\mathbb{R}}f(-x)\rho(x)\text{d} x = - \sigma’^2\mathbb{E}\left[f(-Z)\right]\\<br>        &amp; = -\int_{\mathbb{R}}\frac{\sigma’^2}{2\pi\sigma\sigma’}\exp\left(-\frac{x^2}{2}(1/\sigma^2+1/\sigma’^2)\right)\text{d} x\\<br>        &amp; = -\frac{\sigma’^2}{\sqrt{2\pi(\sigma^2+\sigma’^2)}}.<br>\end{aligned}</p><p>因此根据上式，得到比例常数$c$：<br>\begin{aligned}<br>        c &amp;= -2(\beta_0^T\boldsymbol{\Sigma}\beta_0)^{-1}\text{Cov}(\boldsymbol{X}^T \beta_0, F(-\boldsymbol{X}^T\beta_0)) = -2(\beta_0^T\boldsymbol{\Sigma}\beta_0)^{-1}\mathbb{E}\left[\boldsymbol{X}^T \beta_0F(-\boldsymbol{X}^T\beta_0)\right]\\<br>        &amp; = 2(\beta_0^T\boldsymbol{\Sigma}\beta_0)^{-1}\frac{(\beta_0^T\boldsymbol{\Sigma}\beta_0)}{\sqrt{2\pi(\sigma^2+\beta_0^T\boldsymbol{\Sigma}\beta_0)}}\\<br>        &amp; = \sqrt{\frac{2}{\pi(\sigma^2+\beta_0^T\boldsymbol{\Sigma}\beta_0)}}.<br>\end{aligned}</p><blockquote id="fn_1"><sup>1</sup>. 该公式的证明过程可以参考<a href="https://stats.stackexchange.com/questions/30588">这里</a>.  <a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote>]]></content>
    
    
    <summary type="html">The Solution of the Final Round of the 13th National College Student Mathematics Competition (Mathematics Category, Senior Group) Q9.(in Chinese)</summary>
    
    
    
    <category term="Mathematical Competition" scheme="http://inlmouse.github.io/categories/Mathematical-Competition/"/>
    
    
    <category term="Math" scheme="http://inlmouse.github.io/tags/Math/"/>
    
    <category term="Matrix Analysis" scheme="http://inlmouse.github.io/tags/Matrix-Analysis/"/>
    
    <category term="Probability Theory" scheme="http://inlmouse.github.io/tags/Probability-Theory/"/>
    
  </entry>
  
  <entry>
    <title>Fisher Information and Its Application</title>
    <link href="http://inlmouse.github.io/fisherinfo/"/>
    <id>http://inlmouse.github.io/fisherinfo/</id>
    <published>2023-02-13T06:23:38.000Z</published>
    <updated>2023-02-13T07:01:24.018Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍Fisher信息(矩阵)的定义、性质以及其计算与一些简单应用。</p><div class="pdf-container" data-target="./Fisher_Information_and_Its_Application.pdf" data-height="800px"></div>]]></content>
    
    
    <summary type="html">This post mainly introduces the definition, properties, calculation and application of Fisher Information(Matrix).(in Chinese)</summary>
    
    
    
    <category term="Machine Learning" scheme="http://inlmouse.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Math" scheme="http://inlmouse.github.io/tags/Math/"/>
    
    <category term="Information Theory" scheme="http://inlmouse.github.io/tags/Information-Theory/"/>
    
    <category term="Statistic" scheme="http://inlmouse.github.io/tags/Statistic/"/>
    
  </entry>
  
  <entry>
    <title>电子科技大学(UESTC)数学系研究生入学考试模拟试题</title>
    <link href="http://inlmouse.github.io/UESTCGEE/"/>
    <id>http://inlmouse.github.io/UESTCGEE/</id>
    <published>2022-12-12T15:20:03.000Z</published>
    <updated>2023-02-13T06:58:41.149Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>本文收录了适用于电子科技大学(UESTC)数学系研究生入学考试模拟试题（数学分析与高等代数），提供LaTeX源文件下载。题源自2020年以前真题，裴砖，丘砖，谢惠民，其他985高校真题及其改编。不可用做商业用途。</p><h2 id="数学分析601"><a href="#数学分析601" class="headerlink" title="数学分析601"></a>数学分析601</h2><p>共计11套模拟题：<a href="https://www.overleaf.com/read/smpfwcgtxxjj">LaTeX源文件链接</a>。</p><p>样题如下：<br><div class="pdf-container" data-target="./UESTCMA8.pdf" data-height="800px"></div></p><h2 id="高等代数835"><a href="#高等代数835" class="headerlink" title="高等代数835"></a>高等代数835</h2><p>共计8套模拟题：<a href="https://www.overleaf.com/read/gbwddkmkkkjb">LaTeX源文件链接</a>。</p><p>样题如下：<br><div class="pdf-container" data-target="./UESTCAA8.pdf" data-height="800px"></div></p>]]></content>
    
    
    <summary type="html">The simulation questions for the National Graduate Entrance Examination applicable to the Department of Mathematics, UESTC(in Chinese)</summary>
    
    
    
    <category term="Graduate Entrance Examination" scheme="http://inlmouse.github.io/categories/Graduate-Entrance-Examination/"/>
    
    
    <category term="Mathematical Analysis" scheme="http://inlmouse.github.io/tags/Mathematical-Analysis/"/>
    
    <category term="GEE" scheme="http://inlmouse.github.io/tags/GEE/"/>
    
    <category term="Advanced Algebra" scheme="http://inlmouse.github.io/tags/Advanced-Algebra/"/>
    
  </entry>
  
  <entry>
    <title>Sparse Representation of the Signal: A Brief Introduction and Some Discussion</title>
    <link href="http://inlmouse.github.io/SparseRepresentation/"/>
    <id>http://inlmouse.github.io/SparseRepresentation/</id>
    <published>2022-12-02T08:04:06.352Z</published>
    <updated>2022-12-06T03:03:31.069Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Sparse-Vector-and-Sparse-Representation"><a href="#Sparse-Vector-and-Sparse-Representation" class="headerlink" title="Sparse Vector and Sparse Representation"></a>Sparse Vector and Sparse Representation</h2><p>对矩阵$\boldsymbol{A}\in \mathbb{C}^{m\times n}$ 有以下是常用的7种范数<sup><a href="#fn_1" id="reffn_1">1</a></sup>：</p><ul><li>$m_1$范数：$||\boldsymbol{A}||_{m_1}=\sum_{i=1}^{m}\sum_{j=1}^{n}|a_{ij}|.$</li><li>F范数<sup><a href="#fn_2" id="reffn_2">2</a></sup>：$||\boldsymbol{A}||_F=\sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}|a_{ij}|^2}=\sqrt{tr(\boldsymbol{A}^H\boldsymbol{A})}.$</li><li>M范数/最大范数：$||\boldsymbol{A}||_M=\max\{m,n\}\max_{i,j}|a_{ij}|.$</li><li>G范数/几何平均范数：$||\boldsymbol{A}||_G=\sqrt{mn}\max_{i,j}|a_{ij}|.$</li><li>1范数/列和范数<sup><a href="#fn_3" id="reffn_3">3</a></sup>：$||\boldsymbol{A}||_1=\max_{j}\sum_{i=1}^m|a_{ij}|.$</li><li>2范数/谱范数：$||\boldsymbol{A}||_2=\sqrt{\boldsymbol{A}^H\boldsymbol{A}\text{的最大特征值}}.$</li><li>$\infty$范数/行和范数：$||\boldsymbol{A}||_{\infty}=\max_{i}\sum_{j=1}^n|a_{ij}|.$</li></ul><blockquote id="fn_1"><sup>1</sup>. 矩阵的范数定义除开满足非负性，齐次性和三角不等式外，还需满足<strong>相容性</strong>: \begin{equation}\label{compatibility} \forall \boldsymbol{A},\boldsymbol{B}\in \mathbb{C}^{n\times n}, \exists || \boldsymbol{AB}||\leq ||\boldsymbol{A}||\cdot ||\boldsymbol{B}||. \end{equation}<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. Here, $\boldsymbol{A}^H$ means <strong>Hermitian Matrix</strong>(埃尔米特/厄米/自伴随矩阵): $\boldsymbol{A}^H=(\bar{a}_{ji})_{n\times n}$.<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. p-范数并不是按照这里给出的公式定义的，而是从属于某向量范数$||\cdot||_v$导出的矩阵范数:$||\boldsymbol{A}||=\max_{\bf{x}\neq\bf{0}}\frac{||\boldsymbol{Ax}||_v}{||\boldsymbol{x}|_v}$，简称<strong>导出范数/从属范数</strong>，且满足：$||\boldsymbol{E}||=1$.<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><h3 id="Sparse-Representation"><a href="#Sparse-Representation" class="headerlink" title="Sparse Representation"></a>Sparse Representation</h3><p>一个含有大多数零元素的向量或者矩阵成为稀疏向量(sparse vector)或者稀疏矩阵(sparse matrix)。也即是：给定$K\in\mathbb{N}^+, |\boldsymbol{x}|_0 \leq K$。</p><p>给定一个向量$\boldsymbol{x}\in\mathbb{R}^n$，可以定义如下稀疏测度(sparse measure)：<br>\begin{equation}<br>\text{sparseness}(\boldsymbol{x}) = \frac{\sqrt{n} - |\boldsymbol{x}|_1/|\boldsymbol{x}|_2}{\sqrt{n} - 1}.<br>\end{equation}</p><p>信号向量$\boldsymbol{y}\in\mathbb{R}^m$最多可以分解为$m$个正交基$\boldsymbol{g}_k\in\mathbb{R}^m$，这些正交基的集合成为完备正交基(complete orthogonal basis)。此时，信号分解</p><p>\begin{equation}<br>    \boldsymbol{y} = \boldsymbol{G}\boldsymbol{c} = \sum_{i=1}^{m}c_i\boldsymbol{g}_i,<br>\end{equation}<br>中的系数向量$\boldsymbol{c}$一定是非稀疏的。</p><p>若将信号向量$\boldsymbol{y}\in\mathbb{R}^m$分解为$n$个$m$维向量$\boldsymbol{a}_i\in\mathbb{R}$（其中$n&gt;m$）的线性组合：<br>\begin{equation}\label{equ:overcomplete}<br>\boldsymbol{y} = \boldsymbol{A}\boldsymbol{x} = \sum_{i=1}^{n}x_i\boldsymbol{a}_i. \tag{1}<br>\end{equation}<br>则$\boldsymbol{a}_i\in\mathbb{R}$不可能是正交基的集合。为区别于基，这些列向量通常称为<strong>原子</strong>(atom)或框架。由于原子数大于向量空间的维数，所以称这些原子的集合是过完备的(overcompete)。过完备的原子组成的矩阵$\boldsymbol{A}=[\boldsymbol{a}_1, \cdots , \boldsymbol{a}_n]$称为字典或者库(dictionary)。</p><p>对于字典$\boldsymbol{A}\in\mathbb{R}^{m\times n}$，可以做如下假设：</p><ul><li>$n &gt; m$；</li><li>$rank(\boldsymbol{A}) = m$；</li><li>$|\boldsymbol{a}_i|_2 = 1, i = 1, \cdots, n $；</li></ul><p>信号过完备分解式\ref{equ:overcomplete}为欠定方程，存在无穷多组解向量$\boldsymbol{x}$。求解这种欠定方程有两种常用方法：</p><ul><li>古典方法（求最小$L_2$范数解），即是：\begin{equation} \min|\boldsymbol{x}|_2, s.t. \boldsymbol{Ax} = \boldsymbol{y}. \end{equation} 这种方式的优点是：有唯一解，其物理意义为最小能量解。然而由于这种解的每个元素通常为非零值，故不符合很多实际应用的稀疏表示要求。</li><li>现代方法（求最小$L_0$范数解），即是：\begin{equation}\label{equ:sr} \min|\boldsymbol{x}|_0, s.t. \boldsymbol{Ax} = \boldsymbol{y}. \tag{2}\end{equation} 这种方式的优点是：很多实际应用只选择一个稀疏解向量。然而在计算上难以处理。</li></ul><p>假定观测向量存在加性误差或者噪声，最小$L_0$范数解为：<br>\begin{equation}\label{equ:sa}<br>\min|\boldsymbol{x}|_0, s.t. |\boldsymbol{Ax - y}|_2 \leq \varepsilon, \tag{3}<br>\end{equation}<br>其中$\varepsilon$为很小的误差或者扰动。</p><p>当系数向量$\boldsymbol{x}$是稀疏向量时，信号分解$\boldsymbol{y} = \boldsymbol{Ax}$称为（信号的）稀疏分解(sparse decomposition)。其中字典矩阵$\boldsymbol{A}$的列常称为解释变量(explanatory variables)；向量$\boldsymbol{y}$称为相应变量(response variable)或目标信号；$\boldsymbol{Ax}$称为相应的线性预测；$\boldsymbol{x}$可视为目标信号相对于字典$\boldsymbol{A}$的一种表示。</p><p>因此，称式\ref{equ:sr}是目标信号$\boldsymbol{y}$相对于字典$\boldsymbol{A}$的<strong>稀疏表示</strong>(sparse representation)，而式\ref{equ:sa}称为目标信号的<strong>稀疏逼近</strong>(sparse approximation)。</p><p>稀疏表示属于线性求逆问题(linera inverse problem)。在通信和信息论中，$\boldsymbol{A}\in\mathbb{R}^{m\times N}$ 和$\boldsymbol{x}\in\mathbb{R}^N$分别代表编码矩阵和待发送的明文，观测向量$\boldsymbol{y}\in\mathbb{R}^m$则称密文。线性求逆问题便成了解码问题：即如何从密文恢复明文。</p><h2 id="Application-Sparse-Representation-in-Face-Recognition"><a href="#Application-Sparse-Representation-in-Face-Recognition" class="headerlink" title="Application: Sparse Representation in Face Recognition"></a>Application: Sparse Representation in Face Recognition</h2><p>我们考虑close-set的人脸识别应用：假定共有$c$类目标，每一目标的脸部图像已经被向量化编码（可以是直接矩阵拉直，也可以是通过CNN进行特征提取），表示为了$m\times 1$的归一化列向量（通常我们这里的$m$为512或者128）。于是第$i$类目标的$N_i$张训练图像即可表示成$\boldsymbol{D}_i=[\boldsymbol{d}_{i,1}, \cdots , \boldsymbol{d}_{i,N_i}]\in\mathbb{R}^{m\times N}$。给定一个足够丰富的训练集$\boldsymbol{D}_{i}$，则第$i$类目标的非训练集新图片$\boldsymbol{y}$可以被表示为已知训练图像的一线性组合$\boldsymbol{y}\approx\boldsymbol{D}_i\boldsymbol{\alpha}_i$，其中$\boldsymbol{\alpha}_i$为系数向量。问题是：在实际应用中往往不知道新图像分属哪一类，而需要识别：判断该样本的属性。</p><p>于是我们已这$c$类目标的所有训练样本构造一个字典：<br>\begin{equation}<br>    \boldsymbol{D} = [\boldsymbol{D}_1, \cdots, \boldsymbol{D}_c] =  [\boldsymbol{d}_{1,1}, \cdots , \boldsymbol{d}_{1,N_1}, \cdots , \boldsymbol{d}_{c,1}, \cdots , \boldsymbol{d}_{c,N_c}]\in\mathbb{R}^{m\times N}<br>\end{equation}<br>其中$N = \sum_{i=1}^cN_i$。于是，待识别的人脸图像编码$\boldsymbol{y}$可以表示为线性组合：<br>\begin{equation}<br>    \boldsymbol{y} = \boldsymbol{D}\boldsymbol{\alpha}_0= [\boldsymbol{d}_{1,1}, \cdots , \boldsymbol{d}_{1,N_1}, \cdots , \boldsymbol{d}_{c,1}, \cdots , \boldsymbol{d}_{c,N_c}]\begin{bmatrix} \boldsymbol{0}_{N_1} \\ \vdots \\ \boldsymbol{0}_{N_{i-1}} \\ \boldsymbol{\alpha}_i \\\boldsymbol{0}_{N_{i+1}} \\ \vdots \\ \boldsymbol{0}_{N_c} \end{bmatrix}<br>\end{equation}</p><p>现在，人脸识别变成了一个矩阵方程求解的问题或者线性求你问题：已知数据向量$\boldsymbol{y}$和数据矩阵$\boldsymbol{D}$，求矩阵方程$\boldsymbol{y} = \boldsymbol{D}\boldsymbol{\alpha}_0$的解向量$\boldsymbol{\alpha}_0$。需要注意的是，通常$m &lt; N$，因为方程欠定，有无穷多解，其中<strong>最稀疏的解才是我们感兴趣的</strong>。鉴于此，问题划归为式\ref{equ:sr}的问题。</p><h2 id="Optimization-Theory-for-Solving-Sparse-Matrix-Equations"><a href="#Optimization-Theory-for-Solving-Sparse-Matrix-Equations" class="headerlink" title="Optimization Theory for Solving Sparse Matrix Equations"></a>Optimization Theory for Solving Sparse Matrix Equations</h2><h3 id="L-1-Norm-Minimization"><a href="#L-1-Norm-Minimization" class="headerlink" title="$L_1$ Norm Minimization"></a>$L_1$ Norm Minimization</h3><p>$L_1$范数最小化也称为$L_1$线性规划或者$L_1$范数正则化最小二乘。</p><p>直接求解优化问题P0，必须筛选出系数向量$\boldsymbol{x}$中所有可能的非零元素。这个方法是<strong>不可跟踪的</strong>(untractable)或者NP hard<sup><a href="#fn_4" id="reffn_4">4</a></sup>的，因为搜索空间过于庞大。</p><p>向量$\boldsymbol{x}$的非零元素指标集称为<strong>支撑集</strong>，记为$\text{supp}(\boldsymbol{x}) = \{i:x_i\neq 0\}$，支撑集的长度即是$L_0$拟范数<sup><a href="#fn_5" id="reffn_5">5</a></sup>：</p><p>\begin{equation}<br>|\boldsymbol{x}|_0 = |supp(\boldsymbol{x})|.<br>\end{equation}</p><p>K-稀疏向量的集合记为$\Sigma_K = \{\boldsymbol{x}\in\mathbb{R}^N:|\boldsymbol{x}|_0\leq K\}$。若$\hat{\boldsymbol{x}}\in\Sigma_K$，则称向量$\hat{\boldsymbol{x}}$是$\boldsymbol{x}$的K-项逼近或者K-稀疏逼近。</p><p>一般地，称向量$\hat{\boldsymbol{x}}$是$\boldsymbol{x}$在$L_p$范数<sup><a href="#fn_6" id="reffn_6">6</a></sup>下的K-稀疏逼近，若：<br>\begin{equation}<br>|\boldsymbol{x} - \hat{\boldsymbol{x}}|_p = \inf_{\boldsymbol{z}\in\Sigma_K}|\boldsymbol{x} - \boldsymbol{z}|_p.<br>\end{equation}</p><p>显然$L_0$是$L_p$范数范数的特殊形式：$|\boldsymbol{x}|_0 = \lim\limits_{p\to 0}|\boldsymbol{x}|_p^p$。由于当且仅当$p\geq 1$时$|\boldsymbol{x}|_p$为凸函数，所以$L_1$范数时最接近于$L_0$拟范数的凸目标函数。于是从最优化角度讲，称$L_1$范数是$L_0$拟范数的凸松弛[1]。因此，<strong>$L_0$拟范数最小化问题便可以转变为凸松弛的$L_1$范数最小化问题</strong>：<br>\begin{equation}\label{equ:l1sr}<br>\min_{\boldsymbol{x}}|\boldsymbol{x}|_1, s.t. \boldsymbol{y} = \boldsymbol{Ax}.\tag{4}<br>\end{equation}<br>由于$|\boldsymbol{x}|_1$是凸函数，并且约束等式$\boldsymbol{y} = \boldsymbol{Ax}$为一个仿射变换，因此这是一个凸优化问题。</p><p>存在观测噪声的情况下，等式约束可以松弛为不等式约束的最优化问题（$L_1$最小化）：<br>\begin{equation}\label{equ:l1sr-e}<br>\min_{\boldsymbol{x}}|\boldsymbol{x}|_1, s.t. |\boldsymbol{y} - \boldsymbol{Ax}| \leq \varepsilon.\tag{5}<br>\end{equation}</p><p>$L_1$范数下的最优化问题又称为基追踪(base pursuit, BP)。这是一个二次约束线性规划问题(quadratically constrained linear problem, QCLP)。</p><p>若$\boldsymbol{x}_1$是$L_1$的解，$\boldsymbol{x}_0$是$L_0$的解，则有[2]：<br>\begin{equation}<br>|\boldsymbol{x}_1|_1 \leq |\boldsymbol{x}_0|_1.<br>\end{equation}<br>因为$\boldsymbol{x}_1$是可行解，$\boldsymbol{x}_0$是最优解。同时$A\boldsymbol{x}_1 = A\boldsymbol{x}_0$。</p><p>同样的，式\ref{equ:l1sr-e}也有两种变形：</p><ul><li>（$L_1$惩罚最小化）利用$\boldsymbol{x}$是K稀疏向量的约束，将$L_1$不等式范数最小化变成$L_2$：    \begin{equation}\min_{\boldsymbol{x}}\frac{1}{2}|\boldsymbol{y}- \boldsymbol{Ax}|_2^2,s.t.|\boldsymbol{x}|\leq q.    \end{equation}    划归为一类二次规划(quadratic program, QP)问题。</li><li>利用Lagrangian乘子法，将$L_1$不等式范数最小化变成：    \begin{equation}\label{equ:presu-Tik}    \min_{\lambda, \boldsymbol{x}}\frac{1}{2}|\boldsymbol{y}- \boldsymbol{Ax}|_2^2 + \lambda|\boldsymbol{x}|_1.    \end{equation}    划归为一类基追踪去噪(basis pursuit denoising, BPDN)问题[3]。</li></ul><p>在基于小波变换的图像/信号重构和恢复（deconv）中，也经常会遇到基追踪去噪问题。</p><p>参数稀疏的好处主要有以下两点：</p><ul><li>特征选择(Feature Selection)</li><li>可解释性(Interpretability)</li></ul><blockquote id="fn_4"><sup>4</sup>. 指所有NP问题都能在多项式时间复杂度内归约到的问题.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. $L_0$范数不满足范数公理中的齐次性：$|c\boldsymbol{x}|_0 = |c||\boldsymbol{x}|_0$，故严格来讲它是一种虚拟的范数，也称拟范数。<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. $\forall p\in\mathbb{R}^+, |\boldsymbol{x}|_p = \left( \sum_{i\in supp(\boldsymbol{x})}|x_i|^p\right)^{1/p} $.<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote><h3 id="Restricted-Isometry-Property-Condition"><a href="#Restricted-Isometry-Property-Condition" class="headerlink" title="Restricted Isometry Property Condition"></a>Restricted Isometry Property Condition</h3><p>前文讨论了$L_1$范数最小化问题是$L_0$范数最小化某种程度的凸松弛。接下来考察两种问题的解之间的关系。<br><strong>Definition 1.(Restricted Isometry Property Condition)</strong>[4][5]<br><em>若存在矩阵$\boldsymbol{A}$和K-稀疏向量$|\boldsymbol{x}|_0\leq K$，<br>    \begin{equation}<br>        (1-\delta_K)|\boldsymbol{x}|_2^2 \leq |\boldsymbol{A}_K\boldsymbol{x}|_2^2 \leq (1 + \delta_K)|\boldsymbol{x}|_2^2,<br>    \end{equation}<br>    其中$0\leq \delta_K &lt; 1$是一个与稀疏度K有关的常数(约束等距常数, restricted isometry constants, RIC)，$\boldsymbol{A}_K$是字典矩阵$\boldsymbol{A}$的任意K列组成的子矩阵。则称矩阵$\boldsymbol{A}$满足K阶RIP条件。</em></p><p><strong>当RIP条件满足时，非凸的$L_0$范数最小化问题与$L_1$范数最小化问题等价。</strong>也即是：<br>\begin{equation}<br>\min||\boldsymbol{x}||_0\,\,s.t. \boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}\overset{\text{概率为1 的}}{\underset{}{\iff}}\min||\boldsymbol{x}||_1\,\,s.t. \boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}<br>\end{equation}</p><p>带参数$\delta_K$的K阶RIP条件简记为$RIP(K,\delta_K)$，定义为所有使$RIP(K,\delta_K)$成立的参数$\delta$的下确界：<br>\begin{equation}<br>    \delta_K = \inf\left\lbrace \delta: (1-\delta)|\boldsymbol{z}|_2^2 \leq |\boldsymbol{A}_{supp(z)}\boldsymbol{z}|_2^2 \leq (1 + \delta)|\boldsymbol{z}|_2^2, \forall |supp(z)| \leq K, \forall \boldsymbol{z}\in\mathbb{R}^{|supp(\boldsymbol{z})|} \right\rbrace<br>\end{equation}<br>显然若$\boldsymbol{A}_K$正交，则$\delta_K = 0$。于是，RIC的非零值实际上可以评价该矩阵的非正交程度。此外，由于$\boldsymbol{A}_K$的任意性，要求$\boldsymbol{A}$在每一列的能量分布投影尽可能均匀。</p><p>RIC有三个重要性质：</p><ul><li>系数信号精确重构的充分条件[6]： 若字典矩阵$\boldsymbol{A}$分别满足$\delta_K, \delta_{2K}, \delta_{3K}$的RIP条件，并且：    \begin{equation}\delta_K + \delta_{2K} + \delta_{3K} &lt; 1.\end{equation}        则$L_1$范数最小化可以精确重构所有K稀疏信号。也即是，在此条件下，若无噪声存在，则K稀疏信号可以确保由$L_1$范数最小化精确恢复；并且在有噪声的情况下可以稳定估计。</li><li>RIC与特征值的关系[7]：若字典矩阵$\boldsymbol{A}\in\mathbb{R}^{m\times n}$满足$RIP(K,\delta_K)$，则：\begin{equation}1 - \delta_K \leq \lambda_{min}(\boldsymbol{A}_K^T\boldsymbol{A}_K) \leq \lambda_{max}(\boldsymbol{A}_K^T\boldsymbol{A}_K) \leq 1 + \delta_K.\end{equation}</li><li>单调性[6]：若$K\leq K’$，则$\delta_K \leq \delta_{K’}$.</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]: Tropp, J. A. . “Algorithms for simultaneous sparse approximation. Part II: Convex relaxation.” Signal Processing 86.3(2006):589-602.<br>[2]: David, L., and Donoho. “For most large underdetermined systems of linear equations the minimal 1-norm solution is also the sparsest solution.” Communications on Pure and Applied Mathematics (2006).<br>[3]: Chen, S. S. . “Atomic decomposition by basis persuit.” Siam J Sci Comp 20(1999).<br>[4]: Dandes, E. J. . “Near-optimal signal recovery from random projections.” Universal encoding strategies IEEE Transactions on Information Theory 52(2006).<br>[5]: Foucart, Simon , and  M. J. Lai . “Sparsest solutions of underdetermined linear systems via $0{    extbackslashell$0q$0-minimization for $00{    extlessq{    extbackslashleq 1$0.” (2009).<br>[6]: Cai, T. T. ,  L. Wang , and  G. Xu . “New bounds for restricted isometry constants.” IEEE Press (2010).<br>[7]: Dai, W. , and  O. Milenkovic . “Subspace Pursuit for Compressive Sensing Signal Reconstruction.” IEEE Transactions on Information Theory 55.5(2009).</p>]]></content>
    
    
    <summary type="html">使用少量基本信号的线性组合表示一目标信号，称为信号的稀疏表示。信号的稀疏表示是过去近25年来信号处理界一个非常引人关注的研究领域（现在凉透了），众多研究论文和专题研讨会表明了该领域曾经的的蓬勃发展。信号稀疏表示的目的就是在给定的超完备字典中用尽可能少的原子来表示信号，可以获得信号更为简洁的表示方式，从而使我们更容易地获取信号中所蕴含的信息，更方便进一步对信号进行加工处理，如压缩、编码等。(in Chinese)</summary>
    
    
    
    <category term="Signal Processing" scheme="http://inlmouse.github.io/categories/Signal-Processing/"/>
    
    
    <category term="Math" scheme="http://inlmouse.github.io/tags/Math/"/>
    
    <category term="Machine Learning" scheme="http://inlmouse.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>From Cross Entropy to Metric Learning: A Brief Introduction and Some Discussion</title>
    <link href="http://inlmouse.github.io/CEMLE/"/>
    <id>http://inlmouse.github.io/CEMLE/</id>
    <published>2022-12-01T04:13:00.000Z</published>
    <updated>2022-12-12T14:32:47.836Z</updated>
    
    <content type="html"><![CDATA[<h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><p>信息熵是定义信息量的一种测度，如同热力学中定义一样[1]：熵越大混乱程度越大，信息熵越大信息量越大。因此信息熵在直观上需要满足以下两点要求：</p><ul><li>越不可能($p(x)$越小)发生的事件($x$)信息量越大，确定事件($p(x)$很大)的信息量很小；</li><li>独立事件的信息量可叠加；</li></ul><p>满足以上要求的测度定义方式并不唯一，但是数学严谨化之后满足性质的熵几乎是唯一的<sup><a href="#fn_1" id="reffn_1">1</a></sup>。于是有如下定理：</p><p><strong>Theorem 1</strong>(离散信息熵表述唯一性定理[2]). <em>任何具有上述性质的离散的熵，其函数形式必为（Shannon熵 或von Neumann熵）和（Shannon熵 或Hartley 熵）的线性组合。</em></p><p>因此在此定义：<br><strong>Definition 1</strong> (Shannon熵). <em>若$p(x_i)$表随机事件$X$观测为$x_i$的概率，则Shannon熵：<br>\begin{equation}<br>H(X) = -\sum_{i=1}^np(x_i)\log_ap(x_i).<br>\end{equation}<br>这里如果$a=e$则$H(X)$的单位为奈培(NP)，如果$a=2$则$H(X)$的单位为比特(bit)，实际的不同只会让相差一个常系数，并不影响其实际意义。</em></p><blockquote id="fn_1"><sup>1</sup>. 在数学上需要严格满足以下三点：1. 不变性：相空间的熵在保测度正则变换下不变，量子熵在unitary变换下不变；2. 可加性或者泛可加性(subadditive)；3. 连续延拓后需要是凸函数；<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><h2 id="相对熵-KL-Divergence"><a href="#相对熵-KL-Divergence" class="headerlink" title="相对熵:KL Divergence"></a>相对熵:KL Divergence</h2><p>相对熵又称KL散度(Kullback-Leibler Divergence)，是衡量两个事件或者分布之间相似度的度量。当然这个度量也不唯一，但是那是另外一个话题。</p><p><strong>Definition 2</strong> (KL散度). <em>对于随机事件$X$和$Y$，其概率分布分别为$p(x),q(x)$，则KL散度：<br>\begin{equation}\label{equ:KL}<br>D_{KL}(X||Y) = \sum_{i=1}^np(x_i)\log_a\dfrac{p(x_i)}{q(x_i)}. \tag{1}<br>\end{equation}</em></p><p>值得注意的是\ref{equ:KL}式不满足对称性($D_{KL}(X||Y) \neq D_{KL}(Y||X)$)和三角不等式，这一点在JS散度中得到了改善。</p><h2 id="交叉熵-Cross-Entropy"><a href="#交叉熵-Cross-Entropy" class="headerlink" title="交叉熵:Cross Entropy"></a>交叉熵:Cross Entropy</h2><p>考查\ref{equ:KL}式：<br>\begin{equation}<br>D_{KL}(X||Y) = -H(X)-\sum_{i=1}^np(x_i)\log_aq(x_i).<br>\end{equation}<br>注意到在实际使用中往往用$p(x)$来表示样本的真实分布，那么$H(X)$的值往往是不变的，因而直接考察后一项即可。</p><p><strong>Definition 3</strong> (交叉熵). <em>对于随机事件$X$和$Y$，其概率分布分别为$p(x),q(x)$，则交叉熵：<br>\begin{equation}<br>H(X,Y) = -\sum_{i=1}^np(x_i)\log_aq(x_i).<br>\end{equation}</em></p><h2 id="交叉熵的MLE-最大似然估计-解释"><a href="#交叉熵的MLE-最大似然估计-解释" class="headerlink" title="交叉熵的MLE(最大似然估计)解释"></a>交叉熵的MLE(最大似然估计)解释</h2><p>现在有一个真实分布为$p(x)$的随机变量$X$，我们对它进行了$N$次独立同分布实验，对于每个可能的结果$x_i(i=1,\cdots,n)$观察到的次数为$N(x_i)$，显然$\sum_{i=1}^nN(x_i)=N$，那么它的似然值就可以写成：<br>\begin{equation}<br>L=\prod_{i=1}^np(x_i)^{N(x_i)}.<br>\end{equation}</p><p>考察其对数似然值：<br>\begin{equation}\label{equ:lnl}<br>\ln L=\sum_{i=1}^nN(x_i)\ln p(x_i).\tag{2}<br>\end{equation}</p><p>\ref{equ:lnl}式有两个缺点，其一它是个负数，其二它的数值跟样本数有关，样本越多数值越小。因此除以总的样本数归一化，再取相反数，然后改用频率表示：<br>\begin{equation}<br>-\frac{\ln L}{N}=-\sum_{i=1}^n\frac{N(x_i)}{N}\ln p(x_i) = -\sum_{i=1}^n q(x_i)\ln p(x_i) =H(Y,X),<br>\end{equation}<br>显然$\frac{N(x_i)}{N}$即是观测到的概率$q(x_i)$。</p><p>下面在给定$q(x)$的情况下考察$-\dfrac{\ln L}{N}$的最小值时$p(x)$的取值，考虑拉格朗日乘子法，考察拉式量：<br>\begin{equation}<br>W=-\sum_{i=1}^n q(x_i)\ln p(x_i) + \lambda(\sum_{i=1}^np(x_i)-1).<br>\end{equation}<br>求偏导可得：</p><p>\begin{equation}<br>-\frac{q(x)}{p(x)}+\lambda=0,<br>\end{equation}<br>即是$\forall i, p(x_i),q(x_i)$成比例，再由概率归一化条件知：<br>\begin{equation}<br>p(x)=q(x).<br>\end{equation}</p><center><strong>因此可以看出，交叉熵最小实质上就是似然值最大。</strong></center><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]: Witten, E. . “A Mini-Introduction To Information Theory.”, 10.1007/s40766-020-00004-5. 2018.<br>[2]: Aczel, Janos. et al. “Why the Shannon and Hartley entropies are ‘natural’”, 131—146/Advances in applied probability. 1974.</p>]]></content>
    
    
    <summary type="html">Some discussion of Cross Entropy Loss and its equivalence between MLE.(in Chinese)</summary>
    
    
    
    <category term="Machine Learning" scheme="http://inlmouse.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Math" scheme="http://inlmouse.github.io/tags/Math/"/>
    
    <category term="Information Theory" scheme="http://inlmouse.github.io/tags/Information-Theory/"/>
    
  </entry>
  
  <entry>
    <title>沉痛悼念江泽民同志</title>
    <link href="http://inlmouse.github.io/deepmournjzm/"/>
    <id>http://inlmouse.github.io/deepmournjzm/</id>
    <published>2022-11-30T04:13:00.000Z</published>
    <updated>2022-12-02T15:59:32.629Z</updated>
    
    <content type="html"><![CDATA[<img src="/deepmournjzm/jzm.jpg" class width="320" height="445" title="永垂不朽"> <center>江泽民同志遗像。新华社照片，北京，2022年11月30日。</center><blockquote><p>我们敬爱的江泽民同志患白血病合并多脏器功能衰竭，抢救无效，于2022年11月30日12时13分在上海逝世，享年96岁。</p><footer><strong>新华社北京11月30日电,</strong><cite><a href="http://sethgodin.typepad.com/seths_blog/2009/07/welcome-to-island-marketing.html">告全党全军全国各族人民书</a></cite></footer></blockquote><center><strong>吹拉弹唱，我的太阳成绝响； 印俄日德，葛底斯堡仍绕梁。 High Level!</strong></center>]]></content>
    
    
    <summary type="html">苟利国家生死以，岂因祸福避趋之。(in Chinese)</summary>
    
    
    
    <category term="Political Commentary" scheme="http://inlmouse.github.io/categories/Political-Commentary/"/>
    
    
    <category term="politic" scheme="http://inlmouse.github.io/tags/politic/"/>
    
  </entry>
  
</feed>
