<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Strategic Research Office of Individual Eleven</title>
  
  <subtitle>Frontier Explorer</subtitle>
  <link href="http://inlmouse.github.io/atom.xml" rel="self"/>
  
  <link href="http://inlmouse.github.io/"/>
  <updated>2023-08-02T06:19:54.232Z</updated>
  <id>http://inlmouse.github.io/</id>
  
  <author>
    <name>Patrick Sylvestre</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SK Model in Statistical Physics with Replica Method</title>
    <link href="http://inlmouse.github.io/ReplicaSKModel/"/>
    <id>http://inlmouse.github.io/ReplicaSKModel/</id>
    <published>2023-08-02T03:36:14.000Z</published>
    <updated>2023-08-02T06:19:54.232Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Sherrington–Kirkpatrick-Model"><a href="#Sherrington–Kirkpatrick-Model" class="headerlink" title="Sherrington–Kirkpatrick Model"></a>Sherrington–Kirkpatrick Model</h2><p>Sherrington–Kirkpatrick模型(以下简称SK模型)是相互作用范围为无限的Edwards-Anderson模型，是描述自旋玻璃(Spin-Glass)的平均场模型。该模型在形式上基本等价于玻尔兹曼机(Boltzmann Machine)。SK模型源于Sherrington和Kirkpatrick为了使平均场理论有效而对哈密顿量的修改。</p><p>这里讨论SK模型是为了阐述使用副本方法(Replica Method)的基本计算方法。因为尽管SK模型很简单，但它的理论分析非常丰富，涉及连续的现象、序参量、状态的超度量空间和随机分支过程等。</p><h2 id="理论模型"><a href="#理论模型" class="headerlink" title="理论模型"></a>理论模型</h2><p>我们先从哈密顿量开始，SK 模型的哈密顿量为：<br>\begin{equation}<br>    H = - \sum_{i&lt;j}J_{ij}S_{i}S_{j} - h \sum_{i}S_{i}. \label{equ:skhamilton}<br>\end{equation}<br>其中，$S_{i} \in \{-1,+1\}$，在 Ising 模型中表示微观磁矩的指向是上方还是下方；第一项求和遍及$N$格点格子中的所有自旋对，但并不双重计数；$h$ 是均匀磁场；耦合常数$J_{ij}\sim\mathcal{N}(\frac{J_{0}}{N}, \frac{J^{2}}{N})$为高斯随机变量：</p><p>\begin{equation}<br>    P(J_{ij}) = \frac{1}{J} \sqrt{\frac{N}{2\pi}} e^{-\frac{N}{2J^{2}} (J_{ij}-\frac{J_{0}}{N})^{2}}.<br>\end{equation}<br>宽度$J$与位置$i,j$的距离无关。注意到加入非零的相互作用均值$J_0$不会带来其他问题，只会让公式更复杂。这里对上式进行简化：</p><p>\begin{equation}\label{equ:JProb}<br>    P(J_{ij}) = \frac{1}{J} \sqrt{\frac{N}{2\pi}} e^{-\frac{NJ_{ij}^2}{2J^{2}}}. \tag{1}<br>\end{equation}</p><p>对于无限范围内平移不变的相互作用，平均场理论是严格的，因此我们期望有一种平均场类型的理论能够对这个模型进行有效求解。</p><p>由于这里的无序是淬火<sup><a href="#fn_1" id="reffn_1">1</a></sup>的，因此我们必须计算自由能并按照概率分布函数式\eqref{equ:JProb}对其求期望，即：<br>\begin{equation}<br>    f=-\frac{k_BT}{N}\prod_{i&lt;j}^N\mathbb{E}_{J_{ij}}\left[\ln Z(\{J_{ij}\},h,T)\right]=-\frac{k_BT}{N}\prod_{i&lt;j}^N\int {\mathrm d} J_{ij}P(J_{ij})\ln Z(\{J_{ij}\},h,T).<br>\end{equation}<br>其中$k_B$为玻尔兹曼常数，且有：<br>\begin{equation}\label{equ:skpartial}<br>    \begin{aligned}<br>        Z &amp; = \underbrace{\sum_{S_1=\pm 1}\cdots\sum_{S_N=\pm 1}}_\text{N重求和} \exp\left\{ \beta \left[ \sum_{i&lt;j}^N J_{ij}S_{i}S_{j} + h \sum_{i=1}^N S_{i}\right] \right\}\\<br>        &amp; = \sum_{S_1=\pm 1}\cdots\sum_{S_N=\pm 1}\exp\left\{ \beta \left[ \sum_{i&lt;j}^N J_{ij}S_{i}S_{j} + \frac{h}{2(N-1)} \sum_{i&lt;j}^N(S_i+S_j)\right] \right\}\\<br>        &amp; = \sum_{S_1=\pm 1}\cdots\sum_{S_N=\pm 1}\prod_{i&lt;j}^N \exp\left\{ \beta \left[ J_{ij}S_{i}S_{j} + \frac{h}{2(N-1)}(S_i+S_j)\right] \right\}.<br>    \end{aligned}\tag{2}<br>\end{equation}</p><blockquote id="fn_1"><sup>1</sup>. 如果我们从冷却出发，系统如果冷却得很快称为<strong>淬火</strong>，如果冷却得慢称为<strong>退火</strong>。如果我们从无序出发，如果一个系统依赖于不随时间演化的随机变量，那么它称为<strong>淬火无序</strong>；如果它依赖于随时间演化的随机变量，那么它称为<strong>退火无序</strong>。淬火无序的一个具体例子即是当温度变化时，磁性原子分布不会变化。<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><h2 id="基于Replica-Method的求解框架"><a href="#基于Replica-Method的求解框架" class="headerlink" title="基于Replica Method的求解框架"></a>基于Replica Method的求解框架</h2><p>这里我们可以使用狄拉克符号，引入矩阵$\boldsymbol{P}$，其矩阵元:<br>\begin{equation}<br>\langle S_i|\boldsymbol{P}|S_j\rangle = \exp\left\{ \beta \left[ J_{ij}S_{i}S_{j} + \frac{h}{2(N-1)}(S_i+S_j)\right] \right\}.<br>\end{equation}</p><p>注意到$S_i,S_j$只能取值$\pm 1$，因此矩阵$\boldsymbol{P}$为如下的$2\times 2$的矩阵：<br>\begin{equation}<br>    \begin{aligned}<br>        \boldsymbol{P} &amp;=<br>        \begin{pmatrix}\langle 1|\boldsymbol{P}|1\rangle &amp; \langle 1|\boldsymbol{P}|-1\rangle\\<br>        \langle -1|\boldsymbol{P}|1\rangle &amp; \langle -1|\boldsymbol{P}|-1\rangle<br>        \end{pmatrix}\\<br>        &amp;= \begin{pmatrix} e^{\beta J_{ij}+h\beta/(N-1)} &amp; e^{-\beta J_{ij}}\\<br>        e^{-\beta J_{ij}} &amp; e^{\beta J_{ij}-h\beta/(N-1)}<br>        \end{pmatrix}.<br>    \end{aligned}<br>\end{equation}</p><p>那么配分函数\eqref{equ:skpartial}式，可以改写为<sup><a href="#fn_2" id="reffn_2">2</a></sup>：<br>\begin{equation}<br>    \begin{aligned}<br>        Z &amp; = \sum_{S_1=\pm 1}\cdots\sum_{S_N=\pm 1}\prod_{i&lt;j}^N \langle S_i|\boldsymbol{P}|S_j\rangle\\<br>        &amp; =\text{Tr}_{\{S_i\}}\exp[-\beta H(\{J_{ij}\},\{S_{i}\})].<br>    \end{aligned}<br>\end{equation}<br>事实上，计算至此，由于随机变量$J_{ij}$的存在，已经无法继续计算。<strong>为了交换对于耦合常数$J_{ij}$求期望和对所有自旋位形$\{S_{i}\}$求迹的顺序，我们才使用replica方法。</strong><sup><a href="#fn_3" id="reffn_3">3</a></sup><br>因此，配分函数的$n$次幂为：<br>\begin{equation}<br>Z^n = \prod_{\alpha=1}^n\text{Tr}_{\{S_i^\alpha\}}\exp[-\beta H(\{J_{ij}\},\{S_i^\alpha\})] = \text{Tr}_{\{S_i^\alpha\}}\exp\left\{ \beta\sum_{\alpha=1}^n\left[ \sum_{i&lt;j}J_{ij}S_i^\alpha S_j^\alpha + h \sum_{i}S_i^\alpha\right] \right\}.<br>\end{equation}</p><p>注意到<sup><a href="#fn_4" id="reffn_4">4</a></sup>：<br>\begin{equation}<br>\mathbb{E}_{J_{ij}}\left[\exp(\beta J_{ij}\sum_{\alpha=1}^nS_i^\alpha S_j^\alpha)\right]=\exp\left[ \beta^2J^2/2N\sum_{\alpha,\gamma=1}^n S_i^\alpha S_j^\alpha S_i^\gamma S_j^\gamma\right].<br>\end{equation}</p><p>那么显然：<br>\begin{equation}<br>\mathbb{E}_{}\left[Z^n\right] = \text{Tr}_{\{S_i^\alpha\}}\exp\left[ \frac{\beta^2J^2}{2N}\sum_{i&lt;j}^N \sum_{\alpha,\gamma=1}^n S_i^\alpha S_j^\alpha S_i^\gamma S_j^\gamma + \beta h \sum_{i=1}^N\sum_{\alpha=1}^n S_i^\alpha\right].<br>\end{equation}</p><p>我们得到了不同副本之间的自旋的有效相互作用为：<br>\begin{equation}\label{equ:freeengrep}<br>    \begin{aligned}<br>        -\beta f &amp;= \lim_{n\to 0}\frac1n[\mathbb{E}_{}[Z^n]-1]\\<br>        &amp;=\lim_{n\to 0, N\to\infty}\frac{1}{nN}\left\{\text{Tr}_{\{S_i^\alpha\}}\exp\left[ \frac{\beta^2J^2}{2N}\sum_{i&lt;j}^N \sum_{\alpha,\gamma=1}^n S_i^\alpha S_j^\alpha S_i^\gamma S_j^\gamma + \beta h \sum_{i=1}^N\sum_{\alpha=1}^n S_i^\alpha\right]-1\right\}\\<br>        &amp;\approx \lim_{n\to 0, N\to\infty}\frac{1}{nN}\left[\text{Tr}_{\{S_i^\alpha\}}\exp(\mathcal{H})-1\right].<br>    \end{aligned}<br>    \tag{3}<br>\end{equation}<br><strong>事实上，至此，通用的计算就已经结束。下面至文末的计算是基于某一个具体模型的各种分析技巧，除了正交解耦技巧外，其他不具备普适性。</strong></p><blockquote id="fn_2"><sup>2</sup>. 如果$J_{ij}$回退Ising模型，视为常数而非随机变量，那么：<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><p>\begin{equation}<br>    \begin{aligned}<br>        Z &amp; = \sum_{S_1=\pm 1}\cdots\sum_{S_N=\pm 1}\prod_{i&lt;j}^N \langle S_i|\boldsymbol{P}|S_j\rangle\\<br>        &amp; =\sum_{S_1}\langle S_1|\boldsymbol{P}^{N(N-1)/2}|S_1\rangle\\<br>        &amp; = \text{Tr}\boldsymbol{P}^{N(N-1)/2}.<br>    \end{aligned}<br>\end{equation}<br>这里求矩阵的迹Tr是严格的，下文中不严格的复用了这个表示，指代对所有可能求和。</p><blockquote id="fn_3"><sup>3</sup>. 这便是自平均性的体现，当$N\gg \beta J\gg 1$时，以$1/N$为基展开，则有：<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><p>\begin{equation}<br>\mathbb{E}_{}[Z^n] = \mathbb{E}_{}[Z]^n.<br>\end{equation}</p><blockquote id="fn_4"><sup>4</sup>. 这里使用恒等式：<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><p>\begin{equation}\label{equ:guassianintreq}<br>    \sqrt{\frac{a}{2\pi}}\int_{\mathbb{R}}e^{-\frac12(ax^2-\lambda\sqrt{2}x)}{\mathrm d} x = e^{\lambda^2/4a}. \tag{4}<br>\end{equation}</p><h2 id="继续求解"><a href="#继续求解" class="headerlink" title="继续求解"></a>继续求解</h2><p>再次注意到：<br>\begin{equation}<br>    \begin{aligned}<br>        \sum_{i&lt;j}^N \sum_{\alpha,\gamma=1}^n S_i^\alpha S_j^\alpha S_i^\gamma S_j^\gamma  &amp;= \frac12\sum_{i&lt;j}^N \sum_{\alpha\neq\gamma}^n S_i^\alpha S_j^\alpha S_i^\gamma S_j^\gamma + \frac{nN(N-1)}{2} \\<br>        &amp; = \frac12\sum_{\alpha\neq\gamma}^n\left(\sum_{i=1}^N S_i^\alpha S_i^\gamma\right)^2+\frac{nN(N-1)}{2}.<br>    \end{aligned}<br>\end{equation}</p><p>因此有：<br>\begin{equation}<br>\mathcal{H} = \frac{\beta^2J^2nN}{4} + \frac{\beta^2J^2}{2N}\sum_{\alpha,\gamma=1}^n\left(\sum_{i=1}^N S_i^\alpha S_i^\gamma\right)^2 + \beta h\sum_{i=1}^N\sum_{\alpha=1}^n S_i^\alpha.<br>\end{equation}<br>其中，副本的相互总用仅在第二项中出现。由于$\mathcal{H}$中不同位置的自旋总是耦合在一起的，因此，式\eqref{equ:freeengrep}中的求迹运算极端复杂(矩阵不是对角阵)。我们可以使用对角化技术，也即引入一组虚拟变量来消除这些耦合。对式\eqref{equ:guassianintreq}中的$a=N,\lambda = \sqrt{2}\beta J\sum_{i=1}^n S_i^\alpha S_i^\gamma$，有恒等式：<br>\begin{equation}<br>    \sqrt{\frac{N}{2\pi}}\int_{\mathbb{R}}e^{-\frac12Nx_{\alpha\gamma}^2-\beta J\left(\sum_{i=1}^N S_i^\alpha S_i^\gamma\right) x_{\alpha\gamma} }{\mathrm d} x_{\alpha\gamma} = e^{\beta^2J^2\left(\sum_{i=1}^N S_i^\alpha S_i^\gamma\right)^2/2N}.<br>\end{equation}</p><p>这直接导致：<br>\begin{equation}<br>    \begin{aligned}<br>        e^{\mathcal{H}} &amp; = \exp\left[\frac{\beta^2J^2nN}{4} + \beta h\sum_{i=1}^N\sum_{\alpha=1}^n S_i^\alpha\right]\prod_{\alpha,\gamma=1}^n \sqrt{\frac{N}{2\pi}}\int_{\mathbb{R}}e^{-\frac12Nx_{\alpha\gamma}^2-\beta J\left(\sum_{i=1}^N S_i^\alpha S_i^\gamma\right) x_{\alpha\gamma} }{\mathrm d} x_{\alpha\gamma}<br>    \end{aligned}<br>\end{equation}<br>中不同位置的自旋不再耦合在一起，并且上式中求迹就华为对一个位置上(例如$i$)的自旋求迹，并有$N$项乘积。因此，忽略位置标记$i$，研究不同副本之间的自旋耦合上。</p><p>这里使用最速下降法(Method of Steepest Descent，细节可以参考<a href="https://inlmouse.github.io/SteepestDescent/">上一篇博文</a>)，处理上述积分，可以得到：<br>\begin{equation}<br>    \text{Tr}e^{\mathcal{H}}=\exp\left[\frac{\beta^2J^2N}{4}[n-n(n-1)q^2]\right] \left[\text{Tr}\exp\left\{ \beta h\sum_{\alpha=1}^nS^\alpha + \beta^2J^2q\sum_{\alpha,\gamma=1}^nS^\alpha S^\gamma\right\}\right]^N<br>\end{equation}<br>其中$q$为该模型的序参量<sup><a href="#fn_5" id="reffn_5">5</a></sup>。</p><p>再次使用式\eqref{equ:guassianintreq}技巧解耦自旋：<br>\begin{equation}<br>\exp\left[ \beta^2J^2q\sum_{\alpha,\gamma=1}^nS^\alpha S^\gamma \right] = \exp\left[-\frac{n\beta^2J^2q}{2}\right]\int_{\mathbb{R}}e^{-z^2/2}\exp\left[z\beta \sqrt{q}\sum_{\alpha=1}^nS^\alpha\right]\frac{\operatorname{d} z}{\sqrt{2\pi}},<br>\end{equation}<br>得到：<br>\begin{equation}<br>    \begin{aligned}<br>        \text{Tr}e^{\mathcal{H}}&amp;=e^P\left[\text{Tr}\int_{\mathbb{R}}e^{-z^2/2}\exp\left[(z\beta J\sqrt{q} + \beta h)\sum_{\alpha=1}^nS^\alpha\right]\frac{\operatorname{d} z}{\sqrt{2\pi}}\right]^N\\<br>        &amp;=e^P\left[\int_{\mathbb{R}}e^{-z^2/2}(2\cosh\mathcal{Z})^n\frac{\operatorname{d} z}{\sqrt{2\pi}}\right]^N\\<br>        &amp; = e^P\exp\left[N\ln\int_{\mathbb{R}}e^{-z^2/2}(2\cosh\mathcal{Z})^n\frac{\operatorname{d} z}{\sqrt{2\pi}}\right]\\<br>        &amp;\approx  e^P\exp\left[N\ln\int_{\mathbb{R}}e^{-z^2/2}(1+n\ln (2\cosh\mathcal{Z}))\frac{\operatorname{d} z}{\sqrt{2\pi}}\right]\\<br>        &amp;\approx e^P\exp\left[nN\ln\int_{\mathbb{R}}e^{-z^2/2}\ln(2\cosh\mathcal{Z})\frac{\operatorname{d} z}{\sqrt{2\pi}}\right].<br>    \end{aligned}<br>\end{equation}<br>其中$P=N\beta^2J^2(n-n(n-1)q^2)/4-nN\beta^2J^2q/2$，$\mathcal{Z} = z\beta J\sqrt{q}+\beta h$。将上述结果带入\eqref{equ:freeengrep}式，并取极限：<br>\begin{equation}<br>    \beta f = -\frac{\beta^2J^2}{4}(1-q)^2-\int_{\mathbb{R}}e^{-z^2/2}\ln(2\cosh\mathcal{Z})\frac{\operatorname{d} z}{\sqrt{2\pi}}.<br>\end{equation}<br>至此，理论计算求解结束。</p><blockquote id="fn_5"><sup>5</sup>. 该序参量由下式给出：<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><p>\begin{equation}<br>    \begin{aligned}<br>        q = \langle\langle S^\alpha S^\gamma\rangle\rangle &amp;= \left. \frac{\text{Tr}S^\alpha S^\gamma e^{\mathcal{H}}}{\text{Tr}e^{\mathcal{H}}}\right|_{n\to 0}\\<br>        &amp; = \left.\frac{\int_{\mathbb{R}}e^{-z^2/2}(2\sinh\mathcal{Z})^2(2\cosh\mathcal{Z})^{n-2}\frac{\operatorname{d} z}{\sqrt{2\pi}}}{\int_{\mathbb{R}}e^{-z^2/2}(2\cosh\mathcal{Z})^n\frac{\operatorname{d} z}{\sqrt{2\pi}}}\right|_{n\to 0}\\<br>        &amp; = \int_{\mathbb{R}}e^{-z^2/2}\tanh^2\mathcal{Z}\frac{\operatorname{d} z}{\sqrt{2\pi}}.<br>    \end{aligned}<br>\end{equation}</p>]]></content>
    
    
    <summary type="html">Solving the analytical expression of the SK Model using the replica method.(in Chinese)</summary>
    
    
    
    <category term="Statistical Physics" scheme="http://inlmouse.github.io/categories/Statistical-Physics/"/>
    
    
    <category term="Replica Method" scheme="http://inlmouse.github.io/tags/Replica-Method/"/>
    
    <category term="SK Model" scheme="http://inlmouse.github.io/tags/SK-Model/"/>
    
    <category term="Boltzmann Machine" scheme="http://inlmouse.github.io/tags/Boltzmann-Machine/"/>
    
  </entry>
  
  <entry>
    <title>Method of Steepest Descent in Numerical Analysis</title>
    <link href="http://inlmouse.github.io/SteepestDescent/"/>
    <id>http://inlmouse.github.io/SteepestDescent/</id>
    <published>2023-07-02T03:43:17.000Z</published>
    <updated>2023-08-02T06:20:15.417Z</updated>
    
    <content type="html"><![CDATA[<h2 id="最速下降法"><a href="#最速下降法" class="headerlink" title="最速下降法"></a>最速下降法</h2><p>最速下降法<sup><a href="#fn_1" id="reffn_1">1</a></sup>，也称为鞍点法(saddle-point method)，作为Laplace方法的在复分析中的应用[1,2]，辅之以留数定理，以找出一个过最速下降点的contour<sup><a href="#fn_2" id="reffn_2">2</a></sup>的曲线积分，用来取代原有的复数空间的contour积分。该方法是估计型如下的积分：<br>\begin{equation}<br>    I(\lambda) = \int_C f(z)e^{\lambda g(z)}{\mathrm d} z.<br>\end{equation}<br>其中$C$为contour，对于$\lambda \rightarrow \infty$, $f(z),g(z)$为$z$的解析函数。由于被积函数解析，contour $C$可以被另一条性质更好的contour $C’$而不改变积分。特别的，我们选取contour $C’$使得$\Im(g(z))$为常数，剩余部分的估计可以退化为Laplace方法进行估计。</p><p>也即是：<br>\begin{equation}<br>    I(\lambda) = \int_C f(z)e^{\lambda g(z)}{\mathrm d} z = e^{i\lambda \Im(g(z))}\int_{C’}f(z)e^{\lambda \Re(g(z))} {\mathrm d} z.<br>\end{equation}</p><p>由于该方法中，已经利用另一条通过最速下降的鞍点来取代原有的contour积分，经过变数变换后就会变得有如拉普拉斯方法。因此，我们可以透过这新的contour，找到原本的积分的渐进近似解，而这将大大的简化整个计算，得名最速下降法。就好像原本的路径像是在蜿蜒的山路开车，而新的路径就像干脆绕过这座山开，反正目的只是到达目的地而已，留数定理已经帮我们把中间的差都算好了。</p><blockquote id="fn_1"><sup>1</sup>. 尤其注意该方法不同于优化算法中的梯度下降法(gradient descent)。<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. 翻译为”路径”的话，会与path integral相冲，因此这里还是以英文原字称呼。<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>考察积分：<br>\begin{equation}<br>    I(\lambda) = \int_0^1 \cos(\lambda x)\ln x{\mathrm d} x.<br>\end{equation}<br>在$\lambda \to \infty$的渐进数值特性。</p><p>考虑：<br>\begin{equation}<br>    J(\lambda) = \int_0^1 e^{i\lambda z}\ln z{\mathrm d} z.<br>\end{equation}<br>显然$I(\lambda) = \Re(J(\lambda))$。考虑复平面上三点：$P(1,0), Q(0, R),S(1,R)$，那么由柯西积分定理，原积分路径$OP$可以更变为$OQSP$。</p><p>接下来我们令$R\to\infty$，那么积分中$QS$段中由于被积函数$e^{-\lambda R}$项的存在，其贡献趋近于0。那么在$OQ$段记$z=is$，$SP$段记$z=1+is$：<br>\begin{equation}<br>    I(\lambda) = \Re \left[\int_0^\infty e^{-\lambda s}\ln(is){\mathrm d} is\right] -  \Re \left[\int_0^\infty e^{-\lambda s + i\lambda }\ln(1+is){\mathrm d} is\right].<br>\end{equation}<br>其中右边第二个积分的符号是负的因为$SP$向下遍历。</p><p>注意到有恒等式：<br>\begin{equation}<br>    \ln(is) = \ln(se^{i\frac{\pi}{2}}) = \ln(s) + i\frac{\pi}{2}.<br>\end{equation}</p><p>那么：<br>\begin{equation}<br>    \Re \left[\int_0^\infty e^{-\lambda s}\ln(is){\mathrm d} is\right] = \int_0^\infty e^{-\lambda s}\Re\left( i \ln(s) - \frac{\pi}{2} \right){\mathrm d} s = -\frac{\pi}{2\lambda}.<br>\end{equation}</p><p>同理：<br>\begin{equation}<br>    \begin{aligned}<br>        \Re \left[\int_0^\infty e^{-\lambda s + i\lambda }\ln(1+is){\mathrm d} is\right] &amp; = \Re \left[e^{i\lambda} \int_0^\infty e^{-\lambda s}i\ln(1+is){\mathrm d} s\right]\\<br>        &amp;\overset{\ln(1+is)\approx is}{\approx} \Re (e^{i\lambda})\int_0^\infty e^{-\lambda s}s{\mathrm d} s\\<br>        &amp;=-\frac{\cos\lambda}{\lambda^2}.<br>    \end{aligned}<br>\end{equation}</p><p>综上二式，得到：<br>\begin{equation}\label{equ:approxmsd}<br>     I(\lambda) = \int_0^1 \cos(\lambda x)\ln x{\mathrm d} x \approx -\frac{\pi}{2\lambda}+\frac{\cos\lambda}{\lambda^2}.<br>\end{equation}</p><img src="/SteepestDescent/appA.png" class width="400" height="250"> <center> 在$2\leq \lambda \leq 20$的逼近效果。实线为估计式的数值结果，虚线为原积分的数值结果。</center><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]: Bender C M, Orszag S A. Advanced mathematical methods for scientists and engineers I: Asymptotic methods and perturbation theory[M]. Springer Science &amp; Business Media, 1999.<br>[2]: Jones L M. Introduction to mathematical methods of physics[J]. 1979.</p>]]></content>
    
    
    <summary type="html">An introduction of an special extension of Laplace&#39;s method for approximating an integral.(in Chinese)</summary>
    
    
    
    <category term="Numerical Analysis" scheme="http://inlmouse.github.io/categories/Numerical-Analysis/"/>
    
    
    <category term="Math" scheme="http://inlmouse.github.io/tags/Math/"/>
    
    <category term="Complex Analysis" scheme="http://inlmouse.github.io/tags/Complex-Analysis/"/>
    
  </entry>
  
  <entry>
    <title>Stable Diffusion LoRA Model, A First Try</title>
    <link href="http://inlmouse.github.io/sdchunmomo/"/>
    <id>http://inlmouse.github.io/sdchunmomo/</id>
    <published>2023-04-16T02:00:00.000Z</published>
    <updated>2023-04-16T04:37:48.776Z</updated>
    
    <content type="html"><![CDATA[<p>After the craziest March in the recent history of AI development, everyone has witnessed the powerful capabilities of generative AI. In the field of computer vision, there have been countless AIGC (AI-generated content) methods and tools, such as GigaGAN, Stable Diffusion, DALL·E2, MidJourney, LoRA, and many more, whose results are stunning and awe-inspiring.</p><p>I had planned to write a technical report, sort-out and studying <strong>generative models from the perspective of probability theory</strong>. With a rigorous attitude, I intended to first experiment with the capabilities of these new technologies before writing this note. However, things spiraled out of control, and even though I had already played around with things like StyleTransfer 8 years ago, I still found them extremely fascinating.</p><p>I spent some time training a Stable Diffusion RoLA model(<a href="https://civitai.com/models/41309/chunmomo">civitai link</a>), this post will demonstrate some technical details.</p><img src="/sdchunmomo/00074-383632874.png" class width="512" height="768"> <center> The generated image sample, which have an extremely high level of completion to the human eyes.</center><h2 id="LoRA-Low-rank-Adaptation-for-Fast-Text-to-Image-Diffusion-Fine-tuning"><a href="#LoRA-Low-rank-Adaptation-for-Fast-Text-to-Image-Diffusion-Fine-tuning" class="headerlink" title="LoRA: Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning"></a>LoRA: Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning</h2><p>Many people like to fine-tune stable diffusion models to fit their needs and generate higher fidelity images. However, the fine-tuning process is very slow, and finding a good balance between the number of steps and the quality of the results is not easy. Furthermore, the final output of a fully fine-tuned model is very large. Some people use textual inversion as an alternative method, but clearly this is not optimal: textual inversion only creates a small word embedding, resulting in a final image that is not as good as that produced by a fully fine-tuned model.</p><p>As the results, LoRA came out. <a href="https://arxiv.org/abs/2106.09685">LoRA</a> stands for Low-Rank Adaptation, a mathematical technique to deal with the problem of fine-tuning LLMs and reduce the number of parameters that are trained. The idea is very simple. Just like ResNet, training residuals instead of the model itself, which is indeed the work of Microsoft Research. By applying LoRA to cross-attention layers, it links image representations with prompts describing them. </p><img src="/sdchunmomo/latent-diffusion.png" class width="800" height="397"> <center> Latent Diffusion Models(https://arxiv.org/abs/2112.10752), which is used to fine-turn high resolution images in stable diffusion. The yellow blocks are cross-attention layers.</center><h2 id="The-Self-trained-Model-Some-Details"><a href="#The-Self-trained-Model-Some-Details" class="headerlink" title="The Self-trained Model: Some Details"></a>The Self-trained Model: Some Details</h2><p>This LoRA model is trained by a non-NSFW subset of the photobook of the famous cosplayer Chunmomo(蠢沫沫). <strong>So it won’t work well with NSFW prompts</strong>.</p><p>I manually selected 96 face images and 259 bust/full body images, as shown in the figure below. The selection of face concept set is to use images with no occlusion and a single background as much as possible.<br><img src="/sdchunmomo/face.png" class width="880" height="408"> </p><center> Face concept set.</center><img src="/sdchunmomo/body.png" class width="880" height="408"> <center> Body concept set.</center><p>The training process has gone through 50 epochs, and each concept have trained 10 times. Image resolution 512x768, batch size 1(RTX 2080 8GB). </p><img src="/sdchunmomo/loss.png" class width="1038" height="336"> <p>After 19.5 hours of waiting, I finally got the LoRA model. </p><h2 id="Some-Generated-Image-Results"><a href="#Some-Generated-Image-Results" class="headerlink" title="Some Generated Image Results"></a>Some Generated Image Results</h2><img src="/sdchunmomo/00071-3836818767.png" class width="512" height="768"> <center> Key Prompts: white_dress, sailor_collar, sailor_dress, polka_dot, straw_hat, v_arm</center><img src="/sdchunmomo/00062-6173687.png" class width="512" height="768"> <center> Key Prompts: goggles, goggles_on_head, shorts, socks, long_sleeves, street, outdoor</center><img src="/sdchunmomo/00067-1673675340.png" class width="512" height="768"> <center> Key Prompts: goggles, goggles_on_head, shorts, socks, short_sleeves, ocean, water, twin_braids</center><img src="/sdchunmomo/00043-281637387.png" class width="512" height="768"> <center> Key Prompts: sailor_collar, sailor_dress, pleated_skirt, ponytail, cross_arm, sitting</center><h2 id="Important-Statement"><a href="#Important-Statement" class="headerlink" title="Important Statement"></a>Important Statement</h2><p><strong>This model is for academic research use only, commercial use is strictly prohibited. Use of this model to generate NSFW content, as well as any use that compromises individual rights and privacy is prohibited.</strong></p>]]></content>
    
    
    <summary type="html">Technical details of a self-trained LoRA model. 一个自己训练的LoRA模型的技术细节。(in English)</summary>
    
    
    
    <category term="AIGC" scheme="http://inlmouse.github.io/categories/AIGC/"/>
    
    
    <category term="Stable Diffusion" scheme="http://inlmouse.github.io/tags/Stable-Diffusion/"/>
    
    <category term="LoRA" scheme="http://inlmouse.github.io/tags/LoRA/"/>
    
  </entry>
  
  <entry>
    <title>第十三届全国大学生数学竞赛决赛(数学类高年级组)第九题赏析</title>
    <link href="http://inlmouse.github.io/13thNUMCFQ9/"/>
    <id>http://inlmouse.github.io/13thNUMCFQ9/</id>
    <published>2023-03-30T00:38:09.000Z</published>
    <updated>2023-03-30T09:03:57.283Z</updated>
    
    <content type="html"><![CDATA[<p>本题目是于2023年3月在中国上海华东师范大学举办的第十三届全国大学生数学竞赛决赛(数学类高年级组)中的第九题，题目考察范围广泛，需要应用灵活运用矩阵求导、概率论、高维高斯随机向量的性质、条件期望与重期望公式以及Fubini定理等少量积分技巧来解决该问题。问题本身实际为机器学习的背景，稍作修改可以变形为更为真实的应用场景，是难得值得该领域研究者品鉴的好题目。</p><p>官方给出的答案太过于简略，这里给出一个详细解答。</p><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>记$p\geq 2$是整数，$\beta$是$p$维末知参数，$\boldsymbol{X}$是$p$维随机向量服从正态分布$\mathcal{N}(\boldsymbol{0}, \boldsymbol{\Sigma})$。其中$\boldsymbol{\Sigma}$是$p$阶对称正定矩阵，随机误差$\varepsilon$是1维随机变量,服从正态分布$\mathcal{N}(0, \sigma^2)$，且与$\boldsymbol{X}$独立，其中$\sigma^2&gt;0$。记$\text{sgn}(\cdot)$为符号函数，$Y = \text{sgn}(\boldsymbol{X}^{T}\beta_0+\varepsilon)$是1维随机变量。定义：<br>\begin{equation}<br>    \beta^* = \text{argmin}_{\beta\in\mathbb{R}^p}\mathbb{E}\left[(Y-\boldsymbol{X}^T\beta)^2\right].<br>\end{equation}</p><ol><li>请给出$\beta^*$的显式表达式。</li><li>若$F(\cdot)$为$\varepsilon$的分布函数，证明：<br>\begin{equation}<br> \text{Cov}(\boldsymbol{X}, Y) = -2\text{Cov}(\boldsymbol{X}, F(-\boldsymbol{X}^T\beta_0)).<br>\end{equation}</li><li>$\forall \alpha,\beta\in \mathbb{R}^p$，证明：<br>\begin{equation}<br> \mathbb{E}\left[\alpha^T\boldsymbol{X}|\beta^T\boldsymbol{X}\right] = (\alpha^T\boldsymbol{\Sigma}\beta)\left((\beta^T\boldsymbol{\Sigma}\beta)^{-1}\beta^T\boldsymbol{X}\right).<br>\end{equation}</li><li>证明$\beta^*$与$\beta_0$成比例，且要求找到比例因子的显式表达式。</li></ol><h2 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h2><ul><li>首先计算：</li></ul><p>\begin{aligned}<br>            \mathbb{E}\left[(Y-\boldsymbol{X}^T\beta)^2\right] &amp; = \mathbb{E}\left[1 - 2Y\boldsymbol{X}^T\beta + \beta^T\boldsymbol{X}\boldsymbol{X}^T\beta\right]\\<br>            &amp; = 1- 2\mathbb{E}\left[Y\boldsymbol{X}^T\right]\beta + \beta^T\mathbb{E}\left[\boldsymbol{X}\boldsymbol{X}^T\right]\beta\\<br>            &amp; \overset{\mathbb{E}\left[\boldsymbol{X}\right]=\boldsymbol{0}}{=} 1 - 2\text{Cov}(\boldsymbol{X},Y)\beta + \beta^T\boldsymbol{\Sigma}\beta.<br>\end{aligned}</p><p>再对$\beta$求导，直接得到：</p><p>\begin{aligned}<br>            \boldsymbol{0}  = \frac{\partial \left[1 - 2\text{Cov}(\boldsymbol{X},Y)\beta + \beta^T\boldsymbol{\Sigma}\beta\right]}{\partial \beta} &amp; = -2\text{Cov}(\boldsymbol{X},Y) + (\boldsymbol{\Sigma}+\boldsymbol{\Sigma}^T)\beta\\<br>            \beta^* &amp;= \boldsymbol{\Sigma}^{-1}\text{Cov}(\boldsymbol{X},Y).<br>\end{aligned}</p><ul><li>其次注意到：</li></ul><p>\begin{aligned}<br>            \text{Cov}(\boldsymbol{X},Y) &amp; \overset{\text{重期望公式}}{=} \mathbb{E}_{\varepsilon}\left[\mathbb{E}_{\boldsymbol{X}}\left[\boldsymbol{X}\text{sgn}(\boldsymbol{X}^{T}\beta_0+\varepsilon)|\varepsilon\right]\right]\\<br>            &amp; \overset{\text{Fubini定理}}{=} \mathbb{E}_{\boldsymbol{X}}\left[\mathbb{E}_{\varepsilon}\left[\boldsymbol{X}\text{sgn}(\boldsymbol{X}^{T}\beta_0+\varepsilon)|\varepsilon\right]\right]\\<br>            &amp; = \mathbb{E}_{\boldsymbol{X}}\left[F(-\boldsymbol{X}^T\beta_0)\cdot -\boldsymbol{X} + (1-F(-\boldsymbol{X}^T\beta_0))\boldsymbol{X}\right]\\<br>            &amp; = \mathbb{E}_{\boldsymbol{X}}\left[-2F(-\boldsymbol{X}^T\beta_0)\boldsymbol{X} +\boldsymbol{X}\right]\\<br>            &amp; = -2\text{Cov}(F(-\boldsymbol{X}^T\beta_0), \boldsymbol{X}).<br>\end{aligned}</p><ul><li>注意到两个高斯随机变量组成的二维随机向量，依旧服从高斯分布：</li></ul><p>\begin{equation}<br>    \begin{pmatrix}<br>        \alpha^T\boldsymbol{X} \\<br>        \beta^T\boldsymbol{X}<br>    \end{pmatrix}<br>    \sim\mathcal{N}<br>    \left(<br>    \begin{pmatrix}<br>        0 \\<br>        0<br>    \end{pmatrix},<br>    \begin{pmatrix}<br>        \alpha^T\boldsymbol{\Sigma}\alpha &amp;  \alpha^T\boldsymbol{\Sigma}\beta\\<br>        \beta^T\boldsymbol{\Sigma}\alpha &amp; \beta^T\boldsymbol{\Sigma}\beta<br>    \end{pmatrix}<br>    \right).<br>\end{equation}<br> 那么其条件期望可以直接计算<sup><a href="#fn_1" id="reffn_1">1</a></sup>得到：<br>\begin{equation}\label{equ:condexpexe9}<br>        \mathbb{E}\left[\alpha^T\boldsymbol{X}|\beta^T\boldsymbol{X}\right] = (\alpha^T\boldsymbol{\Sigma}\beta)\left((\beta^T\boldsymbol{\Sigma}\beta)^{-1}\beta^T\boldsymbol{X}\right). \tag{1}<br>\end{equation}</p><ul><li>根据\ref{equ:condexpexe9}式，令$\alpha=\text{diag}(1,\cdots,1) = \boldsymbol{E}$为单位阵有：</li></ul><p>\begin{aligned}<br>        \mathbb{E}\left[\boldsymbol{X} F(-\boldsymbol{X}^T\beta_0)\right] &amp;\overset{\text{重期望公式}}{=} \mathbb{E}\left[\mathbb{E}\left[\boldsymbol{X} F(-\boldsymbol{X}^T\beta_0)|\boldsymbol{X}^T\beta_0\right]\right]\\<br>            &amp; = \boldsymbol{\Sigma}\beta_0(\beta_0^T\boldsymbol{\Sigma}\beta_0)^{-1}\text{Cov}(\boldsymbol{X}^T \beta_0, F(-\boldsymbol{X}^T\beta_0)).<br>\end{aligned}</p><p>综上几问的结果我们有：</p><p>\begin{aligned}<br>        \beta^* &amp; = \boldsymbol{\Sigma}^{-1}\text{Cov}(\boldsymbol{X},Y) \\<br>        &amp; = -2\boldsymbol{\Sigma}^{-1}\text{Cov}(F(-\boldsymbol{X}^T\beta_0), \boldsymbol{X})\\<br>        &amp; = -2\boldsymbol{\Sigma}^{-1}\boldsymbol{\Sigma}\beta_0(\beta_0^T\boldsymbol{\Sigma}\beta_0)^{-1}\text{Cov}(\boldsymbol{X}^T \beta_0, F(-\boldsymbol{X}^T\beta_0))\\<br>        &amp; = -2\left[ (\beta_0^T\boldsymbol{\Sigma}\beta_0)^{-1}\text{Cov}(\boldsymbol{X}^T \beta_0, F(-\boldsymbol{X}^T\beta_0)) \right]\beta_0\\<br>        &amp; = c\beta_0.<br>\end{aligned}</p><p>其中，记$f$为$\varepsilon$的密度函数，令$\boldsymbol{X}^T\beta_0 = Z\sim\mathcal{N}(0,\sigma’^2)$，$\sigma’^2 = \beta_0^T\boldsymbol{\Sigma}\beta_0$有密度函数$\rho(x)$。注意到$x\rho(x) = -\sigma’^2\rho’(x)$：</p><p>\begin{aligned}<br>        \mathbb{E}\left[ZF(-Z)\right] &amp; = \int_{\mathbb{R}}xF(-x)\rho(x)\text{d} x = -\sigma’^2\int_{\mathbb{R}}F(-x)\text{d} \rho(x)\\<br>        &amp; \overset{\text{分部积分}}{=} -\sigma’^2\underbrace{\left.F(-x)\rho(x)\right|_{-\infty}^{+\infty}}_{=0} - \sigma’^2\int_{\mathbb{R}}f(-x)\rho(x)\text{d} x = - \sigma’^2\mathbb{E}\left[f(-Z)\right]\\<br>        &amp; = -\int_{\mathbb{R}}\frac{\sigma’^2}{2\pi\sigma\sigma’}\exp\left(-\frac{x^2}{2}(1/\sigma^2+1/\sigma’^2)\right)\text{d} x\\<br>        &amp; = -\frac{\sigma’^2}{\sqrt{2\pi(\sigma^2+\sigma’^2)}}.<br>\end{aligned}</p><p>因此根据上式，得到比例常数$c$：<br>\begin{aligned}<br>        c &amp;= -2(\beta_0^T\boldsymbol{\Sigma}\beta_0)^{-1}\text{Cov}(\boldsymbol{X}^T \beta_0, F(-\boldsymbol{X}^T\beta_0)) = -2(\beta_0^T\boldsymbol{\Sigma}\beta_0)^{-1}\mathbb{E}\left[\boldsymbol{X}^T \beta_0F(-\boldsymbol{X}^T\beta_0)\right]\\<br>        &amp; = 2(\beta_0^T\boldsymbol{\Sigma}\beta_0)^{-1}\frac{(\beta_0^T\boldsymbol{\Sigma}\beta_0)}{\sqrt{2\pi(\sigma^2+\beta_0^T\boldsymbol{\Sigma}\beta_0)}}\\<br>        &amp; = \sqrt{\frac{2}{\pi(\sigma^2+\beta_0^T\boldsymbol{\Sigma}\beta_0)}}.<br>\end{aligned}</p><blockquote id="fn_1"><sup>1</sup>. 该公式的证明过程可以参考<a href="https://stats.stackexchange.com/questions/30588">这里</a>.  <a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote>]]></content>
    
    
    <summary type="html">The Solution of the Final Round of the 13th National College Student Mathematics Competition (Mathematics Category, Senior Group) Q9.(in Chinese)</summary>
    
    
    
    <category term="Mathematical Competition" scheme="http://inlmouse.github.io/categories/Mathematical-Competition/"/>
    
    
    <category term="Math" scheme="http://inlmouse.github.io/tags/Math/"/>
    
    <category term="Matrix Analysis" scheme="http://inlmouse.github.io/tags/Matrix-Analysis/"/>
    
    <category term="Probability Theory" scheme="http://inlmouse.github.io/tags/Probability-Theory/"/>
    
  </entry>
  
  <entry>
    <title>Fisher Information and Its Application</title>
    <link href="http://inlmouse.github.io/fisherinfo/"/>
    <id>http://inlmouse.github.io/fisherinfo/</id>
    <published>2023-02-13T06:23:38.000Z</published>
    <updated>2023-02-13T07:01:24.018Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍Fisher信息(矩阵)的定义、性质以及其计算与一些简单应用。</p><div class="pdf-container" data-target="./Fisher_Information_and_Its_Application.pdf" data-height="800px"></div>]]></content>
    
    
    <summary type="html">This post mainly introduces the definition, properties, calculation and application of Fisher Information(Matrix).(in Chinese)</summary>
    
    
    
    <category term="Machine Learning" scheme="http://inlmouse.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Math" scheme="http://inlmouse.github.io/tags/Math/"/>
    
    <category term="Information Theory" scheme="http://inlmouse.github.io/tags/Information-Theory/"/>
    
    <category term="Statistic" scheme="http://inlmouse.github.io/tags/Statistic/"/>
    
  </entry>
  
  <entry>
    <title>电子科技大学(UESTC)数学系研究生入学考试模拟试题</title>
    <link href="http://inlmouse.github.io/UESTCGEE/"/>
    <id>http://inlmouse.github.io/UESTCGEE/</id>
    <published>2022-12-12T15:20:03.000Z</published>
    <updated>2023-02-13T06:58:41.149Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>本文收录了适用于电子科技大学(UESTC)数学系研究生入学考试模拟试题（数学分析与高等代数），提供LaTeX源文件下载。题源自2020年以前真题，裴砖，丘砖，谢惠民，其他985高校真题及其改编。不可用做商业用途。</p><h2 id="数学分析601"><a href="#数学分析601" class="headerlink" title="数学分析601"></a>数学分析601</h2><p>共计11套模拟题：<a href="https://www.overleaf.com/read/smpfwcgtxxjj">LaTeX源文件链接</a>。</p><p>样题如下：<br><div class="pdf-container" data-target="./UESTCMA8.pdf" data-height="800px"></div></p><h2 id="高等代数835"><a href="#高等代数835" class="headerlink" title="高等代数835"></a>高等代数835</h2><p>共计8套模拟题：<a href="https://www.overleaf.com/read/gbwddkmkkkjb">LaTeX源文件链接</a>。</p><p>样题如下：<br><div class="pdf-container" data-target="./UESTCAA8.pdf" data-height="800px"></div></p>]]></content>
    
    
    <summary type="html">The simulation questions for the National Graduate Entrance Examination applicable to the Department of Mathematics, UESTC(in Chinese)</summary>
    
    
    
    <category term="Graduate Entrance Examination" scheme="http://inlmouse.github.io/categories/Graduate-Entrance-Examination/"/>
    
    
    <category term="Mathematical Analysis" scheme="http://inlmouse.github.io/tags/Mathematical-Analysis/"/>
    
    <category term="GEE" scheme="http://inlmouse.github.io/tags/GEE/"/>
    
    <category term="Advanced Algebra" scheme="http://inlmouse.github.io/tags/Advanced-Algebra/"/>
    
  </entry>
  
  <entry>
    <title>Sparse Representation of the Signal: A Brief Introduction and Some Discussion</title>
    <link href="http://inlmouse.github.io/SparseRepresentation/"/>
    <id>http://inlmouse.github.io/SparseRepresentation/</id>
    <published>2022-12-02T08:04:06.352Z</published>
    <updated>2022-12-06T03:03:31.069Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Sparse-Vector-and-Sparse-Representation"><a href="#Sparse-Vector-and-Sparse-Representation" class="headerlink" title="Sparse Vector and Sparse Representation"></a>Sparse Vector and Sparse Representation</h2><p>对矩阵$\boldsymbol{A}\in \mathbb{C}^{m\times n}$ 有以下是常用的7种范数<sup><a href="#fn_1" id="reffn_1">1</a></sup>：</p><ul><li>$m_1$范数：$||\boldsymbol{A}||_{m_1}=\sum_{i=1}^{m}\sum_{j=1}^{n}|a_{ij}|.$</li><li>F范数<sup><a href="#fn_2" id="reffn_2">2</a></sup>：$||\boldsymbol{A}||_F=\sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}|a_{ij}|^2}=\sqrt{tr(\boldsymbol{A}^H\boldsymbol{A})}.$</li><li>M范数/最大范数：$||\boldsymbol{A}||_M=\max\{m,n\}\max_{i,j}|a_{ij}|.$</li><li>G范数/几何平均范数：$||\boldsymbol{A}||_G=\sqrt{mn}\max_{i,j}|a_{ij}|.$</li><li>1范数/列和范数<sup><a href="#fn_3" id="reffn_3">3</a></sup>：$||\boldsymbol{A}||_1=\max_{j}\sum_{i=1}^m|a_{ij}|.$</li><li>2范数/谱范数：$||\boldsymbol{A}||_2=\sqrt{\boldsymbol{A}^H\boldsymbol{A}\text{的最大特征值}}.$</li><li>$\infty$范数/行和范数：$||\boldsymbol{A}||_{\infty}=\max_{i}\sum_{j=1}^n|a_{ij}|.$</li></ul><blockquote id="fn_1"><sup>1</sup>. 矩阵的范数定义除开满足非负性，齐次性和三角不等式外，还需满足<strong>相容性</strong>: \begin{equation}\label{compatibility} \forall \boldsymbol{A},\boldsymbol{B}\in \mathbb{C}^{n\times n}, \exists || \boldsymbol{AB}||\leq ||\boldsymbol{A}||\cdot ||\boldsymbol{B}||. \end{equation}<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. Here, $\boldsymbol{A}^H$ means <strong>Hermitian Matrix</strong>(埃尔米特/厄米/自伴随矩阵): $\boldsymbol{A}^H=(\bar{a}_{ji})_{n\times n}$.<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. p-范数并不是按照这里给出的公式定义的，而是从属于某向量范数$||\cdot||_v$导出的矩阵范数:$||\boldsymbol{A}||=\max_{\bf{x}\neq\bf{0}}\frac{||\boldsymbol{Ax}||_v}{||\boldsymbol{x}|_v}$，简称<strong>导出范数/从属范数</strong>，且满足：$||\boldsymbol{E}||=1$.<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><h3 id="Sparse-Representation"><a href="#Sparse-Representation" class="headerlink" title="Sparse Representation"></a>Sparse Representation</h3><p>一个含有大多数零元素的向量或者矩阵成为稀疏向量(sparse vector)或者稀疏矩阵(sparse matrix)。也即是：给定$K\in\mathbb{N}^+, |\boldsymbol{x}|_0 \leq K$。</p><p>给定一个向量$\boldsymbol{x}\in\mathbb{R}^n$，可以定义如下稀疏测度(sparse measure)：<br>\begin{equation}<br>\text{sparseness}(\boldsymbol{x}) = \frac{\sqrt{n} - |\boldsymbol{x}|_1/|\boldsymbol{x}|_2}{\sqrt{n} - 1}.<br>\end{equation}</p><p>信号向量$\boldsymbol{y}\in\mathbb{R}^m$最多可以分解为$m$个正交基$\boldsymbol{g}_k\in\mathbb{R}^m$，这些正交基的集合成为完备正交基(complete orthogonal basis)。此时，信号分解</p><p>\begin{equation}<br>    \boldsymbol{y} = \boldsymbol{G}\boldsymbol{c} = \sum_{i=1}^{m}c_i\boldsymbol{g}_i,<br>\end{equation}<br>中的系数向量$\boldsymbol{c}$一定是非稀疏的。</p><p>若将信号向量$\boldsymbol{y}\in\mathbb{R}^m$分解为$n$个$m$维向量$\boldsymbol{a}_i\in\mathbb{R}$（其中$n&gt;m$）的线性组合：<br>\begin{equation}\label{equ:overcomplete}<br>\boldsymbol{y} = \boldsymbol{A}\boldsymbol{x} = \sum_{i=1}^{n}x_i\boldsymbol{a}_i. \tag{1}<br>\end{equation}<br>则$\boldsymbol{a}_i\in\mathbb{R}$不可能是正交基的集合。为区别于基，这些列向量通常称为<strong>原子</strong>(atom)或框架。由于原子数大于向量空间的维数，所以称这些原子的集合是过完备的(overcompete)。过完备的原子组成的矩阵$\boldsymbol{A}=[\boldsymbol{a}_1, \cdots , \boldsymbol{a}_n]$称为字典或者库(dictionary)。</p><p>对于字典$\boldsymbol{A}\in\mathbb{R}^{m\times n}$，可以做如下假设：</p><ul><li>$n &gt; m$；</li><li>$rank(\boldsymbol{A}) = m$；</li><li>$|\boldsymbol{a}_i|_2 = 1, i = 1, \cdots, n $；</li></ul><p>信号过完备分解式\ref{equ:overcomplete}为欠定方程，存在无穷多组解向量$\boldsymbol{x}$。求解这种欠定方程有两种常用方法：</p><ul><li>古典方法（求最小$L_2$范数解），即是：\begin{equation} \min|\boldsymbol{x}|_2, s.t. \boldsymbol{Ax} = \boldsymbol{y}. \end{equation} 这种方式的优点是：有唯一解，其物理意义为最小能量解。然而由于这种解的每个元素通常为非零值，故不符合很多实际应用的稀疏表示要求。</li><li>现代方法（求最小$L_0$范数解），即是：\begin{equation}\label{equ:sr} \min|\boldsymbol{x}|_0, s.t. \boldsymbol{Ax} = \boldsymbol{y}. \tag{2}\end{equation} 这种方式的优点是：很多实际应用只选择一个稀疏解向量。然而在计算上难以处理。</li></ul><p>假定观测向量存在加性误差或者噪声，最小$L_0$范数解为：<br>\begin{equation}\label{equ:sa}<br>\min|\boldsymbol{x}|_0, s.t. |\boldsymbol{Ax - y}|_2 \leq \varepsilon, \tag{3}<br>\end{equation}<br>其中$\varepsilon$为很小的误差或者扰动。</p><p>当系数向量$\boldsymbol{x}$是稀疏向量时，信号分解$\boldsymbol{y} = \boldsymbol{Ax}$称为（信号的）稀疏分解(sparse decomposition)。其中字典矩阵$\boldsymbol{A}$的列常称为解释变量(explanatory variables)；向量$\boldsymbol{y}$称为相应变量(response variable)或目标信号；$\boldsymbol{Ax}$称为相应的线性预测；$\boldsymbol{x}$可视为目标信号相对于字典$\boldsymbol{A}$的一种表示。</p><p>因此，称式\ref{equ:sr}是目标信号$\boldsymbol{y}$相对于字典$\boldsymbol{A}$的<strong>稀疏表示</strong>(sparse representation)，而式\ref{equ:sa}称为目标信号的<strong>稀疏逼近</strong>(sparse approximation)。</p><p>稀疏表示属于线性求逆问题(linera inverse problem)。在通信和信息论中，$\boldsymbol{A}\in\mathbb{R}^{m\times N}$ 和$\boldsymbol{x}\in\mathbb{R}^N$分别代表编码矩阵和待发送的明文，观测向量$\boldsymbol{y}\in\mathbb{R}^m$则称密文。线性求逆问题便成了解码问题：即如何从密文恢复明文。</p><h2 id="Application-Sparse-Representation-in-Face-Recognition"><a href="#Application-Sparse-Representation-in-Face-Recognition" class="headerlink" title="Application: Sparse Representation in Face Recognition"></a>Application: Sparse Representation in Face Recognition</h2><p>我们考虑close-set的人脸识别应用：假定共有$c$类目标，每一目标的脸部图像已经被向量化编码（可以是直接矩阵拉直，也可以是通过CNN进行特征提取），表示为了$m\times 1$的归一化列向量（通常我们这里的$m$为512或者128）。于是第$i$类目标的$N_i$张训练图像即可表示成$\boldsymbol{D}_i=[\boldsymbol{d}_{i,1}, \cdots , \boldsymbol{d}_{i,N_i}]\in\mathbb{R}^{m\times N}$。给定一个足够丰富的训练集$\boldsymbol{D}_{i}$，则第$i$类目标的非训练集新图片$\boldsymbol{y}$可以被表示为已知训练图像的一线性组合$\boldsymbol{y}\approx\boldsymbol{D}_i\boldsymbol{\alpha}_i$，其中$\boldsymbol{\alpha}_i$为系数向量。问题是：在实际应用中往往不知道新图像分属哪一类，而需要识别：判断该样本的属性。</p><p>于是我们已这$c$类目标的所有训练样本构造一个字典：<br>\begin{equation}<br>    \boldsymbol{D} = [\boldsymbol{D}_1, \cdots, \boldsymbol{D}_c] =  [\boldsymbol{d}_{1,1}, \cdots , \boldsymbol{d}_{1,N_1}, \cdots , \boldsymbol{d}_{c,1}, \cdots , \boldsymbol{d}_{c,N_c}]\in\mathbb{R}^{m\times N}<br>\end{equation}<br>其中$N = \sum_{i=1}^cN_i$。于是，待识别的人脸图像编码$\boldsymbol{y}$可以表示为线性组合：<br>\begin{equation}<br>    \boldsymbol{y} = \boldsymbol{D}\boldsymbol{\alpha}_0= [\boldsymbol{d}_{1,1}, \cdots , \boldsymbol{d}_{1,N_1}, \cdots , \boldsymbol{d}_{c,1}, \cdots , \boldsymbol{d}_{c,N_c}]\begin{bmatrix} \boldsymbol{0}_{N_1} \\ \vdots \\ \boldsymbol{0}_{N_{i-1}} \\ \boldsymbol{\alpha}_i \\\boldsymbol{0}_{N_{i+1}} \\ \vdots \\ \boldsymbol{0}_{N_c} \end{bmatrix}<br>\end{equation}</p><p>现在，人脸识别变成了一个矩阵方程求解的问题或者线性求你问题：已知数据向量$\boldsymbol{y}$和数据矩阵$\boldsymbol{D}$，求矩阵方程$\boldsymbol{y} = \boldsymbol{D}\boldsymbol{\alpha}_0$的解向量$\boldsymbol{\alpha}_0$。需要注意的是，通常$m &lt; N$，因为方程欠定，有无穷多解，其中<strong>最稀疏的解才是我们感兴趣的</strong>。鉴于此，问题划归为式\ref{equ:sr}的问题。</p><h2 id="Optimization-Theory-for-Solving-Sparse-Matrix-Equations"><a href="#Optimization-Theory-for-Solving-Sparse-Matrix-Equations" class="headerlink" title="Optimization Theory for Solving Sparse Matrix Equations"></a>Optimization Theory for Solving Sparse Matrix Equations</h2><h3 id="L-1-Norm-Minimization"><a href="#L-1-Norm-Minimization" class="headerlink" title="$L_1$ Norm Minimization"></a>$L_1$ Norm Minimization</h3><p>$L_1$范数最小化也称为$L_1$线性规划或者$L_1$范数正则化最小二乘。</p><p>直接求解优化问题P0，必须筛选出系数向量$\boldsymbol{x}$中所有可能的非零元素。这个方法是<strong>不可跟踪的</strong>(untractable)或者NP hard<sup><a href="#fn_4" id="reffn_4">4</a></sup>的，因为搜索空间过于庞大。</p><p>向量$\boldsymbol{x}$的非零元素指标集称为<strong>支撑集</strong>，记为$\text{supp}(\boldsymbol{x}) = \{i:x_i\neq 0\}$，支撑集的长度即是$L_0$拟范数<sup><a href="#fn_5" id="reffn_5">5</a></sup>：</p><p>\begin{equation}<br>|\boldsymbol{x}|_0 = |supp(\boldsymbol{x})|.<br>\end{equation}</p><p>K-稀疏向量的集合记为$\Sigma_K = \{\boldsymbol{x}\in\mathbb{R}^N:|\boldsymbol{x}|_0\leq K\}$。若$\hat{\boldsymbol{x}}\in\Sigma_K$，则称向量$\hat{\boldsymbol{x}}$是$\boldsymbol{x}$的K-项逼近或者K-稀疏逼近。</p><p>一般地，称向量$\hat{\boldsymbol{x}}$是$\boldsymbol{x}$在$L_p$范数<sup><a href="#fn_6" id="reffn_6">6</a></sup>下的K-稀疏逼近，若：<br>\begin{equation}<br>|\boldsymbol{x} - \hat{\boldsymbol{x}}|_p = \inf_{\boldsymbol{z}\in\Sigma_K}|\boldsymbol{x} - \boldsymbol{z}|_p.<br>\end{equation}</p><p>显然$L_0$是$L_p$范数范数的特殊形式：$|\boldsymbol{x}|_0 = \lim\limits_{p\to 0}|\boldsymbol{x}|_p^p$。由于当且仅当$p\geq 1$时$|\boldsymbol{x}|_p$为凸函数，所以$L_1$范数时最接近于$L_0$拟范数的凸目标函数。于是从最优化角度讲，称$L_1$范数是$L_0$拟范数的凸松弛[1]。因此，<strong>$L_0$拟范数最小化问题便可以转变为凸松弛的$L_1$范数最小化问题</strong>：<br>\begin{equation}\label{equ:l1sr}<br>\min_{\boldsymbol{x}}|\boldsymbol{x}|_1, s.t. \boldsymbol{y} = \boldsymbol{Ax}.\tag{4}<br>\end{equation}<br>由于$|\boldsymbol{x}|_1$是凸函数，并且约束等式$\boldsymbol{y} = \boldsymbol{Ax}$为一个仿射变换，因此这是一个凸优化问题。</p><p>存在观测噪声的情况下，等式约束可以松弛为不等式约束的最优化问题（$L_1$最小化）：<br>\begin{equation}\label{equ:l1sr-e}<br>\min_{\boldsymbol{x}}|\boldsymbol{x}|_1, s.t. |\boldsymbol{y} - \boldsymbol{Ax}| \leq \varepsilon.\tag{5}<br>\end{equation}</p><p>$L_1$范数下的最优化问题又称为基追踪(base pursuit, BP)。这是一个二次约束线性规划问题(quadratically constrained linear problem, QCLP)。</p><p>若$\boldsymbol{x}_1$是$L_1$的解，$\boldsymbol{x}_0$是$L_0$的解，则有[2]：<br>\begin{equation}<br>|\boldsymbol{x}_1|_1 \leq |\boldsymbol{x}_0|_1.<br>\end{equation}<br>因为$\boldsymbol{x}_1$是可行解，$\boldsymbol{x}_0$是最优解。同时$A\boldsymbol{x}_1 = A\boldsymbol{x}_0$。</p><p>同样的，式\ref{equ:l1sr-e}也有两种变形：</p><ul><li>（$L_1$惩罚最小化）利用$\boldsymbol{x}$是K稀疏向量的约束，将$L_1$不等式范数最小化变成$L_2$：    \begin{equation}\min_{\boldsymbol{x}}\frac{1}{2}|\boldsymbol{y}- \boldsymbol{Ax}|_2^2,s.t.|\boldsymbol{x}|\leq q.    \end{equation}    划归为一类二次规划(quadratic program, QP)问题。</li><li>利用Lagrangian乘子法，将$L_1$不等式范数最小化变成：    \begin{equation}\label{equ:presu-Tik}    \min_{\lambda, \boldsymbol{x}}\frac{1}{2}|\boldsymbol{y}- \boldsymbol{Ax}|_2^2 + \lambda|\boldsymbol{x}|_1.    \end{equation}    划归为一类基追踪去噪(basis pursuit denoising, BPDN)问题[3]。</li></ul><p>在基于小波变换的图像/信号重构和恢复（deconv）中，也经常会遇到基追踪去噪问题。</p><p>参数稀疏的好处主要有以下两点：</p><ul><li>特征选择(Feature Selection)</li><li>可解释性(Interpretability)</li></ul><blockquote id="fn_4"><sup>4</sup>. 指所有NP问题都能在多项式时间复杂度内归约到的问题.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. $L_0$范数不满足范数公理中的齐次性：$|c\boldsymbol{x}|_0 = |c||\boldsymbol{x}|_0$，故严格来讲它是一种虚拟的范数，也称拟范数。<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. $\forall p\in\mathbb{R}^+, |\boldsymbol{x}|_p = \left( \sum_{i\in supp(\boldsymbol{x})}|x_i|^p\right)^{1/p} $.<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote><h3 id="Restricted-Isometry-Property-Condition"><a href="#Restricted-Isometry-Property-Condition" class="headerlink" title="Restricted Isometry Property Condition"></a>Restricted Isometry Property Condition</h3><p>前文讨论了$L_1$范数最小化问题是$L_0$范数最小化某种程度的凸松弛。接下来考察两种问题的解之间的关系。<br><strong>Definition 1.(Restricted Isometry Property Condition)</strong>[4][5]<br><em>若存在矩阵$\boldsymbol{A}$和K-稀疏向量$|\boldsymbol{x}|_0\leq K$，<br>    \begin{equation}<br>        (1-\delta_K)|\boldsymbol{x}|_2^2 \leq |\boldsymbol{A}_K\boldsymbol{x}|_2^2 \leq (1 + \delta_K)|\boldsymbol{x}|_2^2,<br>    \end{equation}<br>    其中$0\leq \delta_K &lt; 1$是一个与稀疏度K有关的常数(约束等距常数, restricted isometry constants, RIC)，$\boldsymbol{A}_K$是字典矩阵$\boldsymbol{A}$的任意K列组成的子矩阵。则称矩阵$\boldsymbol{A}$满足K阶RIP条件。</em></p><p><strong>当RIP条件满足时，非凸的$L_0$范数最小化问题与$L_1$范数最小化问题等价。</strong>也即是：<br>\begin{equation}<br>\min||\boldsymbol{x}||_0\,\,s.t. \boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}\overset{\text{概率为1 的}}{\underset{}{\iff}}\min||\boldsymbol{x}||_1\,\,s.t. \boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}<br>\end{equation}</p><p>带参数$\delta_K$的K阶RIP条件简记为$RIP(K,\delta_K)$，定义为所有使$RIP(K,\delta_K)$成立的参数$\delta$的下确界：<br>\begin{equation}<br>    \delta_K = \inf\left\lbrace \delta: (1-\delta)|\boldsymbol{z}|_2^2 \leq |\boldsymbol{A}_{supp(z)}\boldsymbol{z}|_2^2 \leq (1 + \delta)|\boldsymbol{z}|_2^2, \forall |supp(z)| \leq K, \forall \boldsymbol{z}\in\mathbb{R}^{|supp(\boldsymbol{z})|} \right\rbrace<br>\end{equation}<br>显然若$\boldsymbol{A}_K$正交，则$\delta_K = 0$。于是，RIC的非零值实际上可以评价该矩阵的非正交程度。此外，由于$\boldsymbol{A}_K$的任意性，要求$\boldsymbol{A}$在每一列的能量分布投影尽可能均匀。</p><p>RIC有三个重要性质：</p><ul><li>系数信号精确重构的充分条件[6]： 若字典矩阵$\boldsymbol{A}$分别满足$\delta_K, \delta_{2K}, \delta_{3K}$的RIP条件，并且：    \begin{equation}\delta_K + \delta_{2K} + \delta_{3K} &lt; 1.\end{equation}        则$L_1$范数最小化可以精确重构所有K稀疏信号。也即是，在此条件下，若无噪声存在，则K稀疏信号可以确保由$L_1$范数最小化精确恢复；并且在有噪声的情况下可以稳定估计。</li><li>RIC与特征值的关系[7]：若字典矩阵$\boldsymbol{A}\in\mathbb{R}^{m\times n}$满足$RIP(K,\delta_K)$，则：\begin{equation}1 - \delta_K \leq \lambda_{min}(\boldsymbol{A}_K^T\boldsymbol{A}_K) \leq \lambda_{max}(\boldsymbol{A}_K^T\boldsymbol{A}_K) \leq 1 + \delta_K.\end{equation}</li><li>单调性[6]：若$K\leq K’$，则$\delta_K \leq \delta_{K’}$.</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]: Tropp, J. A. . “Algorithms for simultaneous sparse approximation. Part II: Convex relaxation.” Signal Processing 86.3(2006):589-602.<br>[2]: David, L., and Donoho. “For most large underdetermined systems of linear equations the minimal 1-norm solution is also the sparsest solution.” Communications on Pure and Applied Mathematics (2006).<br>[3]: Chen, S. S. . “Atomic decomposition by basis persuit.” Siam J Sci Comp 20(1999).<br>[4]: Dandes, E. J. . “Near-optimal signal recovery from random projections.” Universal encoding strategies IEEE Transactions on Information Theory 52(2006).<br>[5]: Foucart, Simon , and  M. J. Lai . “Sparsest solutions of underdetermined linear systems via $0{    extbackslashell$0q$0-minimization for $00{    extlessq{    extbackslashleq 1$0.” (2009).<br>[6]: Cai, T. T. ,  L. Wang , and  G. Xu . “New bounds for restricted isometry constants.” IEEE Press (2010).<br>[7]: Dai, W. , and  O. Milenkovic . “Subspace Pursuit for Compressive Sensing Signal Reconstruction.” IEEE Transactions on Information Theory 55.5(2009).</p>]]></content>
    
    
    <summary type="html">使用少量基本信号的线性组合表示一目标信号，称为信号的稀疏表示。信号的稀疏表示是过去近25年来信号处理界一个非常引人关注的研究领域（现在凉透了），众多研究论文和专题研讨会表明了该领域曾经的的蓬勃发展。信号稀疏表示的目的就是在给定的超完备字典中用尽可能少的原子来表示信号，可以获得信号更为简洁的表示方式，从而使我们更容易地获取信号中所蕴含的信息，更方便进一步对信号进行加工处理，如压缩、编码等。(in Chinese)</summary>
    
    
    
    <category term="Signal Processing" scheme="http://inlmouse.github.io/categories/Signal-Processing/"/>
    
    
    <category term="Math" scheme="http://inlmouse.github.io/tags/Math/"/>
    
    <category term="Machine Learning" scheme="http://inlmouse.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>From Cross Entropy to Metric Learning: A Brief Introduction and Some Discussion</title>
    <link href="http://inlmouse.github.io/CEMLE/"/>
    <id>http://inlmouse.github.io/CEMLE/</id>
    <published>2022-12-01T04:13:00.000Z</published>
    <updated>2022-12-12T14:32:47.836Z</updated>
    
    <content type="html"><![CDATA[<h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><p>信息熵是定义信息量的一种测度，如同热力学中定义一样[1]：熵越大混乱程度越大，信息熵越大信息量越大。因此信息熵在直观上需要满足以下两点要求：</p><ul><li>越不可能($p(x)$越小)发生的事件($x$)信息量越大，确定事件($p(x)$很大)的信息量很小；</li><li>独立事件的信息量可叠加；</li></ul><p>满足以上要求的测度定义方式并不唯一，但是数学严谨化之后满足性质的熵几乎是唯一的<sup><a href="#fn_1" id="reffn_1">1</a></sup>。于是有如下定理：</p><p><strong>Theorem 1</strong>(离散信息熵表述唯一性定理[2]). <em>任何具有上述性质的离散的熵，其函数形式必为（Shannon熵 或von Neumann熵）和（Shannon熵 或Hartley 熵）的线性组合。</em></p><p>因此在此定义：<br><strong>Definition 1</strong> (Shannon熵). <em>若$p(x_i)$表随机事件$X$观测为$x_i$的概率，则Shannon熵：<br>\begin{equation}<br>H(X) = -\sum_{i=1}^np(x_i)\log_ap(x_i).<br>\end{equation}<br>这里如果$a=e$则$H(X)$的单位为奈培(NP)，如果$a=2$则$H(X)$的单位为比特(bit)，实际的不同只会让相差一个常系数，并不影响其实际意义。</em></p><blockquote id="fn_1"><sup>1</sup>. 在数学上需要严格满足以下三点：1. 不变性：相空间的熵在保测度正则变换下不变，量子熵在unitary变换下不变；2. 可加性或者泛可加性(subadditive)；3. 连续延拓后需要是凸函数；<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><h2 id="相对熵-KL-Divergence"><a href="#相对熵-KL-Divergence" class="headerlink" title="相对熵:KL Divergence"></a>相对熵:KL Divergence</h2><p>相对熵又称KL散度(Kullback-Leibler Divergence)，是衡量两个事件或者分布之间相似度的度量。当然这个度量也不唯一，但是那是另外一个话题。</p><p><strong>Definition 2</strong> (KL散度). <em>对于随机事件$X$和$Y$，其概率分布分别为$p(x),q(x)$，则KL散度：<br>\begin{equation}\label{equ:KL}<br>D_{KL}(X||Y) = \sum_{i=1}^np(x_i)\log_a\dfrac{p(x_i)}{q(x_i)}. \tag{1}<br>\end{equation}</em></p><p>值得注意的是\ref{equ:KL}式不满足对称性($D_{KL}(X||Y) \neq D_{KL}(Y||X)$)和三角不等式，这一点在JS散度中得到了改善。</p><h2 id="交叉熵-Cross-Entropy"><a href="#交叉熵-Cross-Entropy" class="headerlink" title="交叉熵:Cross Entropy"></a>交叉熵:Cross Entropy</h2><p>考查\ref{equ:KL}式：<br>\begin{equation}<br>D_{KL}(X||Y) = -H(X)-\sum_{i=1}^np(x_i)\log_aq(x_i).<br>\end{equation}<br>注意到在实际使用中往往用$p(x)$来表示样本的真实分布，那么$H(X)$的值往往是不变的，因而直接考察后一项即可。</p><p><strong>Definition 3</strong> (交叉熵). <em>对于随机事件$X$和$Y$，其概率分布分别为$p(x),q(x)$，则交叉熵：<br>\begin{equation}<br>H(X,Y) = -\sum_{i=1}^np(x_i)\log_aq(x_i).<br>\end{equation}</em></p><h2 id="交叉熵的MLE-最大似然估计-解释"><a href="#交叉熵的MLE-最大似然估计-解释" class="headerlink" title="交叉熵的MLE(最大似然估计)解释"></a>交叉熵的MLE(最大似然估计)解释</h2><p>现在有一个真实分布为$p(x)$的随机变量$X$，我们对它进行了$N$次独立同分布实验，对于每个可能的结果$x_i(i=1,\cdots,n)$观察到的次数为$N(x_i)$，显然$\sum_{i=1}^nN(x_i)=N$，那么它的似然值就可以写成：<br>\begin{equation}<br>L=\prod_{i=1}^np(x_i)^{N(x_i)}.<br>\end{equation}</p><p>考察其对数似然值：<br>\begin{equation}\label{equ:lnl}<br>\ln L=\sum_{i=1}^nN(x_i)\ln p(x_i).\tag{2}<br>\end{equation}</p><p>\ref{equ:lnl}式有两个缺点，其一它是个负数，其二它的数值跟样本数有关，样本越多数值越小。因此除以总的样本数归一化，再取相反数，然后改用频率表示：<br>\begin{equation}<br>-\frac{\ln L}{N}=-\sum_{i=1}^n\frac{N(x_i)}{N}\ln p(x_i) = -\sum_{i=1}^n q(x_i)\ln p(x_i) =H(Y,X),<br>\end{equation}<br>显然$\frac{N(x_i)}{N}$即是观测到的概率$q(x_i)$。</p><p>下面在给定$q(x)$的情况下考察$-\dfrac{\ln L}{N}$的最小值时$p(x)$的取值，考虑拉格朗日乘子法，考察拉式量：<br>\begin{equation}<br>W=-\sum_{i=1}^n q(x_i)\ln p(x_i) + \lambda(\sum_{i=1}^np(x_i)-1).<br>\end{equation}<br>求偏导可得：</p><p>\begin{equation}<br>-\frac{q(x)}{p(x)}+\lambda=0,<br>\end{equation}<br>即是$\forall i, p(x_i),q(x_i)$成比例，再由概率归一化条件知：<br>\begin{equation}<br>p(x)=q(x).<br>\end{equation}</p><center><strong>因此可以看出，交叉熵最小实质上就是似然值最大。</strong></center><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]: Witten, E. . “A Mini-Introduction To Information Theory.”, 10.1007/s40766-020-00004-5. 2018.<br>[2]: Aczel, Janos. et al. “Why the Shannon and Hartley entropies are ‘natural’”, 131—146/Advances in applied probability. 1974.</p>]]></content>
    
    
    <summary type="html">Some discussion of Cross Entropy Loss and its equivalence between MLE.(in Chinese)</summary>
    
    
    
    <category term="Machine Learning" scheme="http://inlmouse.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Math" scheme="http://inlmouse.github.io/tags/Math/"/>
    
    <category term="Information Theory" scheme="http://inlmouse.github.io/tags/Information-Theory/"/>
    
  </entry>
  
  <entry>
    <title>沉痛悼念江泽民同志</title>
    <link href="http://inlmouse.github.io/deepmournjzm/"/>
    <id>http://inlmouse.github.io/deepmournjzm/</id>
    <published>2022-11-30T04:13:00.000Z</published>
    <updated>2022-12-02T15:59:32.629Z</updated>
    
    <content type="html"><![CDATA[<img src="/deepmournjzm/jzm.jpg" class width="320" height="445" title="永垂不朽"> <center>江泽民同志遗像。新华社照片，北京，2022年11月30日。</center><blockquote><p>我们敬爱的江泽民同志患白血病合并多脏器功能衰竭，抢救无效，于2022年11月30日12时13分在上海逝世，享年96岁。</p><footer><strong>新华社北京11月30日电,</strong><cite><a href="http://sethgodin.typepad.com/seths_blog/2009/07/welcome-to-island-marketing.html">告全党全军全国各族人民书</a></cite></footer></blockquote><center><strong>吹拉弹唱，我的太阳成绝响； 印俄日德，葛底斯堡仍绕梁。 High Level!</strong></center>]]></content>
    
    
    <summary type="html">苟利国家生死以，岂因祸福避趋之。(in Chinese)</summary>
    
    
    
    <category term="Political Commentary" scheme="http://inlmouse.github.io/categories/Political-Commentary/"/>
    
    
    <category term="politic" scheme="http://inlmouse.github.io/tags/politic/"/>
    
  </entry>
  
</feed>
