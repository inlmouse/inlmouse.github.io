<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Strategic Research Office of Individual Eleven</title>
  
  <subtitle>Frontier Explorer</subtitle>
  <link href="http://inlmouse.github.io/atom.xml" rel="self"/>
  
  <link href="http://inlmouse.github.io/"/>
  <updated>2022-12-19T14:23:16.664Z</updated>
  <id>http://inlmouse.github.io/</id>
  
  <author>
    <name>Patrick Sylvestre</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>电子科技大学(UESTC)数学系研究生入学考试数模拟试题</title>
    <link href="http://inlmouse.github.io/UESTCGEE/"/>
    <id>http://inlmouse.github.io/UESTCGEE/</id>
    <published>2022-12-12T15:20:03.000Z</published>
    <updated>2022-12-19T14:23:16.664Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>本文收录了适用于电子科技大学(UESTC)数学系研究生入学考试模拟试题（数学分析与高等代数），提供LaTeX源文件下载。题源自2020年以前真题，裴砖，丘砖，谢惠民，其他985高校真题及其改编。不可用做商业用途。</p><h2 id="数学分析601"><a href="#数学分析601" class="headerlink" title="数学分析601"></a>数学分析601</h2><p>共计11套模拟题：<a href="https://www.overleaf.com/read/smpfwcgtxxjj">LaTeX源文件链接</a>。</p><p>样题如下：</p><object data="./UESTCMA8.pdf" type="application/pdf" width="100%" height="100%"><iframe src="./UESTCMA8.pdf" width="100%" height="100%" style="border: none;">This browser does not support PDFs. Please download the PDF to view it: <a href="./UESTCMA8.pdf">Download PDF</a></iframe></object><h2 id="高等代数835"><a href="#高等代数835" class="headerlink" title="高等代数835"></a>高等代数835</h2><p>共计8套模拟题：<a href="https://www.overleaf.com/read/gbwddkmkkkjb">LaTeX源文件链接</a>。</p><p>样题如下：</p><object data="./UESTCAA8.pdf" type="application/pdf" width="100%" height="100%"><iframe src="./UESTCAA8.pdf" width="100%" height="100%" style="border: none;">This browser does not support PDFs. Please download the PDF to view it: <a href="./UESTCAA8.pdf">Download PDF</a></iframe></object>]]></content>
    
    
    <summary type="html">The simulation questions for the National Graduate Entrance Examination applicable to the Department of Mathematics, UESTC(in Chinese)</summary>
    
    
    
    <category term="Graduate Entrance Examination" scheme="http://inlmouse.github.io/categories/Graduate-Entrance-Examination/"/>
    
    
    <category term="Mathematical Analysis" scheme="http://inlmouse.github.io/tags/Mathematical-Analysis/"/>
    
    <category term="GEE" scheme="http://inlmouse.github.io/tags/GEE/"/>
    
  </entry>
  
  <entry>
    <title>Sparse Representation of the Signal: A Brief Introduction and Some Discussion</title>
    <link href="http://inlmouse.github.io/SparseRepresentation/"/>
    <id>http://inlmouse.github.io/SparseRepresentation/</id>
    <published>2022-12-02T08:04:06.352Z</published>
    <updated>2022-12-06T03:03:31.069Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Sparse-Vector-and-Sparse-Representation"><a href="#Sparse-Vector-and-Sparse-Representation" class="headerlink" title="Sparse Vector and Sparse Representation"></a>Sparse Vector and Sparse Representation</h2><p>对矩阵$\boldsymbol{A}\in \mathbb{C}^{m\times n}$ 有以下是常用的7种范数<sup><a href="#fn_1" id="reffn_1">1</a></sup>：</p><ul><li>$m_1$范数：$||\boldsymbol{A}||_{m_1}=\sum_{i=1}^{m}\sum_{j=1}^{n}|a_{ij}|.$</li><li>F范数<sup><a href="#fn_2" id="reffn_2">2</a></sup>：$||\boldsymbol{A}||_F=\sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}|a_{ij}|^2}=\sqrt{tr(\boldsymbol{A}^H\boldsymbol{A})}.$</li><li>M范数/最大范数：$||\boldsymbol{A}||_M=\max\{m,n\}\max_{i,j}|a_{ij}|.$</li><li>G范数/几何平均范数：$||\boldsymbol{A}||_G=\sqrt{mn}\max_{i,j}|a_{ij}|.$</li><li>1范数/列和范数<sup><a href="#fn_3" id="reffn_3">3</a></sup>：$||\boldsymbol{A}||_1=\max_{j}\sum_{i=1}^m|a_{ij}|.$</li><li>2范数/谱范数：$||\boldsymbol{A}||_2=\sqrt{\boldsymbol{A}^H\boldsymbol{A}\text{的最大特征值}}.$</li><li>$\infty$范数/行和范数：$||\boldsymbol{A}||_{\infty}=\max_{i}\sum_{j=1}^n|a_{ij}|.$</li></ul><blockquote id="fn_1"><sup>1</sup>. 矩阵的范数定义除开满足非负性，齐次性和三角不等式外，还需满足<strong>相容性</strong>: \begin{equation}\label{compatibility} \forall \boldsymbol{A},\boldsymbol{B}\in \mathbb{C}^{n\times n}, \exists || \boldsymbol{AB}||\leq ||\boldsymbol{A}||\cdot ||\boldsymbol{B}||. \end{equation}<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><blockquote id="fn_2"><sup>2</sup>. Here, $\boldsymbol{A}^H$ means <strong>Hermitian Matrix</strong>(埃尔米特/厄米/自伴随矩阵): $\boldsymbol{A}^H=(\bar{a}_{ji})_{n\times n}$.<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a></blockquote><blockquote id="fn_3"><sup>3</sup>. p-范数并不是按照这里给出的公式定义的，而是从属于某向量范数$||\cdot||_v$导出的矩阵范数:$||\boldsymbol{A}||=\max_{\bf{x}\neq\bf{0}}\frac{||\boldsymbol{Ax}||_v}{||\boldsymbol{x}|_v}$，简称<strong>导出范数/从属范数</strong>，且满足：$||\boldsymbol{E}||=1$.<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a></blockquote><h3 id="Sparse-Representation"><a href="#Sparse-Representation" class="headerlink" title="Sparse Representation"></a>Sparse Representation</h3><p>一个含有大多数零元素的向量或者矩阵成为稀疏向量(sparse vector)或者稀疏矩阵(sparse matrix)。也即是：给定$K\in\mathbb{N}^+, |\boldsymbol{x}|_0 \leq K$。</p><p>给定一个向量$\boldsymbol{x}\in\mathbb{R}^n$，可以定义如下稀疏测度(sparse measure)：<br>\begin{equation}<br>\text{sparseness}(\boldsymbol{x}) = \frac{\sqrt{n} - |\boldsymbol{x}|_1/|\boldsymbol{x}|_2}{\sqrt{n} - 1}.<br>\end{equation}</p><p>信号向量$\boldsymbol{y}\in\mathbb{R}^m$最多可以分解为$m$个正交基$\boldsymbol{g}_k\in\mathbb{R}^m$，这些正交基的集合成为完备正交基(complete orthogonal basis)。此时，信号分解</p><p>\begin{equation}<br>    \boldsymbol{y} = \boldsymbol{G}\boldsymbol{c} = \sum_{i=1}^{m}c_i\boldsymbol{g}_i,<br>\end{equation}<br>中的系数向量$\boldsymbol{c}$一定是非稀疏的。</p><p>若将信号向量$\boldsymbol{y}\in\mathbb{R}^m$分解为$n$个$m$维向量$\boldsymbol{a}_i\in\mathbb{R}$（其中$n&gt;m$）的线性组合：<br>\begin{equation}\label{equ:overcomplete}<br>\boldsymbol{y} = \boldsymbol{A}\boldsymbol{x} = \sum_{i=1}^{n}x_i\boldsymbol{a}_i. \tag{1}<br>\end{equation}<br>则$\boldsymbol{a}_i\in\mathbb{R}$不可能是正交基的集合。为区别于基，这些列向量通常称为<strong>原子</strong>(atom)或框架。由于原子数大于向量空间的维数，所以称这些原子的集合是过完备的(overcompete)。过完备的原子组成的矩阵$\boldsymbol{A}=[\boldsymbol{a}_1, \cdots , \boldsymbol{a}_n]$称为字典或者库(dictionary)。</p><p>对于字典$\boldsymbol{A}\in\mathbb{R}^{m\times n}$，可以做如下假设：</p><ul><li>$n &gt; m$；</li><li>$rank(\boldsymbol{A}) = m$；</li><li>$|\boldsymbol{a}_i|_2 = 1, i = 1, \cdots, n $；</li></ul><p>信号过完备分解式\ref{equ:overcomplete}为欠定方程，存在无穷多组解向量$\boldsymbol{x}$。求解这种欠定方程有两种常用方法：</p><ul><li>古典方法（求最小$L_2$范数解），即是：\begin{equation} \min|\boldsymbol{x}|_2, s.t. \boldsymbol{Ax} = \boldsymbol{y}. \end{equation} 这种方式的优点是：有唯一解，其物理意义为最小能量解。然而由于这种解的每个元素通常为非零值，故不符合很多实际应用的稀疏表示要求。</li><li>现代方法（求最小$L_0$范数解），即是：\begin{equation}\label{equ:sr} \min|\boldsymbol{x}|_0, s.t. \boldsymbol{Ax} = \boldsymbol{y}. \tag{2}\end{equation} 这种方式的优点是：很多实际应用只选择一个稀疏解向量。然而在计算上难以处理。</li></ul><p>假定观测向量存在加性误差或者噪声，最小$L_0$范数解为：<br>\begin{equation}\label{equ:sa}<br>\min|\boldsymbol{x}|_0, s.t. |\boldsymbol{Ax - y}|_2 \leq \varepsilon, \tag{3}<br>\end{equation}<br>其中$\varepsilon$为很小的误差或者扰动。</p><p>当系数向量$\boldsymbol{x}$是稀疏向量时，信号分解$\boldsymbol{y} = \boldsymbol{Ax}$称为（信号的）稀疏分解(sparse decomposition)。其中字典矩阵$\boldsymbol{A}$的列常称为解释变量(explanatory variables)；向量$\boldsymbol{y}$称为相应变量(response variable)或目标信号；$\boldsymbol{Ax}$称为相应的线性预测；$\boldsymbol{x}$可视为目标信号相对于字典$\boldsymbol{A}$的一种表示。</p><p>因此，称式\ref{equ:sr}是目标信号$\boldsymbol{y}$相对于字典$\boldsymbol{A}$的<strong>稀疏表示</strong>(sparse representation)，而式\ref{equ:sa}称为目标信号的<strong>稀疏逼近</strong>(sparse approximation)。</p><p>稀疏表示属于线性求逆问题(linera inverse problem)。在通信和信息论中，$\boldsymbol{A}\in\mathbb{R}^{m\times N}$ 和$\boldsymbol{x}\in\mathbb{R}^N$分别代表编码矩阵和待发送的明文，观测向量$\boldsymbol{y}\in\mathbb{R}^m$则称密文。线性求逆问题便成了解码问题：即如何从密文恢复明文。</p><h2 id="Application-Sparse-Representation-in-Face-Recognition"><a href="#Application-Sparse-Representation-in-Face-Recognition" class="headerlink" title="Application: Sparse Representation in Face Recognition"></a>Application: Sparse Representation in Face Recognition</h2><p>我们考虑close-set的人脸识别应用：假定共有$c$类目标，每一目标的脸部图像已经被向量化编码（可以是直接矩阵拉直，也可以是通过CNN进行特征提取），表示为了$m\times 1$的归一化列向量（通常我们这里的$m$为512或者128）。于是第$i$类目标的$N_i$张训练图像即可表示成$\boldsymbol{D}_i=[\boldsymbol{d}_{i,1}, \cdots , \boldsymbol{d}_{i,N_i}]\in\mathbb{R}^{m\times N}$。给定一个足够丰富的训练集$\boldsymbol{D}_{i}$，则第$i$类目标的非训练集新图片$\boldsymbol{y}$可以被表示为已知训练图像的一线性组合$\boldsymbol{y}\approx\boldsymbol{D}_i\boldsymbol{\alpha}_i$，其中$\boldsymbol{\alpha}_i$为系数向量。问题是：在实际应用中往往不知道新图像分属哪一类，而需要识别：判断该样本的属性。</p><p>于是我们已这$c$类目标的所有训练样本构造一个字典：<br>\begin{equation}<br>    \boldsymbol{D} = [\boldsymbol{D}_1, \cdots, \boldsymbol{D}_c] =  [\boldsymbol{d}_{1,1}, \cdots , \boldsymbol{d}_{1,N_1}, \cdots , \boldsymbol{d}_{c,1}, \cdots , \boldsymbol{d}_{c,N_c}]\in\mathbb{R}^{m\times N}<br>\end{equation}<br>其中$N = \sum_{i=1}^cN_i$。于是，待识别的人脸图像编码$\boldsymbol{y}$可以表示为线性组合：<br>\begin{equation}<br>    \boldsymbol{y} = \boldsymbol{D}\boldsymbol{\alpha}_0= [\boldsymbol{d}_{1,1}, \cdots , \boldsymbol{d}_{1,N_1}, \cdots , \boldsymbol{d}_{c,1}, \cdots , \boldsymbol{d}_{c,N_c}]\begin{bmatrix} \boldsymbol{0}_{N_1} \\ \vdots \\ \boldsymbol{0}_{N_{i-1}} \\ \boldsymbol{\alpha}_i \\\boldsymbol{0}_{N_{i+1}} \\ \vdots \\ \boldsymbol{0}_{N_c} \end{bmatrix}<br>\end{equation}</p><p>现在，人脸识别变成了一个矩阵方程求解的问题或者线性求你问题：已知数据向量$\boldsymbol{y}$和数据矩阵$\boldsymbol{D}$，求矩阵方程$\boldsymbol{y} = \boldsymbol{D}\boldsymbol{\alpha}_0$的解向量$\boldsymbol{\alpha}_0$。需要注意的是，通常$m &lt; N$，因为方程欠定，有无穷多解，其中<strong>最稀疏的解才是我们感兴趣的</strong>。鉴于此，问题划归为式\ref{equ:sr}的问题。</p><h2 id="Optimization-Theory-for-Solving-Sparse-Matrix-Equations"><a href="#Optimization-Theory-for-Solving-Sparse-Matrix-Equations" class="headerlink" title="Optimization Theory for Solving Sparse Matrix Equations"></a>Optimization Theory for Solving Sparse Matrix Equations</h2><h3 id="L-1-Norm-Minimization"><a href="#L-1-Norm-Minimization" class="headerlink" title="$L_1$ Norm Minimization"></a>$L_1$ Norm Minimization</h3><p>$L_1$范数最小化也称为$L_1$线性规划或者$L_1$范数正则化最小二乘。</p><p>直接求解优化问题P0，必须筛选出系数向量$\boldsymbol{x}$中所有可能的非零元素。这个方法是<strong>不可跟踪的</strong>(untractable)或者NP hard<sup><a href="#fn_4" id="reffn_4">4</a></sup>的，因为搜索空间过于庞大。</p><p>向量$\boldsymbol{x}$的非零元素指标集称为<strong>支撑集</strong>，记为$\text{supp}(\boldsymbol{x}) = \{i:x_i\neq 0\}$，支撑集的长度即是$L_0$拟范数<sup><a href="#fn_5" id="reffn_5">5</a></sup>：</p><p>\begin{equation}<br>|\boldsymbol{x}|_0 = |supp(\boldsymbol{x})|.<br>\end{equation}</p><p>K-稀疏向量的集合记为$\Sigma_K = \{\boldsymbol{x}\in\mathbb{R}^N:|\boldsymbol{x}|_0\leq K\}$。若$\hat{\boldsymbol{x}}\in\Sigma_K$，则称向量$\hat{\boldsymbol{x}}$是$\boldsymbol{x}$的K-项逼近或者K-稀疏逼近。</p><p>一般地，称向量$\hat{\boldsymbol{x}}$是$\boldsymbol{x}$在$L_p$范数<sup><a href="#fn_6" id="reffn_6">6</a></sup>下的K-稀疏逼近，若：<br>\begin{equation}<br>|\boldsymbol{x} - \hat{\boldsymbol{x}}|_p = \inf_{\boldsymbol{z}\in\Sigma_K}|\boldsymbol{x} - \boldsymbol{z}|_p.<br>\end{equation}</p><p>显然$L_0$是$L_p$范数范数的特殊形式：$|\boldsymbol{x}|_0 = \lim\limits_{p\to 0}|\boldsymbol{x}|_p^p$。由于当且仅当$p\geq 1$时$|\boldsymbol{x}|_p$为凸函数，所以$L_1$范数时最接近于$L_0$拟范数的凸目标函数。于是从最优化角度讲，称$L_1$范数是$L_0$拟范数的凸松弛[1]。因此，<strong>$L_0$拟范数最小化问题便可以转变为凸松弛的$L_1$范数最小化问题</strong>：<br>\begin{equation}\label{equ:l1sr}<br>\min_{\boldsymbol{x}}|\boldsymbol{x}|_1, s.t. \boldsymbol{y} = \boldsymbol{Ax}.\tag{4}<br>\end{equation}<br>由于$|\boldsymbol{x}|_1$是凸函数，并且约束等式$\boldsymbol{y} = \boldsymbol{Ax}$为一个仿射变换，因此这是一个凸优化问题。</p><p>存在观测噪声的情况下，等式约束可以松弛为不等式约束的最优化问题（$L_1$最小化）：<br>\begin{equation}\label{equ:l1sr-e}<br>\min_{\boldsymbol{x}}|\boldsymbol{x}|_1, s.t. |\boldsymbol{y} - \boldsymbol{Ax}| \leq \varepsilon.\tag{5}<br>\end{equation}</p><p>$L_1$范数下的最优化问题又称为基追踪(base pursuit, BP)。这是一个二次约束线性规划问题(quadratically constrained linear problem, QCLP)。</p><p>若$\boldsymbol{x}_1$是$L_1$的解，$\boldsymbol{x}_0$是$L_0$的解，则有[2]：<br>\begin{equation}<br>|\boldsymbol{x}_1|_1 \leq |\boldsymbol{x}_0|_1.<br>\end{equation}<br>因为$\boldsymbol{x}_1$是可行解，$\boldsymbol{x}_0$是最优解。同时$A\boldsymbol{x}_1 = A\boldsymbol{x}_0$。</p><p>同样的，式\ref{equ:l1sr-e}也有两种变形：</p><ul><li>（$L_1$惩罚最小化）利用$\boldsymbol{x}$是K稀疏向量的约束，将$L_1$不等式范数最小化变成$L_2$：    \begin{equation}\min_{\boldsymbol{x}}\frac{1}{2}|\boldsymbol{y}- \boldsymbol{Ax}|_2^2,s.t.|\boldsymbol{x}|\leq q.    \end{equation}    划归为一类二次规划(quadratic program, QP)问题。</li><li>利用Lagrangian乘子法，将$L_1$不等式范数最小化变成：    \begin{equation}\label{equ:presu-Tik}    \min_{\lambda, \boldsymbol{x}}\frac{1}{2}|\boldsymbol{y}- \boldsymbol{Ax}|_2^2 + \lambda|\boldsymbol{x}|_1.    \end{equation}    划归为一类基追踪去噪(basis pursuit denoising, BPDN)问题[3]。</li></ul><p>在基于小波变换的图像/信号重构和恢复（deconv）中，也经常会遇到基追踪去噪问题。</p><p>参数稀疏的好处主要有以下两点：</p><ul><li>特征选择(Feature Selection)</li><li>可解释性(Interpretability)</li></ul><blockquote id="fn_4"><sup>4</sup>. 指所有NP问题都能在多项式时间复杂度内归约到的问题.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a></blockquote><blockquote id="fn_5"><sup>5</sup>. $L_0$范数不满足范数公理中的齐次性：$|c\boldsymbol{x}|_0 = |c||\boldsymbol{x}|_0$，故严格来讲它是一种虚拟的范数，也称拟范数。<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a></blockquote><blockquote id="fn_6"><sup>6</sup>. $\forall p\in\mathbb{R}^+, |\boldsymbol{x}|_p = \left( \sum_{i\in supp(\boldsymbol{x})}|x_i|^p\right)^{1/p} $.<a href="#reffn_6" title="Jump back to footnote [6] in the text."> &#8617;</a></blockquote><h3 id="Restricted-Isometry-Property-Condition"><a href="#Restricted-Isometry-Property-Condition" class="headerlink" title="Restricted Isometry Property Condition"></a>Restricted Isometry Property Condition</h3><p>前文讨论了$L_1$范数最小化问题是$L_0$范数最小化某种程度的凸松弛。接下来考察两种问题的解之间的关系。<br><strong>Definition 1.(Restricted Isometry Property Condition)</strong>[4][5]<br><em>若存在矩阵$\boldsymbol{A}$和K-稀疏向量$|\boldsymbol{x}|_0\leq K$，<br>    \begin{equation}<br>        (1-\delta_K)|\boldsymbol{x}|_2^2 \leq |\boldsymbol{A}_K\boldsymbol{x}|_2^2 \leq (1 + \delta_K)|\boldsymbol{x}|_2^2,<br>    \end{equation}<br>    其中$0\leq \delta_K &lt; 1$是一个与稀疏度K有关的常数(约束等距常数, restricted isometry constants, RIC)，$\boldsymbol{A}_K$是字典矩阵$\boldsymbol{A}$的任意K列组成的子矩阵。则称矩阵$\boldsymbol{A}$满足K阶RIP条件。</em></p><p><strong>当RIP条件满足时，非凸的$L_0$范数最小化问题与$L_1$范数最小化问题等价。</strong>也即是：<br>\begin{equation}<br>\min||\boldsymbol{x}||_0\,\,s.t. \boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}\overset{\text{概率为1 的}}{\underset{}{\iff}}\min||\boldsymbol{x}||_1\,\,s.t. \boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}<br>\end{equation}</p><p>带参数$\delta_K$的K阶RIP条件简记为$RIP(K,\delta_K)$，定义为所有使$RIP(K,\delta_K)$成立的参数$\delta$的下确界：<br>\begin{equation}<br>    \delta_K = \inf\left\lbrace \delta: (1-\delta)|\boldsymbol{z}|_2^2 \leq |\boldsymbol{A}_{supp(z)}\boldsymbol{z}|_2^2 \leq (1 + \delta)|\boldsymbol{z}|_2^2, \forall |supp(z)| \leq K, \forall \boldsymbol{z}\in\mathbb{R}^{|supp(\boldsymbol{z})|} \right\rbrace<br>\end{equation}<br>显然若$\boldsymbol{A}_K$正交，则$\delta_K = 0$。于是，RIC的非零值实际上可以评价该矩阵的非正交程度。此外，由于$\boldsymbol{A}_K$的任意性，要求$\boldsymbol{A}$在每一列的能量分布投影尽可能均匀。</p><p>RIC有三个重要性质：</p><ul><li>系数信号精确重构的充分条件[6]： 若字典矩阵$\boldsymbol{A}$分别满足$\delta_K, \delta_{2K}, \delta_{3K}$的RIP条件，并且：    \begin{equation}\delta_K + \delta_{2K} + \delta_{3K} &lt; 1.\end{equation}        则$L_1$范数最小化可以精确重构所有K稀疏信号。也即是，在此条件下，若无噪声存在，则K稀疏信号可以确保由$L_1$范数最小化精确恢复；并且在有噪声的情况下可以稳定估计。</li><li>RIC与特征值的关系[7]：若字典矩阵$\boldsymbol{A}\in\mathbb{R}^{m\times n}$满足$RIP(K,\delta_K)$，则：\begin{equation}1 - \delta_K \leq \lambda_{min}(\boldsymbol{A}_K^T\boldsymbol{A}_K) \leq \lambda_{max}(\boldsymbol{A}_K^T\boldsymbol{A}_K) \leq 1 + \delta_K.\end{equation}</li><li>单调性[6]：若$K\leq K’$，则$\delta_K \leq \delta_{K’}$.</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]: Tropp, J. A. . “Algorithms for simultaneous sparse approximation. Part II: Convex relaxation.” Signal Processing 86.3(2006):589-602.<br>[2]: David, L., and Donoho. “For most large underdetermined systems of linear equations the minimal 1-norm solution is also the sparsest solution.” Communications on Pure and Applied Mathematics (2006).<br>[3]: Chen, S. S. . “Atomic decomposition by basis persuit.” Siam J Sci Comp 20(1999).<br>[4]: Dandes, E. J. . “Near-optimal signal recovery from random projections.” Universal encoding strategies IEEE Transactions on Information Theory 52(2006).<br>[5]: Foucart, Simon , and  M. J. Lai . “Sparsest solutions of underdetermined linear systems via $0{    extbackslashell$0q$0-minimization for $00{    extlessq{    extbackslashleq 1$0.” (2009).<br>[6]: Cai, T. T. ,  L. Wang , and  G. Xu . “New bounds for restricted isometry constants.” IEEE Press (2010).<br>[7]: Dai, W. , and  O. Milenkovic . “Subspace Pursuit for Compressive Sensing Signal Reconstruction.” IEEE Transactions on Information Theory 55.5(2009).</p>]]></content>
    
    
    <summary type="html">使用少量基本信号的线性组合表示一目标信号，称为信号的稀疏表示。信号的稀疏表示是过去近25年来信号处理界一个非常引人关注的研究领域（现在凉透了），众多研究论文和专题研讨会表明了该领域曾经的的蓬勃发展。信号稀疏表示的目的就是在给定的超完备字典中用尽可能少的原子来表示信号，可以获得信号更为简洁的表示方式，从而使我们更容易地获取信号中所蕴含的信息，更方便进一步对信号进行加工处理，如压缩、编码等。(in Chinese)</summary>
    
    
    
    <category term="Signal Processing" scheme="http://inlmouse.github.io/categories/Signal-Processing/"/>
    
    
    <category term="Math" scheme="http://inlmouse.github.io/tags/Math/"/>
    
    <category term="Machine Learning" scheme="http://inlmouse.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>From Cross Entropy to Metric Learning: A Brief Introduction and Some Discussion</title>
    <link href="http://inlmouse.github.io/CEMLE/"/>
    <id>http://inlmouse.github.io/CEMLE/</id>
    <published>2022-12-01T04:13:00.000Z</published>
    <updated>2022-12-12T14:32:47.836Z</updated>
    
    <content type="html"><![CDATA[<h2 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h2><p>信息熵是定义信息量的一种测度，如同热力学中定义一样[1]：熵越大混乱程度越大，信息熵越大信息量越大。因此信息熵在直观上需要满足以下两点要求：</p><ul><li>越不可能($p(x)$越小)发生的事件($x$)信息量越大，确定事件($p(x)$很大)的信息量很小；</li><li>独立事件的信息量可叠加；</li></ul><p>满足以上要求的测度定义方式并不唯一，但是数学严谨化之后满足性质的熵几乎是唯一的<sup><a href="#fn_1" id="reffn_1">1</a></sup>。于是有如下定理：</p><p><strong>Theorem 1</strong>(离散信息熵表述唯一性定理[2]). <em>任何具有上述性质的离散的熵，其函数形式必为（Shannon熵 或von Neumann熵）和（Shannon熵 或Hartley 熵）的线性组合。</em></p><p>因此在此定义：<br><strong>Definition 1</strong> (Shannon熵). <em>若$p(x_i)$表随机事件$X$观测为$x_i$的概率，则Shannon熵：<br>\begin{equation}<br>H(X) = -\sum_{i=1}^np(x_i)\log_ap(x_i).<br>\end{equation}<br>这里如果$a=e$则$H(X)$的单位为奈培(NP)，如果$a=2$则$H(X)$的单位为比特(bit)，实际的不同只会让相差一个常系数，并不影响其实际意义。</em></p><blockquote id="fn_1"><sup>1</sup>. 在数学上需要严格满足以下三点：1. 不变性：相空间的熵在保测度正则变换下不变，量子熵在unitary变换下不变；2. 可加性或者泛可加性(subadditive)；3. 连续延拓后需要是凸函数；<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><h2 id="相对熵-KL-Divergence"><a href="#相对熵-KL-Divergence" class="headerlink" title="相对熵:KL Divergence"></a>相对熵:KL Divergence</h2><p>相对熵又称KL散度(Kullback-Leibler Divergence)，是衡量两个事件或者分布之间相似度的度量。当然这个度量也不唯一，但是那是另外一个话题。</p><p><strong>Definition 2</strong> (KL散度). <em>对于随机事件$X$和$Y$，其概率分布分别为$p(x),q(x)$，则KL散度：<br>\begin{equation}\label{equ:KL}<br>D_{KL}(X||Y) = \sum_{i=1}^np(x_i)\log_a\dfrac{p(x_i)}{q(x_i)}. \tag{1}<br>\end{equation}</em></p><p>值得注意的是\ref{equ:KL}式不满足对称性($D_{KL}(X||Y) \neq D_{KL}(Y||X)$)和三角不等式，这一点在JS散度中得到了改善。</p><h2 id="交叉熵-Cross-Entropy"><a href="#交叉熵-Cross-Entropy" class="headerlink" title="交叉熵:Cross Entropy"></a>交叉熵:Cross Entropy</h2><p>考查\ref{equ:KL}式：<br>\begin{equation}<br>D_{KL}(X||Y) = -H(X)-\sum_{i=1}^np(x_i)\log_aq(x_i).<br>\end{equation}<br>注意到在实际使用中往往用$p(x)$来表示样本的真实分布，那么$H(X)$的值往往是不变的，因而直接考察后一项即可。</p><p><strong>Definition 3</strong> (交叉熵). <em>对于随机事件$X$和$Y$，其概率分布分别为$p(x),q(x)$，则交叉熵：<br>\begin{equation}<br>H(X,Y) = -\sum_{i=1}^np(x_i)\log_aq(x_i).<br>\end{equation}</em></p><h2 id="交叉熵的MLE-最大似然估计-解释"><a href="#交叉熵的MLE-最大似然估计-解释" class="headerlink" title="交叉熵的MLE(最大似然估计)解释"></a>交叉熵的MLE(最大似然估计)解释</h2><p>现在有一个真实分布为$p(x)$的随机变量$X$，我们对它进行了$N$次独立同分布实验，对于每个可能的结果$x_i(i=1,\cdots,n)$观察到的次数为$N(x_i)$，显然$\sum_{i=1}^nN(x_i)=N$，那么它的似然值就可以写成：<br>\begin{equation}<br>L=\prod_{i=1}^np(x_i)^{N(x_i)}.<br>\end{equation}</p><p>考察其对数似然值：<br>\begin{equation}\label{equ:lnl}<br>\ln L=\sum_{i=1}^nN(x_i)\ln p(x_i).\tag{2}<br>\end{equation}</p><p>\ref{equ:lnl}式有两个缺点，其一它是个负数，其二它的数值跟样本数有关，样本越多数值越小。因此除以总的样本数归一化，再取相反数，然后改用频率表示：<br>\begin{equation}<br>-\frac{\ln L}{N}=-\sum_{i=1}^n\frac{N(x_i)}{N}\ln p(x_i) = -\sum_{i=1}^n q(x_i)\ln p(x_i) =H(Y,X),<br>\end{equation}<br>显然$\frac{N(x_i)}{N}$即是观测到的概率$q(x_i)$。</p><p>下面在给定$q(x)$的情况下考察$-\dfrac{\ln L}{N}$的最小值时$p(x)$的取值，考虑拉格朗日乘子法，考察拉式量：<br>\begin{equation}<br>W=-\sum_{i=1}^n q(x_i)\ln p(x_i) + \lambda(\sum_{i=1}^np(x_i)-1).<br>\end{equation}<br>求偏导可得：</p><p>\begin{equation}<br>-\frac{q(x)}{p(x)}+\lambda=0,<br>\end{equation}<br>即是$\forall i, p(x_i),q(x_i)$成比例，再由概率归一化条件知：<br>\begin{equation}<br>p(x)=q(x).<br>\end{equation}</p><center><strong>因此可以看出，交叉熵最小实质上就是似然值最大。</strong></center><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1]: Witten, E. . “A Mini-Introduction To Information Theory.”, 10.1007/s40766-020-00004-5. 2018.<br>[2]: Aczel, Janos. et al. “Why the Shannon and Hartley entropies are ‘natural’”, 131—146/Advances in applied probability. 1974.</p>]]></content>
    
    
    <summary type="html">Some discussion of Cross Entropy Loss and its equivalence between MLE.(in Chinese)</summary>
    
    
    
    <category term="Machine Learning" scheme="http://inlmouse.github.io/categories/Machine-Learning/"/>
    
    
    <category term="Math" scheme="http://inlmouse.github.io/tags/Math/"/>
    
    <category term="Information Theory" scheme="http://inlmouse.github.io/tags/Information-Theory/"/>
    
  </entry>
  
  <entry>
    <title>沉痛悼念江泽民同志</title>
    <link href="http://inlmouse.github.io/deepmournjzm/"/>
    <id>http://inlmouse.github.io/deepmournjzm/</id>
    <published>2022-11-30T04:13:00.000Z</published>
    <updated>2022-12-02T15:59:32.629Z</updated>
    
    <content type="html"><![CDATA[<img src="/deepmournjzm/jzm.jpg" class width="320" height="445" title="永垂不朽"> <center>江泽民同志遗像。新华社照片，北京，2022年11月30日。</center><blockquote><p>我们敬爱的江泽民同志患白血病合并多脏器功能衰竭，抢救无效，于2022年11月30日12时13分在上海逝世，享年96岁。</p><footer><strong>新华社北京11月30日电,</strong><cite><a href="http://sethgodin.typepad.com/seths_blog/2009/07/welcome-to-island-marketing.html">告全党全军全国各族人民书</a></cite></footer></blockquote><center><strong>吹拉弹唱，我的太阳成绝响； 印俄日德，葛底斯堡仍绕梁。 High Level!</strong></center>]]></content>
    
    
    <summary type="html">苟利国家生死以，岂因祸福避趋之。(in Chinese)</summary>
    
    
    
    <category term="Political Commentary" scheme="http://inlmouse.github.io/categories/Political-Commentary/"/>
    
    
    <category term="politic" scheme="http://inlmouse.github.io/tags/politic/"/>
    
  </entry>
  
</feed>
